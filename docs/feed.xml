<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hujin Blog</title>
    <description>Hujin，Openstack &amp; SDN &amp; Kubernetes Lover，Software Engineer，| 与你一起发现更大的世界</description>
    <link>http://0.0.0.0:4000/blog/</link>
    <atom:link href="http://0.0.0.0:4000/blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 14 Aug 2024 03:15:53 +0000</pubDate>
    <lastBuildDate>Wed, 14 Aug 2024 03:15:53 +0000</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
      <item>
        <title>sanlock原理和使用</title>
        <description>&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;sanlock 是一个基于 SAN 的分布式锁管理器。集群中的每个节点都各自运行 sanlock 服务，锁的状态都被写到了共享存储上，使用 Disk Paxos 算法读写共享存储以实现对分布式锁的获取、释放和超时。一般认为 SAN 中的 LUN 的可靠性要比集群中的主机高，对主机来说，应用程序进程可能会崩溃、重启，主机的 IP 网络也可能会发生故障，而 SAN 是通过专门的光网连接的，还可以配置多路径以提高可用性和吞吐量，一个 LUN 也可能被冗余映射到阵列中的多块磁盘上，因此 SAN 服务质量很高。通过利用 Disk Paxos 算法，sanlock 服务的所有数据都保存在共享存储上，即使服务器进程崩溃也不会影响可靠性。在一个集群里，通常 SAN 都是核心，可以认为如果 SAN 出现问题，整个集群的功能都会受到严重影响，因此选择以共享存储作为 sanlock 服务的基础是合理的。&lt;/p&gt;

&lt;p&gt;oVirt 使用了 sanlock 作为其存储域的分布式锁管理器，sanlock 被用来保证领导节点的唯一性，并且用来保护共享存储设备上的存储域的元数据的一致性，以及保护同一个虚拟机镜像不会被两台虚拟机同时使用。libvirt也使用了 sanlock，它的 sanlock 插件可以在虚拟机启动前为所有的磁盘请求锁保护，以防止在意外情况下有两台虚拟机使用同一个磁盘镜像。&lt;/p&gt;

&lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;p&gt;在 sanlock 集群中，不存在专门的锁服务器或节点，每一个节点上都运行 sanlock 后台服务，所有的节点都是平等的。&lt;/p&gt;

&lt;p&gt;锁的状态被保存在共享存储上，sanlock 使用的共享存储既可以是通过 SAN 访问的 LUN，也可以是 NAS（比如 NFS 中的某个文件），所有节点都能访问共享存储，共同维护锁的状态。&lt;/p&gt;

&lt;p&gt;sanlock 的锁是名义锁，并非强制锁，其语义类似 pthread 的 mutex；被 sanlock 保护的对象，即使不上锁也能直接访问，完全由程序员自由决定 sanlock 的锁保护的是何种资源，何时上锁和释放。&lt;/p&gt;

&lt;h3 id=&quot;delta-lease&quot;&gt;Delta Lease&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Delta Lease只由 sanlock 内部使用，不对外暴露，主要作用是保证节点 ID 的唯一性，并提供简单的成员管理服务&lt;/li&gt;
  &lt;li&gt;Delta Lease 用来确保特定的节点 ID(1-2000范围内) 只能由唯一一个节点持有，并且每个 Delta Lease 只需要一个块就能实现&lt;/li&gt;
  &lt;li&gt;原理是在向共享存储写完数据，并等待足够长的时间后再读，如果发现数据没有被起他节点更改，就认为没有冲突并成功获取了租约的所有权。&lt;/li&gt;
  &lt;li&gt;从原理上来看Delta Lease 的获取需要比较长的时间，但比较省空间&lt;/li&gt;
  &lt;li&gt;把所有的 Delta Lease 磁盘块连续的保存在一个文件或者裸磁盘设备里，就形成了一个 sanlock 的 lockspace&lt;/li&gt;
  &lt;li&gt;每个节点每隔20s发送一次心跳，更新lockspace lease的timestamp属性&lt;/li&gt;
  &lt;li&gt;一旦一个节点获取到某个lockspace的delta lease后（加入这个lockspace），表示这个节点上的进程可以获取此lockspace中的resource锁&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img1.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个节点通过在不同的 lockspace 中获取 Delta Lease，可以加入不同的 sanlock 集群。&lt;/p&gt;

&lt;p&gt;图中主机名为 hostA 的节点，已经成功获取了 Host ID 1 的 Delta Lease 后，hostB 通过读取 Delta Lease 的磁盘块，发现租约已经被占有，就无法再次获取&lt;/p&gt;

&lt;h3 id=&quot;paxos-lease&quot;&gt;Paxos Lease&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;基于 Disk Paxos 算法&lt;/li&gt;
  &lt;li&gt;通过对共享存储的特定的顺序的读写操作实现对租约的获取、续租和释放。一旦获取了 Paxos Lease 后，sanlock 就自动为其续租&lt;/li&gt;
  &lt;li&gt;共享存储上的 Paxos Lease 实例被划分为与 sanlock 集群节点数相同数量的块&lt;/li&gt;
  &lt;li&gt;集群中的每个节点都可以写自己 ID 对应的块，但只能读其他节点对应的块&lt;/li&gt;
  &lt;li&gt;某节点要占有一个租约，只需要将请求写入自己对应的块，然后读取所有其他节点的块，查看是否有冲突&lt;/li&gt;
  &lt;li&gt;获取时间短，但是占用大量空间&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img2.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图中：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Host ID 为 1 的主机成功获取了 Paxos Lease 后，Host ID 为 3 的主机在读取共享存储时，就能检测到冲突，判定为获取失败&lt;/li&gt;
  &lt;li&gt;按照 Disk Paxos 算法，将正确的所有者编号写入自己的专有块，并递增提案编号&lt;/li&gt;
  &lt;li&gt;Host ID 为 2 的主机没有获取 Paxos Lease 的意向，则不必参与到事务中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当节点由于sanlock crash或者无法访问存储等情况导致租期无法续约时，sanlock进入recovery mode，会先停止节点上已经获取resource lease的进程&lt;/p&gt;

&lt;p&gt;有几种方式：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;graceful shutdown：调用进程服务暴露给sanlock的graceful_shutdown的handler方法，或者给进程发送sigterm信号；如果没有退出继续使用下面的方法&lt;/li&gt;
  &lt;li&gt;force shutdown：发送sigkill信号给进程；如果没有退出继续使用下面的方法&lt;/li&gt;
  &lt;li&gt;host reset：触发节点的watchdog进程reset节点&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;节点异常后其他节点获取锁的流程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img3.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;正常情况下，进程获取到锁后，resource lease这部分就没有io了，因为resource lease不发送心跳&lt;/li&gt;
  &lt;li&gt;锁所在节点异常后，则节点无法发送delta lease心跳，但是锁本身还是分配给源节点&lt;/li&gt;
  &lt;li&gt;其他节点请求锁时，会检测锁对应的节点，然后检查节点的delta lease心跳，如果心跳正常则获取失败，反之获取成功&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;共享锁：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img9.png&quot; alt=&quot;delta lease&quot; /&gt;
在 Sanlock 的共享模式下，多个节点可以同时获取同一个资源的锁，这与独占模式不同，在独占模式下，一次只能有一个节点获取锁&lt;/p&gt;

&lt;p&gt;共享模式的主要用途包括：&lt;/p&gt;

&lt;p&gt;资源协调：允许多个节点同时访问某个资源，例如，读取配置信息或共享状态。
并行操作：在某些情况下，多个节点可能需要执行不会相互冲突的操作，共享锁可以用来协调这些操作。
共享锁的特点是：&lt;/p&gt;

&lt;p&gt;非独占：多个节点可以同时持有锁。
可重入：持有锁的节点可以多次获取同一个锁，而不会导致死锁。
协调：虽然多个节点可以同时持有锁，但Sanlock仍然负责协调锁的获取和释放，确保操作的原子性。&lt;/p&gt;

&lt;p&gt;全局锁：&lt;/p&gt;

&lt;p&gt;全局锁与全局信息相关联，全局信息包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;全局VG 命名空间&lt;/li&gt;
  &lt;li&gt;PV集合和未使用的设备&lt;/li&gt;
  &lt;li&gt;PV的属性，例如PV大小&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;读取这些信息的命令会以共享模式获取全局锁，更改这些信息的命令则以独占模式获取。&lt;/p&gt;

&lt;p&gt;例如’vgs’ 命令以共享模式获取全局锁，因为它报告所有VG名称的列表，而vgcreate命令以独占模式获取全局锁，因为它创建了一个新的VG名称，并且从未使用的PV列表中取出一个PV。&lt;/p&gt;

&lt;p&gt;当LVM命令收到tag参数，或使用select时，就必须读取所有vg以匹配tag或select，将获取全局锁&lt;/p&gt;

&lt;p&gt;VG锁：&lt;/p&gt;

&lt;p&gt;vg锁与每个共享vg相关联。VG锁在读取VG时以共享模式获取，在更改vg或激活lvs时以独占模式获取。这个锁将访问vg的操作顺序执行，以防止所有主机上访问该vg的lvm命令并发执行&lt;/p&gt;

&lt;p&gt;命令 ‘vgs &lt;vgname&gt;&apos; 不获取全局锁（它不需要所有VG名称的列表），但会针对每个vg名称参数获取vg锁&lt;/vgname&gt;&lt;/p&gt;

&lt;p&gt;LV锁:&lt;/p&gt;

&lt;p&gt;LV锁在激活LV之前获取，并在LV停用后释放。如果无法获取LV锁，则不会激活LV。（LV锁是持久的，并且在激活命令完成后仍然保持。全局锁和VG锁是临时的，只在LVM命令运行时保持）&lt;/p&gt;

&lt;h3 id=&quot;wdmdfence&quot;&gt;wdmd&amp;amp;fence&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img4.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;p&gt;sanlock 在启动后会打开一个 wdmd 连接，定时发送保活心跳。&lt;/p&gt;

&lt;p&gt;sanlock 同时会监视获取了 Paxos Lease 的进程，如果进程崩溃了，sanlock 就会自动将 Paxos Lease 释放。&lt;/p&gt;

&lt;p&gt;如果由于某种原因导致 sanlock 无法访问共享存储，sanlock 就无法续租，这时 sanlock 会自动向持有 Paxos Lease 的进程发 SIGTERM 或者 SIGKILL。&lt;/p&gt;

&lt;p&gt;也可以配置 sanlock 的 killpath 参数，在无法续租的时候，让 sanlock 调用 killpath 指向的程序。&lt;/p&gt;

&lt;p&gt;如果由于种种原因，sanlock 无法关闭持有 Paxos Lease 的进程，sanlock 就不会再向 wdmd 发送保活更新，于是 watchdog 设备会在系统保活更新超时后，重启整台主机。&lt;/p&gt;

&lt;p&gt;有了 wdmd 的配合，sanlock 就能对有问题的节点或进程进行自我 fence&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：由于后端存储性能差会导致心跳发送不及时，触发fence导致机器关闭，安超关闭了fence功能&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;lvm中的sanlock&quot;&gt;lvm中的sanlock&lt;/h3&gt;
&lt;p&gt;lvm对接san存储：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SAN存储端划分好LUN，然后通过映射给安超每个节点，使用该LUN创建sanlock VG，所有节点都能访问&lt;/li&gt;
  &lt;li&gt;在VG基础上，创建lv，写入qcow2，然后将lv挂载给虚拟机用作系统盘和数据盘&lt;/li&gt;
  &lt;li&gt;每个lv的format为qcow2，每个lv需要active才能在当前节点看到它的路径&lt;/li&gt;
  &lt;li&gt;lvchange -asy vgname/lvname可以active这个lv，然后当前节点才能看到lv的完整路径/dev/vgname/lvname&lt;/li&gt;
  &lt;li&gt;qemu-img info /dev/vgname/lvname可以查看lv的基本qcow2信息，在active之后执行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img5.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img6.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img7.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/sanlock-img8.png&quot; alt=&quot;delta lease&quot; /&gt;&lt;/p&gt;

&lt;p&gt;lvm中lockspace和resource lease文件是同一个文件，一个vg对应一个lockspace文件（/dev/mapper/[vg-name]-lvmlock）&lt;/p&gt;

&lt;p&gt;每个lockspace文件中offset如上图所示：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;offset从0开始是lockspace空间，只需要占用1M空间，一个节点占用512byte，最大支持2k个节点&lt;/li&gt;
  &lt;li&gt;到offset是65M的位置，保存global lock，占用1M&lt;/li&gt;
  &lt;li&gt;到offset是66M的位置，保存vg lock，占用1M&lt;/li&gt;
  &lt;li&gt;到offset是67M之后的位置，保存lv lock&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;操作&quot;&gt;操作&lt;/h2&gt;
&lt;h3 id=&quot;准备&quot;&gt;准备&lt;/h3&gt;
&lt;p&gt;注：这里使用nfs作为共享存储来demo，nfs挂载在两个节点的/data目录下&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;yum install -y sanlock sanlock-python
systemctl enable sanlock
systemctl start sanlock
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;初始化lockspace和resource文件&quot;&gt;初始化lockspace和resource文件&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 创建两个1M的lease文件
dd if=/dev/zero bs=1048576 count=1 of=/data/hosts_lease
dd if=/dev/zero bs=1048576 count=1 of=/data/resource_lease

# 初始化host_lease
# 命令格式为
#    sanlock direct init -s &amp;lt;name&amp;gt;:&amp;lt;host_id&amp;gt;:&amp;lt;path&amp;gt;:&amp;lt;offset&amp;gt;
# &amp;lt;name&amp;gt;:    sanlock的lockspace名字，任意字符串即可
# &amp;lt;host_id&amp;gt;: 主机id，前面提到的必须唯一的主机编号，初始化时用0替代
# &amp;lt;path&amp;gt;:    主机管理文件的具体路径，这里是/data/hosts_lease
# &amp;lt;offset&amp;gt;:  偏移量，始终提供0即可
sanlock direct init -s test:0:/data/hosts_lease:0
chown sanlock:sanlock /data/hosts_lease

# 初始化resource_lease
# 注意，这里实际上可以通过offset的方式还使用host_lease文件作为resource_lease来用，demo时使用了一个独立的resource_lease文件，offset设置成0
sanlock direct init -r test:testres:/data/resource_lease:0
chown sanlock:sanlock /data/resource_lease
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;节点1添加lockspace&quot;&gt;节点1添加lockspace&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 命令格式为
#    sanlock client add_lockspace -s &amp;lt;name&amp;gt;:&amp;lt;host_id&amp;gt;:&amp;lt;path&amp;gt;:&amp;lt;offset&amp;gt;
# &amp;lt;name&amp;gt;:    sanlock的lockspace名字，前面初始化步骤中名为test
# &amp;lt;host_id&amp;gt;: 主机id，前面提到的必须唯一的主机编号，对于主机1，我们编号为1
# &amp;lt;path&amp;gt;:    主机管理文件的具体路径，这里是/data/hosts_lease
# &amp;lt;offset&amp;gt;:  偏移量，始终提供0即可

sanlock client add_lockspace -s test:1:/data/hosts_lease:0
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;节点2添加lockspace&quot;&gt;节点2添加lockspace&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;sanlock client add_lockspace -s test:2:/data/hosts_lease:0
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;查看lockspace中的节点信息&quot;&gt;查看lockspace中的节点信息&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 ~]# sanlock direct dump /data/hosts_lease 
  offset                            lockspace                                         resource  timestamp  own  gen lver
00000000                                 test       0b8392d3-094f-42af-905d-86514f0d61a7.node2 0002835709 0001 0001
00000512                                 test       7f9208cd-90a5-4c81-8e91-d2d52f32103e.node1 0002835701 0002 0001
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;在两边节点分别启动一个sleep进程&quot;&gt;在两边节点分别启动一个sleep进程&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;sanlock client command -c /usr/bin/sleep 3000 &amp;amp;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;节点1获取锁&quot;&gt;节点1获取锁&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：58521是节点1上sleep的进程号&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里offset设置成0，如果和host_lease共用一个文件时可以指定offset，在offset后面支持指定锁的类型，默认是互斥锁，也支持共享锁（SH）&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node2 ~]# sanlock client acquire -r test:testres:/data/resource_lease:0 -p 58521
acquire pid 58521
acquire done 0
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;节点2获取锁失败&quot;&gt;节点2获取锁失败&lt;/h3&gt;
&lt;p&gt;返回值：-243&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 ~]# ps -ef|grep sleep
root      34426  91554  0 11:05 pts/0    00:00:00 /usr/bin/sleep 3000
root      34615  91554  0 11:05 pts/0    00:00:00 grep --color=auto sleep
[root@node1 ~]# sanlock client acquire -r test:testres:/data/resource_lease:0 -p 34426
acquire pid 34426
acquire done -243
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;节点1手动释放锁后节点2再获取锁&quot;&gt;节点1手动释放锁后节点2再获取锁&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 节点1释放锁
[root@node1 ~]# sanlock client release -r test:testres:/data/resource_lease:0 -p 58521
release pid 58521
release done 0

# 节点2成功获取锁
[root@node2 ~]# sanlock client acquire -r test:testres:/data/resource_lease:0 -p 34426
acquire pid 34426
acquire done 0

# 锁被node2获取
[root@node2 ~]# sanlock direct dump /data/resource_lease
  offset                            lockspace                                         resource  timestamp  own  gen lver
00000000                                 test                                          testres 0002836575 0002 0001 5
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;节点2-kill-sleep进程模拟进程异常后节点1再获取锁&quot;&gt;节点2 kill sleep进程（模拟进程异常）后，节点1再获取锁&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# kill sleep进程后，resource_lease中own还是node2但是timestmap变成了0，锁被释放了
[root@node2 ~]# ps -ef|grep sleep
root      34426  91554  0 11:05 pts/0    00:00:00 /usr/bin/sleep 3000
root      42242  91554  0 11:14 pts/0    00:00:00 grep --color=auto sleep
[root@node2 ~]# kill -9 34426
[root@node2 ~]# sanlock  direct dump /data/resource_lease 
  offset                            lockspace                                         resource  timestamp  own  gen lver
00000000                                 test                                          testres 0000000000 0002 0001 5

# 节点1获取锁
[root@node1 ~]# ps -ef|grep sleep
root      58521 117744  0 11:03 pts/0    00:00:00 /usr/bin/sleep 3000
root      69528 117744  0 11:16 pts/0    00:00:00 grep --color=auto sleep
[root@node1 ~]# sanlock client acquire -r test:testres:/data/resource_lease:0 -p 58521
acquire pid 58521
acquire done 0

[root@node1 ~]# sanlock client status
daemon 0b8392d3-094f-42af-905d-86514f0d61a7.node1
p -1 helper
p -1 listener
p 58521 
p -1 status
s test:1:/data/hosts_lease:0
s LS:2:/data/nfs/test_lockspace:0
r test:testres:/data/resource_lease:0:6 p 58521
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;同一个节点两个不同进程获取锁类似&quot;&gt;同一个节点两个不同进程获取锁类似&lt;/h3&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://pagure.io/sanlock&lt;/li&gt;
  &lt;li&gt;https://cloud.tencent.com/developer/article/1651000&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/maokexu123/article/details/40790939&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2024/07/24/sanlock/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2024/07/24/sanlock/</guid>
        
        <category>sanlock</category>
        
        <category>lvmlockd</category>
        
        
      </item>
    
      <item>
        <title>Neuvector源码之 网络流量拓扑</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;容器环境通信频繁，包含大量的东西向和南北向通信，通过neuvector的网络活动功能，可以全局查看集群中容器间的流量/访问host的流量/访问外网以及来自外网的流量。
网络活动功能比较丰富，包含：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;网络抓包&lt;/li&gt;
  &lt;li&gt;模式切换（学习/告警/保护）&lt;/li&gt;
  &lt;li&gt;实时流量查看（方向/协议/端口/数据统计等）&lt;/li&gt;
  &lt;li&gt;容器的自动发现和展示&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们只深入介绍下实时流量的实现，并讨论下移植到虚拟机场景的可能性&lt;/p&gt;

&lt;h2 id=&quot;架构图&quot;&gt;架构图&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_flow.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;kubernetes的网络是通过cni标准来实现的，实现方式多种多样。neuvector提供一种不依赖特定cni的通用的方式来获取容器流量并展示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_flow1.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们从几个维度来看neuvector的流量拓扑功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;分成三个模块，分别是controller/enforce/dp&lt;/li&gt;
  &lt;li&gt;controller负责提供api接口，汇总监控数据并行成map拓扑数据&lt;/li&gt;
  &lt;li&gt;enforce负责监控节点容器的生命周期，汇总dp的数据，并添加相关容器信息，上报给controller&lt;/li&gt;
  &lt;li&gt;dp是最底层负责解析数据报文，创建连接session并提供连接监控数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;源码&quot;&gt;源码&lt;/h2&gt;
&lt;p&gt;这里先看下容器的探测
&lt;img src=&quot;/blog/img/neuvector_flow2.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;拓扑展示资源包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;container：容器资源，以group为单位，一个group中可能包含多个容器，类似pod&lt;/li&gt;
  &lt;li&gt;node：物理节点&lt;/li&gt;
  &lt;li&gt;external：外部网络&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里通过容器资源的获取来具体介绍流程情况：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;每个节点的enforce容器负责监听当前节点容器runtime的事件，runtime包括：docker/containerd/crio&lt;/li&gt;
  &lt;li&gt;监听的事件包括：创建、停止、删除、拷贝进容器、从容器拷贝出&lt;/li&gt;
  &lt;li&gt;这里会根据节点中proc目录下每个容器的cgroup信息来处理pod中多个容器，这里资源展示以pod单位&lt;/li&gt;
  &lt;li&gt;在监听到事件后更新本地内存数据，同时更新到consul数据库中&lt;/li&gt;
  &lt;li&gt;在controller容器中会watch consul数据库的变化，在发现有容器数据新建后更新本地的内存数据&lt;/li&gt;
  &lt;li&gt;在用户请求api获取容器资源列表时，直接读取内存数据并返回&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后我们再看看connection如何获取的：
&lt;img src=&quot;/blog/img/neuvector_flow3.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当容器创建或者更新时，enforce会调用dp接口下发相关的网络信息&lt;/li&gt;
  &lt;li&gt;当两个容器在相互通信时，所有报文都会被容器所在节点的dp模块监听，dp模块在发现一个新的session创建后，会通过socket连接通知节点上的enforce 容器&lt;/li&gt;
  &lt;li&gt;enforce容器会解析dp发送的消息，同时根据报文mac地址识别出mac对应的源容器，目标容器一般是无法识别的，需要上送到controller中进一步识别&lt;/li&gt;
  &lt;li&gt;在enforce容器中会一个定时任务用来将本地的连接数据通过grpc发送到controller模块&lt;/li&gt;
  &lt;li&gt;在controller中reportconnection方法会更新本地的wlGraph内存数据，并将连接保存到数据库中&lt;/li&gt;
  &lt;li&gt;在用户请求api获取容器资源间连接列表或者指定两个资源间的连接时，直接读取内存数据并返回&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后看看dp中如何获取连接信息的：
&lt;img src=&quot;/blog/img/neuvector_flow4.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当一个新的容器创建完成后，dp会通过socket连接到容器网卡上&lt;/li&gt;
  &lt;li&gt;dp会接受容器网卡的tx和rx报文，其中主要分析rx报文&lt;/li&gt;
  &lt;li&gt;首先会根据报文特征查询对应的session，并更新session中连接信息，主要是一个技术类的参数&lt;/li&gt;
  &lt;li&gt;dp中有一个定时器，每6s将dp中所有进程的连接数据发送到enforce容器
    &lt;blockquote&gt;
      &lt;p&gt;注意：当前只支持ipv4连接的上报&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;虚拟机流量拓扑&quot;&gt;虚拟机流量拓扑&lt;/h2&gt;
&lt;p&gt;进一步分析源码，发现这套机制完全可以应用到虚拟机场景，不过有几个问题需要解决：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;虚拟机的探测：需要监控libvirt事件，检测虚拟机的生命周期&lt;/li&gt;
  &lt;li&gt;虚拟机网卡的探测：需要定时探测虚拟机网卡，包括新增和卸载网卡，并将网卡信息下发给dp，dp和虚拟机网卡建立socket连接&lt;/li&gt;
  &lt;li&gt;保护模式理论上也是可以支持的，但是需要根据虚拟机网络的具体实现在链路中嵌入dp那一套流程（参考另一篇博客）&lt;/li&gt;
  &lt;li&gt;同网络通信是正常支持的，对于vxlan多网络场景，cidr可能相同的情况下，部分根据ip/mac地址获取虚拟机信息的流程需要优化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;虚拟机的流量拓扑已经基本实现，这里有需要可以进一步讨论&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;本次没有针对源码逐步分析，从架构和流程上分析了流量拓扑的实现原理，同时分析虚拟机场景下使用这种方案的可能行。
实际发现流量探测的时效性还是挺高的，基本秒级响应，同时对于异常（告警/攻击/保护）流量可以实时告警。
个人感觉唯一不足的地方是支持的协议不太多，但常用是够的，后面可以看看如何自定义。&lt;/p&gt;

&lt;p&gt;支持的协议：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;协议&lt;/th&gt;
      &lt;th&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;cassandra&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;couchbase&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dhcp&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dns&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;echo&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;grpc&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;http&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;kafka&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mongodb&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mysql&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ntp&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;postgresql&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;redis&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spark&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;sqlinjection&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ssh&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ssl&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tds&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tftp&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tns&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;zookeeper&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2023/10/16/neuvector-visualization/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2023/10/16/neuvector-visualization/</guid>
        
        <category>neuvector</category>
        
        <category>网络流量拓扑</category>
        
        
      </item>
    
      <item>
        <title>Docker容器的网络命名空间为什么不可见？</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;因为做过openstack的虚拟网络，习惯使用netns去管理linux的网络命名空间，也一直理解容器网络应该是类似的。
但是实际在使用时（runtime使用docker）发现每个非hostnetwork的容器创建后并不会查看到一个network namespace(通过ip netns ls查看)；
在使用的runtime是containerd的时候呢，这个namespace又出现了，有点迷糊了，今天就专门看了这里面的原因&lt;/p&gt;

&lt;h2 id=&quot;容器网络命名空间以及问题&quot;&gt;容器网络命名空间以及问题&lt;/h2&gt;
&lt;p&gt;我们将研究Docker容器的网络名称空间文件的问题。具体来说，我们将了解为什么ip netns ls命令看不到网络名称空间文件。
Docker容器的最基础层是Linux cgroup和名称空间机制。这两种机制协同工作，在Docker容器中提供我们所利用的进程和资源隔离。
其中cgroups限制一个进程可以使用的资源，名称空间控制进程间资源的可见性。命名空间中一个类型就是网络命名空间(network namespace)。&lt;/p&gt;

&lt;p&gt;network namespace实质上虚拟化并隔离了进程的网络堆栈。也就是说不同的进程可以有自己独特的防火墙配置、私有IP地址和路由规则。
通过网络命名空间，我们可以为每个Docker容器提供一个与主机网络隔离的网络堆栈。&lt;/p&gt;

&lt;p&gt;在Linux中，管理网络名称空间的主要工具之一是ip netns。这个命令行工具是ip工具的扩展。它允许我们在不同的网络名称空间上执行ip兼容的命令。&lt;/p&gt;

&lt;p&gt;每当我们创建Docker容器时，守护进程都会为容器进程创建名称空间对应文件。然后，它会将这些文件放在目录/proc/{pid}/ns下，其中pid是容器的进程id。&lt;/p&gt;

&lt;p&gt;让我们看一个例子:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ sudo docker run --rm -d ubuntu:latest sleep infinity
2545fdac9b41e463a29b4a61c201b789d567f88d54b6973bdcca9e69ba35ba92
$ sudo docker inspect -f &apos;&apos; 2545fdac9b41e463a29b4a61c201b789d567f88d54b6973bdcca9e69ba35ba92
3357
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在上面的命令中，首先创建一个运行ubuntu:latest映像的Docker容器。然后，我们通过运行sleep infinity来保持容器运行。
最后，我们运行docker inspect命令来获取容器的进程id。&lt;/p&gt;

&lt;p&gt;现在，查看/proc/3357/ns目录，我们可以看到创建了所有不同种类的名称空间:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ sudo ls -la /proc/3357/ns
total 0
dr-x--x--x 2 root root 0 Feb  5 04:24 .
dr-xr-xr-x 9 root root 0 Feb  5 04:24 ..
lrwxrwxrwx 1 root root 0 Feb  5 04:25 cgroup -&amp;gt; &apos;cgroup:[4026531835]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 ipc -&amp;gt; &apos;ipc:[4026532720]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 mnt -&amp;gt; &apos;mnt:[4026532718]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:24 net -&amp;gt; &apos;net:[4026532723]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 pid -&amp;gt; &apos;pid:[4026532721]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 pid_for_children -&amp;gt; &apos;pid:[4026532721]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 time -&amp;gt; &apos;time:[4026531834]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 time_for_children -&amp;gt; &apos;time:[4026531834]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 user -&amp;gt; &apos;user:[4026531837]&apos;
lrwxrwxrwx 1 root root 0 Feb  5 04:25 uts -&amp;gt; &apos;uts:[4026532719]&apos;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;从名称空间对应文件列表中，我们可以看到这个进程的net文件的存在。因为net文件对应于一个Linux网络名称空间，所以理论上我们现在可以列出所有network namespace，
然而事实并非如此。现在运行ip netns ls将显示0个结果:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ ip netns ls
$
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;作为对比让我们手动创建一个网络名称空间。然后，验证它是否在我们运行ip netns时出现:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ sudo ip netns add netA
$ ip netns ls
netA
$
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;正如我们所看到的，它按照预期显示netA。那么为什么不显示docker run创建的network namespace呢？&lt;/p&gt;

&lt;h2 id=&quot;不可见的原因丢失的文件引用&quot;&gt;不可见的原因：丢失的文件引用&lt;/h2&gt;

&lt;p&gt;为了理解这个问题，我们需要知道ip netns ls命令其实是在/var/run/netns目录中查找网络名称空间文件。但是，Docker守护进程在创建后并不会在/var/run/netns目录中创建网络名称空间文件的引用。
因此，ip netns ls无法解析网络命名空间文件。&lt;/p&gt;

&lt;p&gt;解决这种不一致现象的方法是在/var/run/netns目录中为net文件创建一个文件引用。具体来说，我们可以将net名称空间文件绑定挂载到我们在/var/run/netns目录中创建的一个空文件上。&lt;/p&gt;

&lt;p&gt;首先，我们在目录中创建一个空文件，并用名称空间文件所关联的容器id来命名它:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ mkdir -p /var/run/netns
$ touch /var/run/netns/$container_id
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;其中$container_id是一个环境变量，是创建的Docker容器的id&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随后，我们可以运行mount -o bind命令来绑定挂载net文件:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ mount -o bind /proc/3357/ns/net /var/run/netns/$container_id
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;3357是上面获取的容器的pid&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;现在，再次运行相同的ip netns ls命令，我们就能看到docker容器的网络命名空间了:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ ip netns ls
ip netns ls
2545fdac9b41e463a29b4a61c201b789d567f88d54b6973bdcca9e69ba35ba92
netA
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;建立了对网络名称空间文件的文件引用，我们就可以使用ip netns exec运行任何ip命令。例如，我们可以使用ip addr list命令查看网络命名空间上的接口:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;$ ip netns exec $container_id ip addr list
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
4: eth0@if5: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
valid_lft forever preferred_lft forever
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;containerd中如何实现网络命名空间的挂载的&quot;&gt;containerd中如何实现网络命名空间的挂载的&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 ~]# kubectl cluster-info dump |grep containerRuntimeVersion
&quot;f:containerRuntimeVersion&quot;: {},
&quot;containerRuntimeVersion&quot;: &quot;containerd://1.6.6&quot;,
&quot;f:containerRuntimeVersion&quot;: {},
&quot;containerRuntimeVersion&quot;: &quot;containerd://1.6.6&quot;,
&quot;f:containerRuntimeVersion&quot;: {},
&quot;containerRuntimeVersion&quot;: &quot;containerd://1.6.6&quot;,
[root@node1 ~]# ip netns
cni-028380d7-7dcf-44f5-35a4-607654492671 (id: 1)
cni-c37c9aee-54d5-baf1-a606-1c1865b83f6e (id: 0)
cni-63d627ff-481c-20ac-7abe-42ce95db7d28 (id: 21)
cni-1e075915-b90a-e7db-936d-569b71f45c2b (id: 20)
cni-61035ce0-1910-0de3-d59e-837321c2e5e4 (id: 19)
cni-05895113-0f27-8966-8db3-fb45bdd196fe (id: 18)
cni-ac9ba39f-68a4-4fb4-3c14-9d0b893d5687 (id: 17)
cni-ed0a2a49-bd79-775e-b2b1-70b0a662466b (id: 16)
cni-27ad6830-a146-ec01-425c-b6e846cb0207 (id: 15)
cni-cac98bd4-2a73-014f-5ef3-fe634ecdb9b6 (id: 14)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;前面背景中提到的问题原因找到了，但是我们发现在使用containerd作为runtime时，是可以通过ip netns查看到容器的网络命名空间的，这是怎么做的呢？
这里可以简单看下containerd的代码就清楚了:&lt;/p&gt;

&lt;p&gt;containerd/pkg/cri/sbserver/sandbox_run.go&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;// RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure
// the sandbox is in ready state.
func (c *criService) RunPodSandbox(ctx context.Context, r *runtime.RunPodSandboxRequest) (_ *runtime.RunPodSandboxResponse, retErr error) {
    ...

    // Create initial internal sandbox object.
    sandbox := sandboxstore.NewSandbox(
        sandboxstore.Metadata{
            ID:             id,
            Name:           name,
            Config:         config,
            RuntimeHandler: r.GetRuntimeHandler(),
        },
        sandboxstore.Status{
            State: sandboxstore.StateUnknown,
        },
    )

    if _, err := c.client.SandboxStore().Create(ctx, sandboxInfo); err != nil {
        return nil, fmt.Errorf(&quot;failed to save sandbox metadata: %w&quot;, err)
    }
    ...

    // Setup the network namespace if host networking wasn&apos;t requested.
    if !hostNetwork(config) {
        netStart := time.Now()
        // If it is not in host network namespace then create a namespace and set the sandbox
        // handle. NetNSPath in sandbox metadata and NetNS is non empty only for non host network
        // namespaces. If the pod is in host network namespace then both are empty and should not
        // be used.
        var netnsMountDir = &quot;/var/run/netns&quot;
        if c.config.NetNSMountsUnderStateDir {
            netnsMountDir = filepath.Join(c.config.StateDir, &quot;netns&quot;)
        }
        sandbox.NetNS, err = netns.NewNetNS(netnsMountDir)
        if err != nil {
            return nil, fmt.Errorf(&quot;failed to create network namespace for sandbox %q: %w&quot;, id, err)
        }
        // Update network namespace in the store, which is used to generate the container&apos;s spec
        sandbox.NetNSPath = sandbox.NetNS.GetPath()
        ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到在创建容器时会创建一个sandbox容器用来共享网络命名空间，那就认为只有这个容器会对应一个网络命名空间，这里我们重点看NewNetNS方法的调用，最终调用newNS方法&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func newNS(baseDir string, pid uint32) (nsPath string, err error) {
    b := make([]byte, 16)

    _, err = rand.Read(b)
    if err != nil {
        return &quot;&quot;, fmt.Errorf(&quot;failed to generate random netns name: %w&quot;, err)
    }

    // Create the directory for mounting network namespaces
    // This needs to be a shared mountpoint in case it is mounted in to
    // other namespaces (containers)
    if err := os.MkdirAll(baseDir, 0755); err != nil {
        return &quot;&quot;, err
    }

    // create an empty file at the mount point and fail if it already exists
    nsName := fmt.Sprintf(&quot;cni-%x-%x-%x-%x-%x&quot;, b[0:4], b[4:6], b[6:8], b[8:10], b[10:])
    nsPath = path.Join(baseDir, nsName)
    mountPointFd, err := os.OpenFile(nsPath, os.O_RDWR|os.O_CREATE|os.O_EXCL, 0666)
    if err != nil {
        return &quot;&quot;, err
    }
    mountPointFd.Close()

    defer func() {
        // Ensure the mount point is cleaned up on errors
        if err != nil {
            os.RemoveAll(nsPath)
        }
    }()

    if pid != 0 {
        procNsPath := getNetNSPathFromPID(pid)
        // bind mount the netns onto the mount point. This causes the namespace
        // to persist, even when there are no threads in the ns.
        if err = unix.Mount(procNsPath, nsPath, &quot;none&quot;, unix.MS_BIND, &quot;&quot;); err != nil {
            return &quot;&quot;, fmt.Errorf(&quot;failed to bind mount ns src: %v at %s: %w&quot;, procNsPath, nsPath, err)
        }
        return nsPath, nil
    }

    var wg sync.WaitGroup
    wg.Add(1)

    // do namespace work in a dedicated goroutine, so that we can safely
    // Lock/Unlock OSThread without upsetting the lock/unlock state of
    // the caller of this function
    go (func() {
        defer wg.Done()
        runtime.LockOSThread()
        // Don&apos;t unlock. By not unlocking, golang will kill the OS thread when the
        // goroutine is done (for go1.10+)

        var origNS cnins.NetNS
        origNS, err = cnins.GetNS(getCurrentThreadNetNSPath())
        if err != nil {
            return
        }
        defer origNS.Close()

        // create a new netns on the current thread
        err = unix.Unshare(unix.CLONE_NEWNET)
        if err != nil {
            return
        }

        // Put this thread back to the orig ns, since it might get reused (pre go1.10)
        defer origNS.Set()

        // bind mount the netns from the current thread (from /proc) onto the
        // mount point. This causes the namespace to persist, even when there
        // are no threads in the ns.
        err = unix.Mount(getCurrentThreadNetNSPath(), nsPath, &quot;none&quot;, unix.MS_BIND, &quot;&quot;)
        if err != nil {
            err = fmt.Errorf(&quot;failed to bind mount ns at %s: %w&quot;, nsPath, err)
        }
    })()
    wg.Wait()

    if err != nil {
        return &quot;&quot;, fmt.Errorf(&quot;failed to create namespace: %w&quot;, err)
    }

    return nsPath, nil
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;这里baseDir是/var/run/netns，nsName是随机生成的36位字符串， pid是0&lt;/li&gt;
  &lt;li&gt;getCurrentThreadNetNSPath方法直接返回的是容器的网络命名空间路径：/proc/{pid}/task/{pid}/ns/net&lt;/li&gt;
  &lt;li&gt;最终将proc中网络命名空间mount到nsPath对应位置，和上面我们手动操作的情况类似&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;本次我们从简单介绍Linux名称空间和cgroup开始。然后演示了当我们运行ip netns ls时，docker run创建的网络名称空间文件不显示的问题。
随后解释了这是因为文件引用不是在/var/run/netns创建的，而ip netns ls命令只在这个目录中查找网络名称空间。&lt;/p&gt;

&lt;p&gt;随后，我们以一个简单的修正结束了本文，即将文件绑定挂载到/var/run/netns，这样就可以通过ip netns ls找到它。
最后通过查看containerd的代码发现它也是采用类似方式自动将文件mount到/var/run/netns目录下的，这样就比较方便的通过ip netns来管理容器的网络命名空间了，
这里有个不太方便的地方是查询容器和这个network namespace的对应关系，从代码里也看到是随机生成的uuid&lt;/p&gt;
</description>
        <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2023/04/06/netns/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2023/04/06/netns/</guid>
        
        <category>docker</category>
        
        <category>netns</category>
        
        
      </item>
    
      <item>
        <title>Neuvector源码之 文件管理</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;进程规则、文件规则、网络规则、dlp、waf都是在监控组下的功能。本次我们通过源码的方式来深入了解文件规则是如何实现的。
文件规则支持用户自定义关注的文件、目录，设置规则的学习或者保护模式，相应的如果容器访问到了指定的文件，且文件规则设置保护模式，则对文件的写会产生告警。&lt;/p&gt;

&lt;h2 id=&quot;架构图&quot;&gt;架构图&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_file0.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们从几个维度来看neuvector的文件管理功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;页面/接口用户下发的文件规则如何让每个节点的Agent服务感知的&lt;/li&gt;
  &lt;li&gt;文件规则如何和容器关联起来的，Agent中是如何处理的&lt;/li&gt;
  &lt;li&gt;对文件执行读写操作后Agent如何感知&lt;/li&gt;
  &lt;li&gt;fanotify/inotify怎么处理文件的操作的&lt;/li&gt;
  &lt;li&gt;文件管理怎么体现学习模式下学习&lt;/li&gt;
  &lt;li&gt;文件操作告警信息怎么采集的，告警的规则是什么&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我们带着这几个问题，来看看源码，我们先看第一个问题。&lt;/p&gt;

&lt;h2 id=&quot;源码&quot;&gt;源码&lt;/h2&gt;

&lt;h3 id=&quot;下发的文件规则如何到达每个节点的agent服务&quot;&gt;下发的文件规则如何到达每个节点的Agent服务&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_file1.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们先看看界面新增文件规则的流程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;获取group对象，获取group中profile和rule两组数据&lt;/li&gt;
  &lt;li&gt;profile是文件规则，rule是文件规则中应用规则&lt;/li&gt;
  &lt;li&gt;将新增数据和已有数据合并，然后保存到数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;handlerFileMonitorConfig：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func handlerFileMonitorConfig(w http.ResponseWriter, r *http.Request, ps httprouter.Params) {
    ...
    // Check if we can config the profile. Only need authorize group
    grp, err := cacher.GetGroupBrief(group, false, acc)
    if err != nil {
        restRespNotFoundLogAccessDenied(w, login, err)
        return
    }

    if grp.Kind != share.GroupKindContainer {
        // &quot;nodes&quot; : share.GroupKindNode
        log.WithFields(log.Fields{&quot;group&quot;: group, &quot;kind&quot;: grp.Kind}).Error(&quot;Get profile failed!&quot;)
        restRespError(w, http.StatusBadRequest, api.RESTErrObjectNotFound)
        return
    }

    var profChanged bool
    profConf, profRev := clusHelper.GetFileMonitorProfile(group)
    ruleConf, ruleRev := clusHelper.GetFileAccessRule(group)

    ...
    // validate add
    if config.AddFilters != nil {
        for _, filter := range config.AddFilters {
            path := filter.Filter
            filter.Filter = filepath.Clean(filter.Filter)
            if filter.Filter == &quot;.&quot; || filter.Filter == &quot;/&quot; {
                restRespErrorMessage(w, http.StatusBadRequest, api.RESTErrInvalidRequest,
                    fmt.Sprintf(&quot;Unsupported filter: %s[%s]&quot;, path, filter.Filter))
                return
            }

            // append the &quot;/&quot; back
            if path[len(path)-1:] == &quot;/&quot; {
                filter.Filter += &quot;/&quot;
            }

            base, regex, ok := parseFileFilter(filter.Filter)
            if !ok {
                restRespErrorMessage(w, http.StatusBadRequest, api.RESTErrInvalidRequest,
                    fmt.Sprintf(&quot;Unsupported filter: %s&quot;, filter.Filter))
                return
            }

            for i, cfilter := range profConf.Filters {
                if cfilter.Filter == filter.Filter {
                    // conflict, delete predefined
                    if !cfilter.CustomerAdd {
                        profConf.Filters = append(profConf.Filters[:i], profConf.Filters[i+1:]...)
                        // replace the rule below
                        idx := utils.FilterIndexKey(cfilter.Path, cfilter.Regex)
                        delete(ruleConf.Filters, idx)
                        break
                    } else {
                        restRespErrorMessage(w, http.StatusBadRequest, api.RESTErrInvalidRequest,
                            fmt.Sprintf(&quot;duplicate filter: %s&quot;, filter.Filter))
                        return
                    }
                }
            }
            flt := share.CLUSFileMonitorFilter{
                Filter:      filter.Filter,
                Path:        base,
                Regex:       regex,
                Recursive:   filter.Recursive,
                CustomerAdd: true,
            }
            if fileAccessOptionSet.Contains(filter.Behavior) {
                flt.Behavior = filter.Behavior
            } else {
                restRespErrorMessage(w, http.StatusBadRequest, api.RESTErrInvalidRequest, &quot;Invalid File access option&quot;)
                return
            }

            profConf.Filters = append(profConf.Filters, flt)
            // add rule
            idx := utils.FilterIndexKey(flt.Path, flt.Regex)
            capps := make([]string, len(filter.Apps))
            for j, app := range filter.Apps {
                capps[j] = app
            }
            frule := &amp;amp;share.CLUSFileAccessFilterRule{
                Apps:        capps,
                CreatedAt:   tm,
                UpdatedAt:   tm,
                Behavior:    flt.Behavior,
                CustomerAdd: true,
            }
            ruleConf.Filters[idx] = frule
            profChanged = true
        }
    }

    ...

    if profChanged {
        // Write to cluster
        if err := clusHelper.PutFileMonitorProfile(group, profConf, profRev); err != nil {
            log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Write cluster fail&quot;)
            restRespError(w, http.StatusInternalServerError, api.RESTErrFailWriteCluster)
            return
        }
    }
    // Write access rule
    if err := clusHelper.PutFileAccessRule(group, ruleConf, ruleRev); err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Write cluster fail&quot;)
        restRespError(w, http.StatusInternalServerError, api.RESTErrFailWriteCluster)
        return
    }

    ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;看看数据怎么保存的，这里有个小坑：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;默认数据保存在/object/config/file_monitor/&lt;group-name&gt;下面&lt;/group-name&gt;&lt;/li&gt;
  &lt;li&gt;在保存数据前还有一个DuplicateNetworkKey操作，会将数据保存一份到/node/[node-id]/common/profile/file/&lt;group-name&gt;下面&lt;/group-name&gt;&lt;/li&gt;
  &lt;li&gt;后面那个保存位置的数据会被每个enforcer watch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PutFileMonitorProfile:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (m clusterHelper) PutFileMonitorProfile(name string, conf *share.CLUSFileMonitorProfile, rev uint64) error {
    key := share.CLUSFileMonitorKey(name)
    value, _ := json.Marshal(conf)
    m.DuplicateNetworkKey(key, value)
    return cluster.PutRev(key, value, rev)
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面看了controller中数据保存流程，下面来看看enforcer中数据watch流程&lt;/p&gt;

&lt;p&gt;和其他功能类似，肯定有个地方在watch这个数据变化，然后执行一些操作,这里先看file_monitor部分的逻辑（access_rule是类似的）&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func profileDerivedProc(nType cluster.ClusterNotifyType, key string, value []byte) {
    which := share.CLUSNetworkKey2Subject(key)
    value, _ = utils.UnzipDataIfValid(value)
    // log.WithFields(log.Fields{&quot;key&quot;: key}).Debug(&quot;GRP:&quot;)
    switch which {
    case share.ProfileGroup:                     // group
        systemConfigGroup(nType, key, value)
    case share.ProfileProcess:                   // process
        profileConfigGroup(nType, key, value)
    case share.ProfileFileMonitor:               // file
        systemConfigFileMonitor(nType, key, value)  
    case share.ProfileFileAccess:                // fileAccess
        systemConfigFileAccessRule(nType, key, value)
    case share.ProfileScript:                    // script, 这个在数据库里没找到
        systemConfigScript(nType, key, value)
    default:
        log.WithFields(log.Fields{&quot;derived&quot;: which}).Debug(&quot;Miss handler&quot;)
    }
}

func systemConfigFileMonitor(nType cluster.ClusterNotifyType, key string, value []byte) {
  switch nType {
  case cluster.ClusterNotifyAdd, cluster.ClusterNotifyModify:
      ...
      updateGroupProfileCache(nType, name, profile)   // 这里重点看下这个函数
  case cluster.ClusterNotifyDelete: // required no group member that means no belonged containers, either
  }   }
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;文件规则和监控组容器关联&quot;&gt;文件规则和监控组、容器关联&lt;/h3&gt;

&lt;p&gt;用户规则创建后保存到数据库，enforcer watch到数据变化会更新本地内存数据&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;对比内存数据，不一致则更新&lt;/li&gt;
  &lt;li&gt;targets是当前group中容器id列表&lt;/li&gt;
  &lt;li&gt;可以看到grpNotifyFile是当前节点所有容器id的集合&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;updateGroupProfileCache:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func updateGroupProfileCache(nType cluster.ClusterNotifyType, name string, obj interface{}) bool {
    ...
    targets := utils.NewSet()
    switch obj.(type) {
    ...
    case share.CLUSFileMonitorProfile:
        file := obj.(share.CLUSFileMonitorProfile)
        if file.Mode != grpCache.file.Mode || len(grpCache.file.Filters) == 0 || reflect.DeepEqual(file.Filters, grpCache.file.Filters) == false {
            for i, _ := range file.Filters {
                file.Filters[i].DerivedGroup = name // late filled-up to save kv storages
            }
            grpCache.file = &amp;amp;file
            targets = grpCache.members.Clone()
            if targets.Cardinality() &amp;gt; 0 {
                fileUpdated = true
            }
        }
    ...

    if fileUpdated {
        grpNotifyFile = grpNotifyFile.Union(targets)
    }

    targets.Clear()
    targets = nil
    return true
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;看到这里有点好奇，group和容器是怎么关联上的，也就是上面代码中grpCache.members在哪里维护的：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在enforcer中监听runtime事件时，会监听容器的创建事件，对应的回调函数有个groupWorkloadJoin&lt;/li&gt;
  &lt;li&gt;在groupWorkloadJoin中根据workload的learnedGroupName获取对应的系统组，然后加入&lt;/li&gt;
  &lt;li&gt;用户自定义组根据Criteria和domain来识别是否包含此workload&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;registerEventHandlers&amp;amp;groupWorkloadJoin:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func registerEventHandlers() {
    ...
    evhdls.Register(EV_WORKLOAD_START, []eventHandlerFunc{
        groupWorkloadJoin,
        scanWorkloadAdd,
    })
    ...


func groupWorkloadJoin(id string, param interface{}) {
    ...
    if cache, ok := groupCacheMap[wlc.learnedGroupName]; !ok || isDummyGroupCache(cache) {
        ...
    } else {
        if !cache.members.Contains(wl.ID) {
            wlc.groups.Add(wlc.learnedGroupName)
            cache.members.Add(wl.ID)
            memberUpdated = true
            log.WithFields(log.Fields{&quot;group&quot;: wlc.learnedGroupName}).Debug(&quot;Join group&quot;)
        }
    }

    // Join user defined group
    for _, cache := range groupCacheMap {
        if cache.group.CfgType == share.Learned {
            continue
        }

        if share.IsGroupMember(cache.group, wlc.workload) {
            if !cache.members.Contains(wl.ID) {
                wlc.groups.Add(cache.group.Name)
                cache.members.Add(wl.ID)
                memberUpdated = true
                log.WithFields(log.Fields{&quot;group&quot;: cache.group.Name}).Debug(&quot;Join group&quot;)
            }
    ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;到这里我们可以看到内存中的数据一直在实时更新，监控组和容器的关系也建立好了，文件规则也更新到了指定组对象了，如何使用这些数据呢，我们继续往下看&lt;/p&gt;

&lt;h3 id=&quot;定时更新fanotify关注的容器文件&quot;&gt;定时更新fanotify关注的容器文件&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_file2.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;enforcer中创建了一个定时任务，每隔5s执行一次，遍历所有容器，将内存中的文件规则应用到的fanotify中&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func fileMemberChanges(members utils.Set) {
    log.WithFields(log.Fields{&quot;count&quot;: members.Cardinality()}).Debug(&quot;GRP:&quot;)
    for cid := range members.Iter() {
        ...
        c, ok := gInfo.activeContainers[id]
        gInfoRUnlock()
        if ok {
            applyFileGroupProfile(c)
        } ...
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在applyFileGroupProfile中涉及三个主要流程：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;calculateFileGroupProfile： 根据用户创建的文件规则筛选出所有相关file，根据监控组模式设置file的mask属性&lt;/li&gt;
  &lt;li&gt;更新容器matchRules：将计算到的file/access规则添加到workload对象中（纯数据转换处理逻辑）&lt;/li&gt;
  &lt;li&gt;StartWatch：将file添加到fanotify中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;calculateFileGroupProfile就是获取内存中group的file和access规则，
getFileMonitorProfile会加载数据库中的数据，如果内存不存在指定的group信息时&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func calculateFileGroupProfile(id, svc string) (*share.CLUSFileMonitorProfile, *share.CLUSFileAccessRule, bool) {
    log.WithFields(log.Fields{&quot;id&quot;: id, &quot;svc&quot;: svc}).Debug(&quot;GRP: &quot;)

    file := &amp;amp;share.CLUSFileMonitorProfile{
        Filters:    make([]share.CLUSFileMonitorFilter, 0),
        FiltersCRD: make([]share.CLUSFileMonitorFilter, 0),
    }
    ...
    for _, grpCache := range grpProfileCacheMap {
        if grpCache.members.Contains(id) {
            file.Filters = append(file.Filters, grpCache.file.Filters...)
            file.FiltersCRD = append(file.FiltersCRD, grpCache.file.FiltersCRD...)
            mergeFileAccessProfile(access, grpCache.access)
        }
    }
    grpCacheLock.Unlock()

    // log.WithFields(log.Fields{&quot;filter&quot;: file.Filters}).Debug(&quot;GRP:&quot;)
    ok, svc_file := getFileMonitorProfile(svc)
    if !ok {
        log.WithFields(log.Fields{&quot;id&quot;: id, &quot;svc&quot;: svc}).Debug(&quot;GRP: no file profile&quot;)
        return nil, nil, false
    }

    // basic information
    file.Group = svc_file.Group
    file.Mode = svc_file.Mode

    // merge regular files
    file.Filters = append(file.Filters, svc_file.Filters...)
    file.Filters = mergeFileMonitorProfile(file.Filters)
    ...
    return file, access, true
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;我们重点看下StartWatch方法&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) StartWatch(id string, rootPid int, conf *FsmonConfig, capBlock, bNeuvectorSvc bool) {
    ...
    dirs, files := w.getCoreFile(id, rootPid, conf.Profile)

    w.fanotifier.SetMode(rootPid, access, perm, capBlock, bNeuvectorSvc)

    w.addCoreFile(id, dirs, files)

    w.fanotifier.StartMonitor(rootPid)
    ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;getCoreFile: 会根据用户下发的文件规则（path可能带*）会获取所有相关的目录和文件路径&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) getCoreFile(cid string, pid int, profile *share.CLUSFileMonitorProfile) (map[string]*osutil.FileInfoExt, []*osutil.FileInfoExt) {
    dirList := make(map[string]*osutil.FileInfoExt)
    singleFiles := make([]*osutil.FileInfoExt, 0)

    // get files and dirs from all filters
    for _, filter := range profile.Filters {
        flt := &amp;amp;filterRegex{path: filterIndexKey(filter)}
        flt.regex, _ = regexp.Compile(fmt.Sprintf(&quot;^%s$&quot;, flt.path))
        bBlockAccess := filter.Behavior == share.FileAccessBehaviorBlock
        bUserAdded := filter.CustomerAdd
        if strings.Contains(filter.Path, &quot;*&quot;) {
            subDirs := w.getSubDirList(pid, filter.Path, cid)
            for _, sub := range subDirs {
                singles := w.getDirAndFileList(pid, sub, filter.Regex, cid, flt, filter.Recursive, bBlockAccess, bUserAdded, dirList)
                singleFiles = append(singleFiles, singles...)
            }
        } else {
            singles := w.getDirAndFileList(pid, filter.Path, filter.Regex, cid, flt, filter.Recursive, bBlockAccess, bUserAdded, dirList)
            singleFiles = append(singleFiles, singles...)
        }
    }

    ...
    return dirList, singleFiles
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;SetMode：内存中创建rootFd对象，需要注意一点如果监控组是保护模式会设置permControl为true，这个在后面处理文件事件会用到&lt;/p&gt;

&lt;p&gt;addCoreFile&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;向fanotify注册需要关注的文件列表，以及设置文件mask，以addFile为例&lt;/li&gt;
  &lt;li&gt;这里如果监控path是包管理路径会额外再调用inotify来监控&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;addCoreFile&amp;amp;addFile:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) addCoreFile(cid string, dirList map[string]*osutil.FileInfoExt, singleFiles []*osutil.FileInfoExt) {
    // add files
    for _, finfo := range singleFiles {
        // need to move the cross link files to dirs
        di, ok := dirList[filepath.Dir(finfo.Path)]
        if ok &amp;amp;&amp;amp; !isRunTimeAddedFile(finfo.Path) {
            finfo.Filter = di.Filter
            di.Children = append(di.Children, finfo)
        } else {
            finfo.ContainerId = cid
            w.addFile(finfo)
        }
    }

    // add directories
    ...
} 


func (w *FileWatch) addFile(finfo *osutil.FileInfoExt) {
    w.fanotifier.AddMonitorFile(finfo.Path, finfo.Filter, finfo.Protect, finfo.UserAdded, w.cbNotify, finfo)
    if _, path := global.SYS.ParseContainerFilePath(finfo.Path); packageFile.Contains(path) {
        w.inotifier.AddMonitorFile(finfo.Path, w.cbNotify, finfo)
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;fanotify的addFile&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;path形如：/host/proc/17490/root/usr/bin，解析到容器的pid和操作的文件路径&lt;/li&gt;
  &lt;li&gt;fn.roots[rootPid]得到容器的root fd&lt;/li&gt;
  &lt;li&gt;这里注意mask的取值逻辑：userAdded/protect基本都符合可以忽略，permControl表示监控组是保护模式，configPerm正常是true，可以理解成监控组保护模式下默认给文件设置mask是FAN_OPEN_PERM，其他情况都是FAN_OPEN，这个需要结合后面fanotify事件的处理流程一起看&lt;/li&gt;
  &lt;li&gt;这样就将所有需要监听的文件权限、路径、容器id等所有相关的元数据都处理好了，后面就是实际应用到fanotify中了&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;addFile&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (fn *FaNotify) addFile(path string, filter interface{}, protect, isDir, userAdded bool, files map[string]interface{}, cb NotifyCallback, params interface{}) bool {
    ...
    rootPid, rPath, err := ParseMonitorPath(path)
    ...
    r, ok := fn.roots[rootPid]
    ...

    var mask uint64 = faMarkMask
    if userAdded || protect { // user-defined or protected: including access control
        if r.permControl { // protect mode
            if fn.configPerm { // system-wise : access control is available
                mask |= FAN_OPEN_PERM
            } else {
                mask |= FAN_OPEN
            }
        } else {
            mask |= FAN_OPEN
        }
    }

    var file *IFile
    if isDir {
        ...

    } else {
        if _, ok = r.paths[rPath]; ok {
            return false
        }
        file = &amp;amp;IFile{
            path:    path,
            mask:    mask,
            params:  params,
            cb:      cb,
            filter:  filter.(*filterRegex),
            protect: protect,         // access control
            learnt:  r.accessMonitor, // discover mode
            userAdd: userAdded,
        }

        r.paths[rPath] = file
    }
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;fa.StartMonitor真正将文件规则应用到fanotify&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;这里的Mark是调用fanotify，给出需要监控的文件路径、文件事件、权限&lt;/li&gt;
  &lt;li&gt;addHostNetworkFilesCopiedFiles是将hostnetwork的容器中一些通用的文件（/etc/hosts /etc/resolv.conf）进行监控&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;StartMonitor:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (fn *FaNotify) StartMonitor(rootPid int) bool {
    ...
    r, ok := fn.roots[rootPid]
    ...

    ppath := fmt.Sprintf(procRootMountPoint, rootPid)
    for dir, mask := range r.dirMonitorMap {
        path := ppath + dir
        if err := fn.fa.Mark(faMarkAddFlags, mask, unix.AT_FDCWD, path); err != nil {
            log.WithFields(log.Fields{&quot;path&quot;: path, &quot;error&quot;: err}).Error(&quot;FMON:&quot;)
        } else {
            mLog.WithFields(log.Fields{&quot;path&quot;: path, &quot;mask&quot;: fmt.Sprintf(&quot;0x%08x&quot;, mask)}).Debug(&quot;FMON:&quot;)
        }
    }

    //
    fn.addHostNetworkFilesCopiedFiles(r)
    return ok
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;到这里已经将用户下发的文件规则，关联到group，关联到具体的容器（workload）中，同时也将用户规则进行重计算，得到一个完整的文件集合，
将需要关注（监控）的文件以及事件等细节都告诉了fanotify和inotify，下面就看看实际watch到文件事件后如何处理了&lt;/p&gt;

&lt;h3 id=&quot;文件操作的感知&quot;&gt;文件操作的感知&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_file3.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面直接调用fn.fa.Mark去告诉fanotify需要关注的文件，但是这个对象哪里来的？&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;NewFaNotify: 初始化fanotify&lt;/li&gt;
  &lt;li&gt;NewInotify: 初始化inotify&lt;/li&gt;
  &lt;li&gt;MonitorFileEvents：监听来自fanotify和inotify的文件事件并处理&lt;/li&gt;
  &lt;li&gt;fw.loop：这个后面我们讲&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NewFileWatcher:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func NewFileWatcher(config *FileMonitorConfig) (*FileWatch, error) {
    ...

    n, err := NewFaNotify(config.EndChan, config.PidLookup, global.SYS)
    if err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Open fanotify fail&quot;)
        return nil, err
    }
    ni, err := NewInotify()
    if err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Open inotify fail&quot;)
        return nil, err
    }

    go n.MonitorFileEvents()
    go ni.MonitorFileEvents()

    fw := &amp;amp;FileWatch{
        aufs:       config.IsAufs,
        fanotifier: n,
        inotifier:  ni,
        fileEvents: make(map[string]*fileMod),
        groups:     make(map[int]*groupInfo),
        sendrpt:    config.SendReport,
        sendRule:   config.SendAccessRule,
        estRuleSrc: config.EstRule,
        walkerTask: config.WalkerTask,
    }
    go fw.loop()
    ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;文件事件处理：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在获取到文件事件后，获取事件中进程pid、容器的root fd、文件mask&lt;/li&gt;
  &lt;li&gt;这里如果文件fmask是FAN_OPEN_PERM，则perm是1。前面提过如果监控组是保护模式，则会设置对应文件的mask是FAN_OPEN_PERM&lt;/li&gt;
  &lt;li&gt;resp：用来给fanotify回复的结果，计算方式见下文的流程图&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;handleEvents:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (fn *FaNotify) handleEvents() error {
    for {
        ev, err := fn.fa.GetEvent()
        ...
        pid := int(ev.Pid)
        fd := int(ev.File.Fd())
        fmask := uint64(ev.Mask)
        perm := (fmask &amp;amp; (FAN_OPEN_PERM | FAN_ACCESS_PERM)) &amp;gt; 0
        ...
        resp, mask, ifile, pInfo := fn.calculateResponse(pid, fd, fmask, perm)
        if perm {
            fn.fa.Response(ev, resp)
        }
        ev.File.Close()
        ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_file4.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;resp的值默认是true&lt;/li&gt;
  &lt;li&gt;如果文件规则是保护模式，且操作进程不在允许的应用中，则resp是false&lt;/li&gt;
  &lt;li&gt;将fmask转换成mask（一个操作会有多个事件，只有第一个事件中fmask是设置过的）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;calculateResponse:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (fn *FaNotify) calculateResponse(pid, fd int, fmask uint64, perm bool) (bool, uint32, *IFile, *ProcInfo) {
    ...

    ifile, _, mask := fn.lookupFile(r, linkPath, pInfo)
    if ifile == nil {
        return true, mask, nil, nil
    }

    // log.WithFields(log.Fields{&quot;protect&quot;: ifile.protect, &quot;perm&quot;: perm, &quot;path&quot;: linkPath, &quot;ifile&quot;: ifile, &quot;evMask&quot;: fmt.Sprintf(&quot;0x%08x&quot;, fmask)}).Debug(&quot;FMON:&quot;)

    // permition decision
    resp := true
    if ifile.protect { // always verify app for block-access
        resp = fn.lookupRule(r, ifile, pInfo, linkPath)
        // log.WithFields(log.Fields{&quot;resp&quot;: resp}).Debug(&quot;FMON:&quot;)
    }

    if (fmask &amp;amp; FAN_MODIFY) &amp;gt; 0 {
        mask |= syscall.IN_MODIFY
        log.WithFields(log.Fields{&quot;path&quot;: linkPath}).Info(&quot;FMON: modified&quot;)
    } else if (fmask &amp;amp; FAN_CLOSE_WRITE) &amp;gt; 0 {
        mask |= syscall.IN_CLOSE_WRITE
        log.WithFields(log.Fields{&quot;path&quot;: linkPath}).Info(&quot;FMON: cls_wr&quot;)
    } else {
        mask |= syscall.IN_ACCESS
            log.WithFields(log.Fields{&quot;path&quot;: linkPath}).Info(&quot;FMON: read&quot;)
        if fn.isFileException(false, linkPath, pInfo, mask) {
            resp = true
            mask &amp;amp;^= syscall.IN_ACCESS
        }
    }

    if perm &amp;amp;&amp;amp; !resp {
        pInfo.Deny = true
        log.WithFields(log.Fields{&quot;path&quot;: linkPath, &quot;app&quot;: pInfo.Path}).Debug(&quot;FMON: denied&quot;)
    }
    return resp, mask, ifile, pInfo
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;综合看一下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_file5.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;只有监控组是保护模式的时候perm才是true&lt;/li&gt;
  &lt;li&gt;perm是true的时候才会给fanotify发送response，其他时候都不会发送&lt;/li&gt;
  &lt;li&gt;只有文件规则是保护模式的时候resp才是false，其他时候都是true&lt;/li&gt;
  &lt;li&gt;换句话说：只有监控组是保护模式且文件规则是保护模式的时候fanotify才会阻断文件操作，其他都是允许&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;规则学习--告警信息上报&quot;&gt;规则学习 &amp;amp; 告警信息上报&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_file6.png&quot; alt=&quot;neuvector file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;前面看了文件事件处理流程，但是我们忽略了一个细节&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;这里有个change参数，用来判断文件是否被修改&lt;/li&gt;
  &lt;li&gt;当fmask是FAN_CLOSE_WRITE表示文件被修改&lt;/li&gt;
  &lt;li&gt;当文件规则是学习模式，或者保护模式下修改了文件 事件都需要report&lt;/li&gt;
  &lt;li&gt;文件被修改或者需要上报都需要调用回调函数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;handleEvents:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (fn *FaNotify) handleEvents() error {
    for {
        ev, err := fn.fa.GetEvent()
        ...
        change := (fmask &amp;amp; FAN_CLOSE_WRITE) &amp;gt; 0
        // log.WithFields(log.Fields{&quot;ifile&quot;: ifile, &quot;pInfo&quot;: pInfo, &quot;Resp&quot;: resp, &quot;Change&quot;: change, &quot;Perm&quot;: perm}).Debug(&quot;FMON:&quot;)

        var bReporting bool
        if ifile.learnt { // discover mode
            bReporting = ifile.userAdd // learn app for customer-added entry
        } else { // monitor or protect mode
            allowRead := resp &amp;amp;&amp;amp; !change
            bReporting = (allowRead == false) // allowed app by block_access
        }

        if bReporting || change { // report changed file
            ifile.cb(ifile.path, mask, ifile.params, pInfo)
        }
    }
    return nil
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;回调函数： 只是将事件更新或者保存到内存中，那事件在哪里处理的呢？&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) cbNotify(filePath string, mask uint32, params interface{}, pInfo *ProcInfo) {
    //ignore the container remove event. they are too many
    if (mask&amp;amp;syscall.IN_IGNORED) != 0 || (mask&amp;amp;syscall.IN_UNMOUNT) != 0 {
        w.inotifier.RemoveMonitorFile(filePath)
        return
    }

    w.mux.Lock()
    defer w.mux.Unlock()
    if fm, ok := w.fileEvents[filePath]; ok {
        fm.mask |= mask
        fm.delay = 0
        fm.pInfo = append(fm.pInfo, pInfo)
    } else {
        pi := make([]*ProcInfo, 1)
        pi[0] = pInfo
        w.fileEvents[filePath] = &amp;amp;fileMod{
            mask:  mask,
            delay: 0,
            finfo: params.(*osutil.FileInfoExt),
            pInfo: pi,
        }
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里在enforcer启动时创建了两个定时任务，分别用来处理事件和学习规则用:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;HandleWatchedFiles会根据路径类型（文件还是目录）调用对应的处理方法&lt;/li&gt;
  &lt;li&gt;reportLearningRules:后面细讲&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HandleWatchedFiles:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) HandleWatchedFiles() {
    events := make(map[string]*fileMod)
    w.mux.Lock()
    for filePath, fmod := range w.fileEvents {
        events[filePath] = fmod
        delete(w.fileEvents, filePath)
    }
    w.mux.Unlock()

    for fullPath, fmod := range events {
        pid, path := global.SYS.ParseContainerFilePath(fullPath)
        //to avoid false alarm of /etc/hosts and /etc/resolv.conf, check whether the container is still exist
        //these two files has attribute changed when the container leave
        //this maybe miss some events file changed right before container leave. But for these kind of event,
        //it is not useful if the container already leave
        //	log.WithFields(log.Fields{&quot;pid&quot;: pid, &quot;path&quot;: path, &quot;pInfo&quot;: fmod.pInfo[0], &quot;fInfo&quot;: fmod.finfo}).Debug(&quot;FMON:&quot;)
        //	if fmod.pInfo != nil {
        //		log.WithFields(log.Fields{&quot;pInfo&quot;: fmod.pInfo[0]}).Debug(&quot;FMON:&quot;)
        //	}
        rootPath := global.SYS.ContainerProcFilePath(pid, &quot;&quot;)
        if _, err := os.Stat(rootPath); err == nil &amp;amp;&amp;amp; path != &quot;&quot; {
            var event uint32
            info, _ := os.Lstat(fullPath)
            if fmod.finfo.FileMode.IsDir() {
                event = w.handleDirEvents(fmod, info, fullPath, path, pid)
            } else {
                event = w.handleFileEvents(fmod, info, fullPath, pid)
            }
            if event != 0 {
                w.learnFromEvents(pid, fmod, path, event)
            }
        }
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里我们看看文件事件处理逻辑:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;如果用户删除文件规则后，会更新enforcer内存数据，也就是将fileinfo删除，这里的info就是nil了，也就需要让fanotify知道不再需要关注这些文件了，执行RemoveMonitorFile方法&lt;/li&gt;
  &lt;li&gt;这里的event是文件操作类型参数，不为空时调用learnFromEvents&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;handleFileEvents:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) handleFileEvents(fmod *fileMod, info os.FileInfo, fullPath string, pid int) uint32 {
    var event uint32
    if info != nil {
        if info.Mode() != fmod.finfo.FileMode {
            //attribute is changed
            event = fileEventAttr
            fmod.finfo.FileMode = info.Mode()
        }
        // check the hash existing and match
        // skip directory new file event, report later
        hash, err := osutil.GetFileHash(fullPath)
        if err != nil &amp;amp;&amp;amp; !osutil.HashZero(fmod.finfo.Hash) ||
            err == nil &amp;amp;&amp;amp; hash != fmod.finfo.Hash ||
            fmod.finfo.Size != info.Size() {
            event |= fileEventModified
            fmod.finfo.Hash = hash
        } else if (fmod.mask &amp;amp; syscall.IN_ACCESS) &amp;gt; 0 {
            event |= fileEventAccessed
        }
        if (fmod.finfo.FileMode &amp;amp; os.ModeSymlink) != 0 {
            //handle symlink
            rpath, err := osutil.GetContainerRealFilePath(pid, fullPath)
            if err == nil &amp;amp;&amp;amp; fmod.finfo.Link != rpath {
                event |= fileEventSymModified
            }
        }
        if (fmod.mask &amp;amp; inodeChangeMask) &amp;gt; 0 {
            w.removeFile(fullPath)
            w.addFile(fmod.finfo)
        }
    } else {
        //file is removed
        event = fileEventRemoved
        w.fanotifier.RemoveMonitorFile(fullPath)
    }
    return event
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;规则学习：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;如果监控组是学习模式，且进程访问的文件path匹配到文件规则，会将进程path保存到监控组的learnRules中（这个数据后面还有一个定时任务来处理）&lt;/li&gt;
  &lt;li&gt;通过最后的判断条件可以看出：非文件访问或者非学习模式下才发告警&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;learnFromEvents:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) learnFromEvents(rootPid int, fmod *fileMod, path string, event uint32) {
    ...
    grp, ok := w.groups[rootPid]
    mode := grp.mode
    if mode == share.PolicyModeLearn {
        flt := fmod.finfo.Filter.(*filterRegex)
        if applyRules, ok := grp.applyRules[flt.path]; ok {
            learnRules, ok := grp.learnRules[flt.path]
            if !ok {
                learnRules = utils.NewSet()
            }
            for _, pf := range fmod.pInfo {
                // only use the process name/path as profile
                if pf != nil &amp;amp;&amp;amp; pf.Path != &quot;&quot; {
                    if !applyRules.Contains(pf.Path) &amp;amp;&amp;amp; !learnRules.Contains(pf.Path) {
                        learnRules.Add(pf.Path)
                        log.WithFields(log.Fields{&quot;rule&quot;: pf.Path, &quot;filter&quot;: flt}).Debug(&quot;FMON:&quot;)
                    }
                }
            }
            // for inotify, cannot learn
            if learnRules.Cardinality() &amp;gt; 0 {
                grp.learnRules[flt.path] = learnRules
            }
        } else {
            log.WithFields(log.Fields{&quot;path&quot;: path}).Debug(&quot;FMON: no access rules&quot;)
        }
    }
    w.mux.Unlock()

    if event != fileEventAccessed ||
        (mode == share.PolicyModeEnforce || mode == share.PolicyModeEvaluate) {
        w.sendMsg(fmod.finfo.ContainerId, path, event, fmod.pInfo, mode)
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;规则学习&quot;&gt;规则学习&lt;/h3&gt;
&lt;p&gt;这里有个注意点，学习模式下文件管理和进程管理工作模式有点区别：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;进程管理会自动学习新的进程规则，并添加到数据库&lt;/li&gt;
  &lt;li&gt;文件管理的规则必须手动创建，只会自动学习应用并添加到对应的规则中（规则允许的应用属性）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在enforcer中还有一个定时任务，用来处理内存中学习到的文件应用数据&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (w *FileWatch) reportLearningRules() {
    learnRules := make([]*share.CLUSFileAccessRuleReq, 0)
    w.mux.Lock()
    for _, grp := range w.groups {
        if len(grp.learnRules) &amp;gt; 0 {
            for flt, rule := range grp.learnRules {
                for itr := range rule.Iter() {
                    prf := itr.(string)
                    rl := &amp;amp;share.CLUSFileAccessRuleReq{
                        GroupName: grp.profile.Group,
                        Filter:    flt,
                        Path:      prf,
                    }
                    learnRules = append(learnRules, rl)
                }
            }
            grp.learnRules = make(map[string]utils.Set)
        }
    }
    w.mux.Unlock()
    if len(learnRules) &amp;gt; 0 {
        w.sendRule(learnRules)
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到通过grpc调用ReportFileAccessRule将学习到的应用发送给controller&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func sendLearnedFileAccessRule(rules []*share.CLUSFileAccessRuleReq) error {
    log.WithFields(log.Fields{&quot;rules&quot;: len(rules)}).Debug(&quot;&quot;)
    client, err := getControllerServiceClient()
    if err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Failed to find ctrl client&quot;)
        return fmt.Errorf(&quot;Fail to find controller client&quot;)
    }

    ctx, cancel := context.WithTimeout(context.Background(), time.Second*3)
    defer cancel()

    ruleArray := &amp;amp;share.CLUSFileAccessRuleArray{
        Rules: rules,
    }

    _, err = client.ReportFileAccessRule(ctx, ruleArray)
    if err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Debug(&quot;Fail to report file rule to controller&quot;)
        return fmt.Errorf(&quot;Fail to report file rule to controller&quot;)
    }
    return nil
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在controller中有个定时任务来处理学习到的文件规则，有兴趣的可以再看下代码，最终就是调用PutFileAccessRule保存到数据库&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func FileReportBkgSvc() {
    for {
        if len(chanFileRules) &amp;gt; 0 {
            if kv.IsImporting() {
                for i := 0; i &amp;lt; len(chanFileRules); i++ {
                    &amp;lt;-chanFileRules
                }
            } else {
                if lock, _ := clusHelper.AcquireLock(share.CLUSLockPolicyKey, policyClusterLockWait); lock != nil {
                    for i := 0; i &amp;lt; len(chanFileRules) &amp;amp;&amp;amp; i &amp;lt; 16; i++ {
                        rules := &amp;lt;-chanFileRules
                        updateFileMonitorProfile(rules)
                    }
                    clusHelper.ReleaseLock(lock)
                }
            }
        } else {
            time.Sleep(time.Millisecond * 100) // yield
        }
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;我们从几位维度去看待文件管理这个功能，了解了文件规则如何下发到enforcer，enforcer如何监控当前节点所有容器的文件操作，文件事件如何处理，
文件学习如何实现。最终呈现在我们面前的文件管理是这样的：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;对象&lt;/th&gt;
      &lt;th&gt;模式&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;监控组&lt;/td&gt;
      &lt;td&gt;学习&lt;/td&gt;
      &lt;td&gt;学习&lt;/td&gt;
      &lt;td&gt;告警&lt;/td&gt;
      &lt;td&gt;告警&lt;/td&gt;
      &lt;td&gt;保护&lt;/td&gt;
      &lt;td&gt;保护&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;文件规则&lt;/td&gt;
      &lt;td&gt;告警&lt;/td&gt;
      &lt;td&gt;保护&lt;/td&gt;
      &lt;td&gt;告警&lt;/td&gt;
      &lt;td&gt;保护&lt;/td&gt;
      &lt;td&gt;告警&lt;/td&gt;
      &lt;td&gt;保护&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;读告警&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;写告警&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;阻断&amp;amp;告警&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;否&lt;/td&gt;
      &lt;td&gt;是&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;进程管理和文件管理的代码流程基本一致，可以参考来看&lt;/p&gt;

&lt;p&gt;以上是代码逻辑以及一些个人理解，有问题的地方可以及时微信联系我更正&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2023/03/14/neuvector-file/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2023/03/14/neuvector-file/</guid>
        
        <category>neuvector</category>
        
        <category>文件管理</category>
        
        <category>fanotify</category>
        
        <category>inotify</category>
        
        
      </item>
    
      <item>
        <title>Calico单节点网络实现</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;最近有跟同事讨论calico的网络实现的话题，说calico 设计很巧妙，把 2-3层都给处理成了3层，想看看是如何实现的&lt;/p&gt;

&lt;h2 id=&quot;实现&quot;&gt;实现&lt;/h2&gt;
&lt;p&gt;查看官方文档，发现有几个解释，通过这几个解释我们实际看看Calico中如何实现将2-3层流量都处理成3层的&lt;/p&gt;

&lt;h3 id=&quot;问题一-为什么容器中有一条到16925411的路由规则&quot;&gt;问题一： 为什么容器中有一条到169.254.1.1的路由规则&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Why does my container have a route to 169.254.1.1?

In a Calico network, each host acts as a gateway router for the workloads that it hosts. In container deployments, Calico uses 169.254.1.1 as the address for the Calico router. By using a link-local address, Calico saves precious IP addresses and avoids burdening the user with configuring a suitable address.
While the routing table may look a little odd to someone who is used to configuring LAN networking, using explicit routes rather than subnet-local gateways is fairly common in WAN networking.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;简单翻译：在Calico网络中，每台主机都充当容器的网关（路由器）。在容器部署中，Calico使用169.254.1.1作为网关路由器的地址。
通过使用link-local地址，Calico节省了宝贵的IP地址，并避免了用户配置合适地址的负担。
虽然对于习惯于配置LAN网络的人来说，路由表可能看起来有点奇怪，但在WAN网络中，使用显式路由而不是子网本地网关是相当常见的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们看看容器中的网卡：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node2 ~]# docker exec -it d3d044626d12 sh
# ip a 
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: tunl0@NONE: &amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if55: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1480 qdisc noqueue state UP group default 
    link/ether d6:84:ed:49:d4:cf brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 11.244.104.3/32 brd 11.244.104.3 scope global eth0
       valid_lft forever preferred_lft forever
# ip r
default via 169.254.1.1 dev eth0 
169.254.1.1 dev eth0 scope link 
# exit
[root@node2 ~]# ip a |grep 55
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 655
36 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 179.20.23.41/24 brd 179.20.23.255 scope global noprefixroute ens160
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
55: calic4eb42dc79d@if4: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1480 qdisc noqueue state UP group default
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;容器中有一张eth0网卡，ip是11.244.104.3，对应的另一端veth设备是calic4eb42dc79d&lt;/li&gt;
  &lt;li&gt;容器中只有两条路由规则，分别是默认路由和一条32位路由&lt;/li&gt;
  &lt;li&gt;下一跳是169.254.1.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从上面容器内的情况可以看到，任何容器流量都是发送给169.254.1.1这个网关的，那么这个网关在哪里，同节点容器如何通信，跨节点容器如何通信呢？我们继续看第二个问题&lt;/p&gt;

&lt;h3 id=&quot;问题二为什么我们在host上没有找到ip地址是16925411的设备&quot;&gt;问题二：为什么我们在host上没有找到ip地址是169.254.1.1的设备？&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Why can&apos;t I see the 169.254.1.1 address mentioned above on my host?

Calico tries hard to avoid interfering with any other configuration on the host. Rather than adding the gateway address to the host side of each workload interface, Calico sets the proxy_arp flag on the interface. This makes the host behave like a gateway, responding to ARPs for 169.254.1.1 without having to actually allocate the IP address to the interface.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;简单翻译是：Calico努力避免干扰主机上的配置。Calico没有将网关地址添加到每个容器网卡的主机端，而是在主机端设备上设置proxy_arp标志。
这使得主机能像网关一样响应169.254.1.1的ARP，而不必实际将IP地址分配给接口。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;稍微解释下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Calico中容器网卡类型是veth，其中一端在容器内，另一端在主机上&lt;/li&gt;
  &lt;li&gt;这里通过设置主机端设备，开启proxy_arp，同时设置这个设备的mac地址固定为ee:ee:ee:ee:ee:ee&lt;/li&gt;
  &lt;li&gt;这样任意发送这个设备上的arp请求，该设备都会发送arp reply包，表示自己是网关，可以将后面的包发送给我，我给你们转发&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/arp.png&quot; alt=&quot;arp流程&quot; /&gt;&lt;/p&gt;

&lt;p&gt;实际在容器中查看：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node2 ~]# ip link show calic4eb42dc79d
55: calic4eb42dc79d@if4: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1480 qdisc noqueue state UP mode DEFAULT group default 
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 7
[root@node2 ~]# sysctl -a  |grep calic4eb42dc79d |grep proxy_arp
net.ipv4.conf.calic4eb42dc79d.proxy_arp = 1
net.ipv4.conf.calic4eb42dc79d.proxy_arp_pvlan = 0
[root@node2 ~]# ip r |grep calic4eb42dc79d
11.244.104.3 dev calic4eb42dc79d scope link 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;calic4eb42dc79d这个设备的mac地址是ee:ee:ee:ee:ee:ee&lt;/li&gt;
  &lt;li&gt;calic4eb42dc79d设备开启了arp代答功能，这样任意发送到这个设备上的arp，且目的mac地址是ee:ee:ee:ee:ee:ee的请求，都会返回arp reply&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们来抓包看看，先看看容器分布：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 ~]# kubectl get pods -owide |grep 11.244.104.3
sectest-app-influxdb-deployment-6df667cd75-bmkk5    1/1     Running     4          2d23h   11.244.104.3     node2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@node1 ~]# kubectl get pods -owide |grep 11.244.104.7
sectest-app-rabbitmq-deployment-545f49dd55-bp5tl    1/1     Running     4          2d23h   11.244.104.7     node2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@node1 ~]# kubectl get pods -owide |grep 11.244.135.31
sectest-app-haproxy-deployment-58bfd8b4b-29f79      1/1     Running     2          2d23h   11.244.135.31    node3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;容器同节点通信抓包：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on calic4eb42dc79d, link-type EN10MB (Ethernet), capture size 262144 bytes
14:10:55.717915 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype ARP (0x0806), length 42: Request who-has 169.254.1.1 tell 11.244.104.3, length 28
14:10:55.717943 ee:ee:ee:ee:ee:ee &amp;gt; d6:84:ed:49:d4:cf, ethertype ARP (0x0806), length 42: Reply 169.254.1.1 is-at ee:ee:ee:ee:ee:ee, length 28
14:10:56.705970 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: 11.244.104.3 &amp;gt; 11.244.104.7: ICMP echo request, id 10, seq 7, length 64
14:10:56.706026 ee:ee:ee:ee:ee:ee &amp;gt; d6:84:ed:49:d4:cf, ethertype IPv4 (0x0800), length 98: 11.244.104.7 &amp;gt; 11.244.104.3: ICMP echo reply, id 10, seq 7, length 64
14:10:57.705969 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: 11.244.104.3 &amp;gt; 11.244.104.7: ICMP echo request, id 10, seq 8, length 64
14:10:57.706019 ee:ee:ee:ee:ee:ee &amp;gt; d6:84:ed:49:d4:cf, ethertype IPv4 (0x0800), length 98: 11.244.104.7 &amp;gt; 11.244.104.3: ICMP echo reply, id 10, seq 8, length 64
14:10:58.705990 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: 11.244.104.3 &amp;gt; 11.244.104.7: ICMP echo request, id 10, seq 9, length 64
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;容器跨节点通信抓包：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;14:10:08.101925 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype ARP (0x0806), length 42: Request who-has 169.254.1.1 tell 11.244.104.3, length 28
14:10:08.101948 ee:ee:ee:ee:ee:ee &amp;gt; d6:84:ed:49:d4:cf, ethertype ARP (0x0806), length 42: Reply 169.254.1.1 is-at ee:ee:ee:ee:ee:ee, length 28
14:10:09.093021 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: 11.244.104.3 &amp;gt; 11.244.135.31: ICMP echo request, id 9, seq 7, length 64
14:10:09.093406 ee:ee:ee:ee:ee:ee &amp;gt; d6:84:ed:49:d4:cf, ethertype IPv4 (0x0800), length 98: 11.244.135.31 &amp;gt; 11.244.104.3: ICMP echo reply, id 9, seq 7, length 64
14:10:10.093016 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: 11.244.104.3 &amp;gt; 11.244.135.31: ICMP echo request, id 9, seq 8, length 64
14:10:10.093386 ee:ee:ee:ee:ee:ee &amp;gt; d6:84:ed:49:d4:cf, ethertype IPv4 (0x0800), length 98: 11.244.135.31 &amp;gt; 11.244.104.3: ICMP echo reply, id 9, seq 8, length 64
14:10:11.093573 d6:84:ed:49:d4:cf &amp;gt; ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: 11.244.104.3 &amp;gt; 11.244.135.31: ICMP echo request, id 9, seq 9, length 64
14:10:11.093910 ee:ee:ee:ee:ee:ee &amp;gt; d6:84:ed:49:d4:cf, ethertype IPv4 (0x0800), length 98: 11.244.135.31 &amp;gt; 11.244.104.3: ICMP echo reply, id 9, seq 9, length 64
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在arp请求169.254.1.1后，在主机中的veth设备会返回arp reply包，容器内部会生成一条neigh信息：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node2 ~]# docker exec -it d3d044626d12 sh
# ip neigh
169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;从抓包结果看二层通信和三层通信的结果都是类似的，都会先arp请求网关169.254.1.1，再将包转发到网关设备。也就是将二层和三层通信都通过三层进行转发&lt;/li&gt;
  &lt;li&gt;数据包到达主机上的veth设备（容器认为的网关）后，根据主机上的路由进行转发即可&lt;/li&gt;
  &lt;li&gt;主机路由是由bgp（bird）服务维护的&lt;/li&gt;
  &lt;li&gt;由于arp在host的veth就终结了（arp代答的功劳），这种实现有效屏蔽的arp广播，降低广播风暴的威胁&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2023/02/27/calico-route/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2023/02/27/calico-route/</guid>
        
        <category>calico</category>
        
        
      </item>
    
      <item>
        <title>Kata容器网络实现</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;最近看到安全容器相关的文章，想着看看kata的网络实现&lt;/p&gt;

&lt;h2 id=&quot;架构图&quot;&gt;架构图&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/kata1.png&quot; alt=&quot;kata调用流程&quot; /&gt;
在k8s中配置使用containerd作为runtime-endpoint实现，在containerd中配置runtime支持runc和kata&lt;/p&gt;

&lt;h2 id=&quot;实现&quot;&gt;实现&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/kata2.png&quot; alt=&quot;kata网络&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;默认情况下，containerd容器在创建sandbox的时候，创建对应的netns出来&lt;/li&gt;
  &lt;li&gt;在创建容器时，cni负责创建和配置容器网卡，也就是在对应的netns中创建和配置容器网卡&lt;/li&gt;
  &lt;li&gt;在创建kata容器时，kata-runtime会在容器中创建一个qemu虚拟机，使用tap0_kata网卡作为虚拟机的虚拟网卡&lt;/li&gt;
  &lt;li&gt;这里kata-runtime支持多种方式将流量从容器原本的veth设备mirror到虚拟机的tap设备上&lt;/li&gt;
  &lt;li&gt;支持的mirror方式有: macvtap none和tcfilter，默认使用tcfilter&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;源码&quot;&gt;源码&lt;/h2&gt;

&lt;p&gt;containerd代码：创建netns&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;// RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure
// the sandbox is in ready state.
func (c *criService) RunPodSandbox(ctx context.Context, r *runtime.RunPodSandboxRequest) (_ *runtime.RunPodSandboxResponse, retErr error) {
    config := r.GetConfig()
    log.G(ctx).Debugf(&quot;Sandbox config %+v&quot;, config)
    ...
    if _, err := c.client.SandboxStore().Create(ctx, sandboxInfo); err != nil {
	    return nil, fmt.Errorf(&quot;failed to save sandbox metadata: %w&quot;, err)
    }
    ...
    if podNetwork {
        netStart := time.Now()
        // If it is not in host network namespace then create a namespace and set the sandbox
        // handle. NetNSPath in sandbox metadata and NetNS is non empty only for non host network
        // namespaces. If the pod is in host network namespace then both are empty and should not
        // be used.
        var netnsMountDir = &quot;/var/run/netns&quot;
        if c.config.NetNSMountsUnderStateDir {
            netnsMountDir = filepath.Join(c.config.StateDir, &quot;netns&quot;)
        }
        sandbox.NetNS, err = netns.NewNetNS(netnsMountDir)
        if err != nil {
            return nil, fmt.Errorf(&quot;failed to create network namespace for sandbox %q: %w&quot;, id, err)
        }
        // Update network namespace in the store, which is used to generate the container&apos;s spec
        sandbox.NetNSPath = sandbox.NetNS.GetPath()
       ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;kata-runtime代码：创建qemu虚拟机并attach网卡&lt;/p&gt;

&lt;p&gt;可以看到在创建完虚拟机后执行了AddEndpoints操作，也就是attach了网卡&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;// startVM starts the VM.
func (s *Sandbox) startVM(ctx context.Context, prestartHookFunc func(context.Context) error) (err error) {
    span, ctx := katatrace.Trace(ctx, s.Logger(), &quot;startVM&quot;, sandboxTracingTags, map[string]string{&quot;sandbox_id&quot;: s.id})
    defer span.End()

    s.Logger().Info(&quot;Starting VM&quot;)

    ...

    if err := s.network.Run(ctx, func() error {
        if s.factory != nil {
            vm, err := s.factory.GetVM(ctx, VMConfig{
                HypervisorType:   s.config.HypervisorType,
                HypervisorConfig: s.config.HypervisorConfig,
                AgentConfig:      s.config.AgentConfig,
            })
            if err != nil {
                return err
            }

            return vm.assignSandbox(s)
        }

        return s.hypervisor.StartVM(ctx, VmStartTimeout)
    }); err != nil {
        return err
    }

    ...

    // 1. Do not scan the netns if we want no network for the vmm.
    // 2. In case of vm factory, scan the netns to hotplug interfaces after vm is started.
    // 3. In case of prestartHookFunc, network config might have been changed. We need to
    //    rescan and handle the change.
    if !s.config.NetworkConfig.DisableNewNetwork &amp;amp;&amp;amp; (s.factory != nil || prestartHookFunc != nil) {
        if _, err := s.network.AddEndpoints(ctx, s, nil, true); err != nil {
            return err
        }
    }

    s.Logger().Info(&quot;VM started&quot;)

    ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;kata-runtime代码：attach网卡流程&lt;/p&gt;

&lt;p&gt;会在指定的network namespace中执行addSingleEndpoint方法&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;// Add adds all needed interfaces inside the network namespace.
func (n *LinuxNetwork) AddEndpoints(ctx context.Context, s *Sandbox, endpointsInfo []NetworkInfo, hotplug bool) ([]Endpoint, error) {
    span, ctx := n.trace(ctx, &quot;AddEndpoints&quot;)
    katatrace.AddTags(span, &quot;type&quot;, n.interworkingModel.GetModel())
    defer span.End()

    if endpointsInfo == nil {
        if err := n.addAllEndpoints(ctx, s, hotplug); err != nil {
            return nil, err
        }
    } else {
        for _, ep := range endpointsInfo {
            if err := doNetNS(n.netNSPath, func(_ ns.NetNS) error {
                if _, err := n.addSingleEndpoint(ctx, s, ep, hotplug); err != nil {
                    n.eps = nil
                    return err
                }

                return nil
            }); err != nil {
                return nil, err
            }
        }
    }
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;addSingleEndpoint&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;支持热挂载网卡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;支持多种网卡类型，这里默认使用tuntap&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;网卡支持限速&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;支持多网卡（看到有单独的attach_interface接口）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (n *LinuxNetwork) addSingleEndpoint(ctx context.Context, s *Sandbox, netInfo NetworkInfo, hotplug bool) (Endpoint, error) {
    ...
        if socketPath != &quot;&quot; {
            networkLogger().WithField(&quot;interface&quot;, netInfo.Iface.Name).Info(&quot;VhostUser network interface found&quot;)
            endpoint, err = createVhostUserEndpoint(netInfo, socketPath)
        } else if netInfo.Iface.Type == &quot;macvlan&quot; {
            networkLogger().Infof(&quot;macvlan interface found&quot;)
            endpoint, err = createMacvlanNetworkEndpoint(idx, netInfo.Iface.Name, n.interworkingModel)
        } else if netInfo.Iface.Type == &quot;macvtap&quot; {
            networkLogger().Infof(&quot;macvtap interface found&quot;)
            endpoint, err = createMacvtapNetworkEndpoint(netInfo)
        } else if netInfo.Iface.Type == &quot;tap&quot; {
            networkLogger().Info(&quot;tap interface found&quot;)
            endpoint, err = createTapNetworkEndpoint(idx, netInfo.Iface.Name)
        } else if netInfo.Iface.Type == &quot;tuntap&quot; {
            if netInfo.Link != nil {
                switch netInfo.Link.(*netlink.Tuntap).Mode {
                case 0:
                    // mount /sys/class/net to get links
                    return nil, fmt.Errorf(&quot;Network device mode not determined correctly. Mount sysfs in caller&quot;)
                case 1:
                    return nil, fmt.Errorf(&quot;tun networking device not yet supported&quot;)
                case 2:
                    networkLogger().Info(&quot;tuntap tap interface found&quot;)
                    endpoint, err = createTuntapNetworkEndpoint(idx, netInfo.Iface.Name, netInfo.Iface.HardwareAddr, n.interworkingModel)
                default:
                    return nil, fmt.Errorf(&quot;tuntap network %v mode unsupported&quot;, netInfo.Link.(*netlink.Tuntap).Mode)
                }
            }
        } else if netInfo.Iface.Type == &quot;veth&quot; {
            networkLogger().Info(&quot;veth interface found&quot;)
            endpoint, err = createVethNetworkEndpoint(idx, netInfo.Iface.Name, n.interworkingModel)
        } else if netInfo.Iface.Type == &quot;ipvlan&quot; {
            networkLogger().Info(&quot;ipvlan interface found&quot;)
            endpoint, err = createIPVlanNetworkEndpoint(idx, netInfo.Iface.Name)
        } else {
            return nil, fmt.Errorf(&quot;Unsupported network interface: %s&quot;, netInfo.Iface.Type)
        }
    }
    ...
    networkLogger().WithField(&quot;endpoint-type&quot;, endpoint.Type()).WithField(&quot;hotplug&quot;, hotplug).Info(&quot;Attaching endpoint&quot;)
    if hotplug {
        if err := endpoint.HotAttach(ctx, s.hypervisor); err != nil {
            return nil, err
        }
    } else {
        if err := endpoint.Attach(ctx, s); err != nil {
            return nil, err
        }
    }
    ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;kata-runtime代码：引流实现&lt;/p&gt;

&lt;p&gt;创建网卡, 这里第一张网卡名称是tap0_kata，虚拟机内部是eth0&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func createTuntapNetworkEndpoint(idx int, ifName string, hwName net.HardwareAddr, internetworkingModel NetInterworkingModel) (*TuntapEndpoint, error) {
    ...

    netPair, err := createNetworkInterfacePair(idx, ifName, internetworkingModel)
    if err != nil {
        return nil, err
    }

    endpoint := &amp;amp;TuntapEndpoint{
        NetPair: netPair,
        TuntapInterface: TuntapInterface{
            Name: fmt.Sprintf(&quot;eth%d&quot;, idx),
            TAPIface: NetworkInterface{
                Name:     fmt.Sprintf(&quot;tap%d_kata&quot;, idx),
                HardAddr: fmt.Sprintf(&quot;%s&quot;, hwName), //nolint:gosimple
            },
        },
        EndpointType: TuntapEndpointType,
    }
    ...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在attach方法中会调用xConnectVMNetwork，这里面会实现容器网络的引流&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;// Attach for tun/tap endpoint adds the tap interface to the hypervisor.
func (endpoint *TuntapEndpoint) Attach(ctx context.Context, s *Sandbox) error {
    span, ctx := tuntapTrace(ctx, &quot;Attach&quot;, endpoint)
    defer span.End()

    h := s.hypervisor
    if err := xConnectVMNetwork(ctx, endpoint, h); err != nil {
        networkLogger().WithError(err).Error(&quot;Error bridging virtual endpoint&quot;)
        return err
    }

    return h.AddDevice(ctx, endpoint, NetDev)
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;引流驱动有macvtap 和tcfilter，其中默认使用tcfilter&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func setupTCFiltering(ctx context.Context, endpoint Endpoint, queues int, disableVhostNet bool) error {
    ...
    if err := addQdiscIngress(tapAttrs.Index); err != nil {
        return err
    }

    if err := addQdiscIngress(attrs.Index); err != nil {
        return err
    }

    if err := addRedirectTCFilter(attrs.Index, tapAttrs.Index); err != nil {
        return err
    }

    if err := addRedirectTCFilter(tapAttrs.Index, attrs.Index); err != nil {
        return err
    }
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;demo&quot;&gt;demo&lt;/h2&gt;

&lt;p&gt;kata部署可以参考网上的文章，这里通过创建一个kata容器，查看容器内的流量转发&lt;/p&gt;

&lt;p&gt;创建kata容器&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 kata]# cat nginx-kata.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-kata
spec:
  selector:
    matchLabels:
      run: my-nginx
  replicas: 1
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      runtimeClassName: kata
      containers:
      - name: my-nginx
        image: httpd:alpine
        ports:
        - containerPort: 80
      - name: my-redis
        image: redis
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;查看容器&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 kata]# kubectl get pods -owide
NAME                             READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
my-nginx-kata-8675cd7c89-wzgr2   2/2     Running   0          45s   10.244.166.150   node1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;查看网络命名空间&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;看到在netns中有两张网卡，eth0和tap0_kata&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;eth0网卡是veth类型，类似普通容器的网卡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;tap0_kata网卡是tap类型，启动qemu虚拟机时使用的虚拟网卡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: tunl0@NONE: &amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if29: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1430 qdisc noqueue state UP group default qlen 1000
    link/ether fe:68:1c:e3:47:da brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.166.150/32 brd 10.244.166.150 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::fc68:1cff:fee3:47da/64 scope link
       valid_lft forever preferred_lft forever
5: tap0_kata: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1430 qdisc mq state UNKNOWN group default qlen 1000
    link/ether 76:c7:1b:ab:30:64 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::74c7:1bff:feab:3064/64 scope link
       valid_lft forever preferred_lft forever
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;tc引流规则&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;对于eth0网卡的入口流量通过tc规则mirror到tap0_kata网卡上&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;对于tap0_kata的入口流量通过tc规则mirror到eth0网卡上&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;这样一个双向的流量通道就建立了&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f tc -s qdisc show dev eth0
qdisc noqueue 0: root refcnt 2
 Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
qdisc ingress ffff: parent ffff:fff1 ----------------
 Sent 480 bytes 5 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
 
[root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f tc -s filter show dev eth0 ingress
filter protocol all pref 49152 u32
filter protocol all pref 49152 u32 fh 800: ht divisor 1
filter protocol all pref 49152 u32 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 5 success 5)
  match 00000000/00000000 at 0 (success 5 )
        action order 1: mirred (Egress Redirect to device tap0_kata) stolen
        index 1 ref 1 bind 1 installed 439 sec used 437 sec
        Action statistics:
        Sent 480 bytes 5 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0
 
[root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f tc -s filter show dev tap0_kata ingress
filter protocol all pref 49152 u32
filter protocol all pref 49152 u32 fh 800: ht divisor 1
filter protocol all pref 49152 u32 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 12 success 12)
  match 00000000/00000000 at 0 (success 12 )
        action order 1: mirred (Egress Redirect to device eth0) stolen
        index 2 ref 1 bind 1 installed 451 sec used 165 sec
        Action statistics:
        Sent 768 bytes 12 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;从上面的代码分析，大致了解了kata的流量路径，类似在一个普通容器基础上启动了一个qemu进程&lt;/li&gt;
  &lt;li&gt;通过tc mirror机制，将容器veth网卡流量镜像到tap网卡（通过macvtap子接口方式也类似）&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2023/02/15/kata-network/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2023/02/15/kata-network/</guid>
        
        <category>kata</category>
        
        
      </item>
    
      <item>
        <title>Neuvector源码之 合规性检测</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;Neuvector安全基线支持 CIS Benchmark 标准，可对容器、镜像、Register、主机、kubernetes 进行安全标准检查，多维度展现容器资产的基线合规情况并帮助建立容器运行环境下的最佳基线配置，减少攻击面
NeuVector 的合规性审核包括 CIS 基线测试、自定义检查、机密审核以及 PCI、GDPR 和其他法规的行业标准模板扫描。本文将通过源码的方式分析合规性检测的具体实现&lt;/p&gt;

&lt;h2 id=&quot;架构图&quot;&gt;架构图&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_bench.png&quot; alt=&quot;neuvector bench&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从架构图中我们可以看到涉及两个模块，分别是:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;controller:负责提供API接口，保存业务数据&lt;/li&gt;
  &lt;li&gt;agent（enforce）：具体执行合规性检测的模块&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;源码&quot;&gt;源码&lt;/h2&gt;

&lt;p&gt;首先在rest.go中查看bench相关的接口，我们主要看下：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;r.GET(&quot;/v1/bench/host/:id/docker&quot;, handlerDockerBench)
r.POST(&quot;/v1/bench/host/:id/docker&quot;, handlerDockerBenchRun)
r.GET(&quot;/v1/bench/host/:id/kubernetes&quot;, handlerKubeBench)
r.POST(&quot;/v1/bench/host/:id/kubernetes&quot;, handlerKubeBenchRun)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里我们通过docker合规性检测来了解具体的实现细节，在handlerDockerBenchRun方法中获取node id，并查询到这个node节点的agent信息，
方便后面的rpc调用&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func RunDockerBench(agentID string) error {
    client, err := findEnforcerServiceClient(agentID)
    if err != nil {
        return err
    }

    ctx, cancel := context.WithTimeout(context.Background(), defaultReqTimeout)
    defer cancel()

    _, err = client.RunDockerBench(ctx, &amp;amp;share.RPCVoid{})
    return err
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;切换到agent的代码中，我们看到RunDockerBench方法实际做的事情是调用了RerunDocker方法，在RerunDocker方法中执行的逻辑非常简单，就是reset
host timer和container timer, 同时设置下数据库中当前节点bench任务的状态为scheduled&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (b *Bench) RerunDocker() {
    log.Info(&quot;&quot;)

    if err := b.dockerCheckPrerequisites(); err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Cannot run Docker CIS benchmark&quot;)
        b.logBenchFailure(benchPlatDocker, share.BenchStatusNotSupport)
        b.putBenchReport(Host.ID, share.BenchDockerHost, nil, share.BenchStatusNotSupport)
    } else {
        b.hostTimer.Reset(hostTimerStart)
        b.conTimer.Reset(containerTimerStart)
        b.putBenchReport(Host.ID, share.BenchDockerHost, nil, share.BenchStatusScheduled)
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_bench2.png&quot; alt=&quot;neuvector bench&quot; /&gt;
通过上图我们发现，在agent启动时会启动一个后台任务BenchLoop，用来定时指定容器、host、k8s平台、自定义的合规性检测&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (b *Bench) BenchLoop() {
    var masterScript, workerScript, remediation string
    b.taskScanner = newTaskScanner(b, scanWorkerMax)
    //after the host bench, it will schedule a container bench automaticly even if no container
    for {
        select {
        case &amp;lt;-b.hostTimer.C:
            b.doDockerHostBench()

        case &amp;lt;-b.kubeTimer.C:
            ...

            b.doKubeBench(masterScript, workerScript, remediation)
        case &amp;lt;-b.conTimer.C:
            containers := b.cloneAllNewContainers()
            if Host.CapDockerBench {
                b.doDockerContainerBench(containers)
            } else {
                b.putBenchReport(Host.ID, share.BenchDockerContainer, nil, share.BenchStatusFinished)
            }

            ...
        case &amp;lt;-b.customConTimer.C:
            ...

            b.doContainerCustomCheck(wls)
        case &amp;lt;-b.customHostTimer.C:
            b.doHostCustomCheck()
        }
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里在reset container timer后，会立即触发定时任务的执行，也就是这里的doDockerContainerBench方法&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (b *Bench) doDockerContainerBench(containers map[string]string) error {
    b.putBenchReport(Host.ID, share.BenchDockerContainer, nil, share.BenchStatusRunning)
    if out, err := b.runDockerContainerBench(containers); err != nil {
        b.logBenchFailure(benchPlatDocker, share.BenchStatusDockerContainerFail)
        b.putBenchReport(Host.ID, share.BenchDockerContainer, nil, share.BenchStatusDockerContainerFail)
        return err
    } else {
        log.Info(&quot;Running benchmark checks done&quot;)

        list := b.getBenchMsg(out)
        b.assignDockerBenchMeta(list)

        b.putBenchReport(Host.ID, share.BenchDockerContainer, list, share.BenchStatusFinished)

        // Going through each container, write report and log
        ...
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里我们重点关注几个方法，分别是&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;runDockerContainerBench： 具体执行容器合规性检测，后面重点介绍&lt;/li&gt;
  &lt;li&gt;getBenchMsg：将执行结果格式化&lt;/li&gt;
  &lt;li&gt;assignDockerBenchMeta：执行结果格式化，主要是格式化合规项名称和profile&lt;/li&gt;
  &lt;li&gt;putBenchReport：将格式化后的结果保存到数据库中，等待获取结果的api调用时从数据库中获取&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我们看看runDockerContainerBench方法：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (b *Bench) runDockerContainerBench(containers map[string]string) ([]byte, error) {
    ...

    if err := b.replaceDockerDaemonCmdline(srcContainerBenchSh, dstContainerBenchSh, cs); err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Replace container docker daemon cmdline error&quot;)
        return nil, fmt.Errorf(&quot;Replace container docker daemon cmdline error, error=%v&quot;, err)
    }

    args := []string{system.NSActRun, &quot;-f&quot;, dstContainerBenchSh, &quot;-m&quot;, global.SYS.GetMountNamespacePath(1)}
    var errb, outb bytes.Buffer

    log.WithFields(log.Fields{&quot;args&quot;: args}).Debug(&quot;Running bench script&quot;)
    cmd := exec.Command(system.ExecNSTool, args...)
    cmd.SysProcAttr = &amp;amp;syscall.SysProcAttr{Setsid: true}
    cmd.Stdout = &amp;amp;outb
    cmd.Stderr = &amp;amp;errb
    b.childCmd = cmd
    err := cmd.Start()
    ...
    return out, nil
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;该方法首先根据模板生成目标的cis检测脚本&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;模板位置：/usr/local/bin/container.tmpl，生成文件位置：/tmp/container.sh&lt;/li&gt;
  &lt;li&gt;根据模板生成脚本时传入当前节点所有容器的信息，脚本中会遍历$containers参数，执行检测任务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;replaceDockerDaemonCmdline&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (b *Bench) replaceDockerDaemonCmdline(srcPath, dstPath string, containers []string) error {
    dat, err := ioutil.ReadFile(srcPath)
    if err != nil {
        return err
    }
    f, err := os.Create(dstPath)
    if err != nil {
        return err
    }
    defer f.Close()

    //containers only apply to container.sh, no effect to host.sh, because no &amp;lt;&amp;lt;&amp;lt;Containers&amp;gt;&amp;gt;&amp;gt; in it
    var containerLines string
    if len(containers) &amp;gt; 0 {
        containerLines = &quot;containers=\&quot;\n&quot; + strings.Join(containers, &quot;\n&quot;) + &quot;\&quot;\n&quot;
    } else {
        containerLines = &quot;containers=\&quot;\&quot;\n&quot;
    }
    r := DockerReplaceOpts{
        Replace_docker_daemon_opts: strings.Join(b.daemonOpts, &quot; &quot;),
        Replace_container_list:     containerLines,
    }
    t := template.New(&quot;bench&quot;)
    t.Delims(&quot;&amp;lt;&amp;lt;&amp;lt;&quot;, &quot;&amp;gt;&amp;gt;&amp;gt;&quot;)
    t.Parse(string(dat))

    if err = t.Execute(f, r); err != nil {
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(&quot;Executing template error&quot;)
        return err
    }
    return nil
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;生成玩cis检测脚本后，会调用nstool命令在host上执行检测命令：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;nstools run -f host.sh -m  /proc/1/ns/mnt -n /proc/1/ns/net
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里因为脚本会遍历节点上所有容器，通过docker命令执行检测操作，我们并不需要在容器内部执行，所以也不一定使用nstool工具，可以直接执行shell脚本&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;到了这里已经完成了某个节点上容器的合规性检测任务，我们看看执行返回的信息：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;...
[WARN] 4.1 - Ensure that a user for the container has been created (Automated)
[WARN]      * Running as root: k8s_busybox_busybox-hujin_default_7191a3da-4c51-467a-a004-db178d79e92a_1158

[WARN] 5.1 - Ensure that, if applicable, an AppArmor Profile is enabled (Automated)
[WARN]      * No AppArmorProfile Found: k8s_busybox_busybox-hujin_default_7191a3da-4c51-467a-a004-db178d79e92a_1158
[PASS] 5.2 - Ensure that, if applicable, SELinux security options are set (Automated)
[PASS] 5.3 - Ensure that Linux kernel capabilities are restricted within containers (Automated)
[PASS] 5.4 - Ensure that privileged containers are not used (Automated)
[PASS] 5.5 - Ensure sensitive host system directories are not mounted on containers (Automated)
[PASS] 5.6 - Ensure sshd is not run within containers (Automated)
[PASS] 5.7 - Ensure privileged ports are not mapped within containers (Automated)
[PASS] 5.8 - Ensure that only needed ports are open on the container (Manual)
[PASS] 5.9 - Ensure that the host&apos;s network namespace is not shared (Automated)
...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;容器的合规性检测功能，neuvector是集成了docker-bench-security项目，将该项目的检测脚本整理成了一个container.tmpl模板&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;后面会将执行结果进行格式化并保存到数据库，代码流程就讲了&lt;/p&gt;

&lt;p&gt;这里还需要稍微提一下法规和cis的关系，我们在获取合规性检测结果的时候，handlerDockerBench - getCISReportFromCluster - _getCISReportFromCluster - 
bench2REST - GetComplianceMeta方法会将合规项添加法规对应的tag标识，方便进行过滤&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func GetComplianceMeta() ([]api.RESTBenchMeta, map[string]api.RESTBenchMeta) {
    if complianceMetas == nil || complianceMetaMap == nil {
        ...
        for _, item := range docker_image_cis_items {
            all = append(all, api.RESTBenchMeta{RESTBenchCheck: item})
        }

        for i, _ := range all {
            item := &amp;amp;all[i]
            item.Tags = make([]string, 0)
            if compliancePCI.Contains(item.TestNum) {
                item.Tags = append(item.Tags, api.ComplianceTemplatePCI)
            }
            if complianceGDPR.Contains(item.TestNum) {
                item.Tags = append(item.Tags, api.ComplianceTemplateGDPR)
            }
            if complianceHIPAA.Contains(item.TestNum) {
                item.Tags = append(item.Tags, api.ComplianceTemplateHIPAA)
            }
            if complianceNIST.Contains(item.TestNum) {
                item.Tags = append(item.Tags, api.ComplianceTemplateNIST)
            }
            ...
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;支持的法规包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;PCI&lt;/li&gt;
  &lt;li&gt;GDPR&lt;/li&gt;
  &lt;li&gt;HIPAA&lt;/li&gt;
  &lt;li&gt;NIST&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;从上面的代码分析，我们看到neuvector支持对host/container/kubernetes平台的合规性检测，同时支持一些常见的法规，可以输出非常清晰的格式化结果并提供下载；
但我们也可以发现一些不足的地方，包括不支持其他runtime、不支持国内的法规等问题。
neuvector实际通过集成一个叫kubernetes-cis-benchmark的项目来实现基线扫描功能，此项目目前基本不维护了，导致支持的k8s版本一直停留在v1.18,且扫描出来的问题有些是不对的。
建议还是使用kube-bench来扫描，或者neuvector可以将kube-bench集成进来&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2022/09/21/neuvector-bench/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2022/09/21/neuvector-bench/</guid>
        
        <category>neuvector</category>
        
        <category>合规性检测</category>
        
        <category>compliance</category>
        
        
      </item>
    
      <item>
        <title>Calico BGP - Bird</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;Calico支持多种网络模式，包括vxlan/ipip/bgp，其中vxlan和ipip属于overlay类型，在嵌套部署模式比较通用，但网络性能相对bgp会低一些。这主要是由于bgp模式下没有数据报文的封包和解包操作&lt;/p&gt;

&lt;p&gt;本文会将calico中bgp相关的操作流程抽离，通过demo的方式来介绍calico中bgp网络的实现&lt;/p&gt;

&lt;h2 id=&quot;架构&quot;&gt;架构&lt;/h2&gt;
&lt;p&gt;calico架构
&lt;img src=&quot;/blog/img/calico.png&quot; alt=&quot;calico_bird&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Calico作为一种常用的Kubernetes网络插件，使用BGP协议对各节点的容器网络进行路由交换。Calico中使用的软件BGP方案是Bird&lt;/p&gt;

&lt;p&gt;BIRD（BIRD Internet Routing Daemon）是一款可运行在Linux和其他类Unix系统上的路由软件，它实现了多种路由协议，比如BGP、OSPF、RIP等。&lt;/p&gt;

&lt;p&gt;BIRD会在内存中维护许多路由表，路由表根据不同的协议，通过与各种“其他事物”交换路由信息，来更新路由规则。这里说的“其他事物”可能是其他的路由表，也可能是外部的路由器，还可以是内核的某些API&lt;/p&gt;

&lt;p&gt;demo架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/calico_demo.png&quot; alt=&quot;calico_bird&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;实现&quot;&gt;实现&lt;/h2&gt;
&lt;h3 id=&quot;模拟calico-cni创建和配置网卡的操作&quot;&gt;模拟calico cni创建和配置网卡的操作&lt;/h3&gt;
&lt;p&gt;node1节点&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 创建namespace和虚拟网卡
ip netns add ns1
ip link add tap1 type veth peer name nsvth netns ns1
ip link set tap1 up
ip netns exec ns1 ip link set lo up
ip netns exec ns1 ip link set nsvth up


# 配置路由和IP地址
ip netns exec ns1 ip addr add 10.244.166.128/24 dev nsvth
ip r add 10.244.166.128/32 dev tap1
ip netns exec ns1 ip r add 169.254.1.1 dev nsvth
ip netns exec ns1 ip r add default via 169.254.1.1 dev nsvth


# 配置neigh
ip link set address ee:ee:ee:ee:ee:ee dev tap1
ip netns exec ns1 ip neigh add 169.254.1.1 dev nsvth lladdr ee:ee:ee:ee:ee:ee
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;node2节点&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 创建namespace和虚拟网卡
ip netns add ns1
ip link add tap1 type veth peer name nsvth netns ns1
ip link set tap1 up
ip netns exec ns1 ip link set lo up
ip netns exec ns1 ip link set nsvth up


# 配置路由和IP地址
ip netns exec ns1 ip addr add 10.244.104.0/24 dev nsvth
ip r add 10.244.104.0/32 dev tap1
ip netns exec ns1 ip r add 169.254.1.1 dev nsvth
ip netns exec ns1 ip r add default via 169.254.1.1 dev nsvth


# 配置neigh
ip link set address ee:ee:ee:ee:ee:ee dev tap1
ip netns exec ns1 ip neigh add 169.254.1.1 dev nsvth lladdr ee:ee:ee:ee:ee:ee
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;每个节点启动bird
启动一个容器并获取内部的bird二进制文件，镜像使用calico/node:v3.11.1&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;docker run --name calico-temp -d calico/node:v3.11.1 sleep 3600
docker cp calico-temp:/usr/bin/bird /usr/bin
docker rm -f calico-temp
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建bird.cfg配置&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;function apply_communities ()
{
}

# Generated by confd
include &quot;bird_aggr.cfg&quot;;
include &quot;bird_ipam.cfg&quot;;

router id 10.20.30.30;

# Configure synchronization between routing tables and kernel.
protocol kernel {
  learn;             # Learn all alien routes from the kernel
  persist;           # Don&apos;t remove routes on bird shutdown
  scan time 2;       # Scan kernel routing table every 2 seconds
  import all;
  export filter calico_kernel_programming; # Default is export none
  graceful restart;  # Turn on graceful restart to reduce potential flaps in
                     # routes when reloading BIRD configuration.  With a full
                     # automatic mesh, there is no way to prevent BGP from
                     # flapping since multiple nodes update their BGP
                     # configuration at the same time, GR is not guaranteed to
                     # work correctly in this scenario.
  merge paths on;    # Allow export multipath routes (ECMP)
}

# Watch interface up/down events.
protocol device {
  debug { states };
  scan time 2;    # Scan interfaces every 2 seconds
}

protocol direct {
  debug { states };
  interface -&quot;cali*&quot;, -&quot;kube-ipvs*&quot;, &quot;*&quot;; # Exclude cali* and kube-ipvs* but
                                          # include everything else.  In
                                          # IPVS-mode, kube-proxy creates a
                                          # kube-ipvs0 interface. We exclude
                                          # kube-ipvs0 because this interface
                                          # gets an address for every in use
                                          # cluster IP. We use static routes
                                          # for when we legitimately want to
                                          # export cluster IPs.
}


# Template for all BGP clients
template bgp bgp_template {
  debug { states };
  description &quot;Connection to BGP peer&quot;;
  local as 64512;
  multihop;
  gateway recursive; # This should be the default, but just in case.
  import all;        # Import all routes, since we don&apos;t know what the upstream
                     # topology is and therefore have to trust the ToR/RR.
  export filter calico_export_to_bgp_peers;  # Only want to export routes for workloads.
  add paths on;
  graceful restart;  # See comment in kernel section about graceful restart.
  connect delay time 2;
  connect retry time 5;
  error wait time 5,30;
}

# ------------- Node-to-node mesh -------------
# For peer /host/node1/ip_addr_v4
# Skipping ourselves (10.20.30.30)

# For peer /host/node2/ip_addr_v4
protocol bgp Mesh_10_20_30_31 from bgp_template {
  neighbor 10.20.30.31 as 64512;
  source address 10.20.30.30;  # The local address we use for the TCP connection
  #passive on; # Mesh is unidirectional, peer will connect to us.
}

# For peer /host/node3/ip_addr_v4
#protocol bgp Mesh_10_20_30_32 from bgp_template {
#  neighbor 10.20.30.32 as 64512;
#  source address 10.20.30.30;  # The local address we use for the TCP connection
#  passive on; # Mesh is unidirectional, peer will connect to us.
#}

# ------------- Global peers -------------
# No global peers configured.

# ------------- Node-specific peers -------------
# No node-specific peers configured.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建bird_ipam.cfg配置&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# Generated by confd
filter calico_export_to_bgp_peers {
  # filter code terminates when it calls `accept;` or `reject;`, call apply_communities() before calico_aggr()
  apply_communities();
  calico_aggr();

  if ( net ~ 10.244.0.0/16 ) then {
    accept;
  }
  reject;
}


filter calico_kernel_programming {

  if ( net ~ 10.244.0.0/16 ) then {
    krt_tunnel = &quot;&quot;;
    accept;
  }

  accept;
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建bird_aggr.cfg配置&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# Generated by confd

protocol static {
   # IP blocks for this host.
   route 10.244.166.128/26 blackhole;
}


# Aggregation of routes on this host; export the block, nothing beneath it.
function calico_aggr ()
{
      # Block 10.244.166.128/26 is confirmed
      if ( net = 10.244.166.128/26 ) then { accept; }
      if ( net ~ 10.244.166.128/26 ) then { reject; }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行bird进程&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;bird -R -s ./bird.ctl -d -c ./bird.cfg
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;此时在node1/node2节点会发现分别多了一条到对方的路由规则&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 bird]# ip r
default via 179.20.23.1 dev ens160 proto static metric 100 
10.20.30.0/24 dev ens192 proto kernel scope link src 10.20.30.30 metric 101 
10.244.104.0/26 via 10.20.30.31 dev ens192 proto bird 
10.244.166.128 dev tap1 scope link 
blackhole 10.244.166.128/26 proto bird 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
179.20.23.0/24 dev ens160 proto kernel scope link src 179.20.23.30 metric 100

[root@node2 ~]# ip r
default via 179.20.23.1 dev ens160 proto static metric 100 
10.20.30.0/24 dev ens192 proto kernel scope link src 10.20.30.31 metric 101 
10.244.104.0 dev tap1 scope link 
blackhole 10.244.104.0/26 proto bird 
10.244.104.1 dev cali6fa4fc0f157 scope link 
10.244.104.2 dev calif2ac964c43c scope link 
10.244.166.128/26 via 10.20.30.30 dev ens192 proto bird 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
179.20.23.0/24 dev ens160 proto kernel scope link src 179.20.23.31 metric 100
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;从node1的ns1中ping node2的ns1 ip&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 bird]# ip netns exec ns ip a 
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: nsvth@if9: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 2e:be:65:26:b3:0d brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.166.128/24 scope global nsvth
       valid_lft forever preferred_lft forever
    inet6 fe80::2cbe:65ff:fe26:b30d/64 scope link 
       valid_lft forever preferred_lft forever

[root@node1 bird]# ip netns exec ns ping 10.244.104.0
PING 10.244.104.0 (10.244.104.0) 56(84) bytes of data.
64 bytes from 10.244.104.0: icmp_seq=1 ttl=62 time=0.299 ms
64 bytes from 10.244.104.0: icmp_seq=2 ttl=62 time=0.356 ms
64 bytes from 10.244.104.0: icmp_seq=3 ttl=62 time=0.314 ms
^C
--- 10.244.104.0 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2033ms
rtt min/avg/max/mdev = 0.299/0.323/0.356/0.024 ms
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;bird手册：The BIRD Internet Routing Daemon Project (network.cz)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;如果对您有帮忙打个赏&quot;&gt;如果对您有帮忙，打个赏&lt;/h2&gt;

</description>
        <pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2022/09/16/calico-bird/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2022/09/16/calico-bird/</guid>
        
        <category>calico</category>
        
        <category>bgp</category>
        
        
      </item>
    
      <item>
        <title>Neuvector源码之 dp引流的实现</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;在neuvector中通过监听runtime事件来动态维护节点上容器基础信息，在界面支持策略配置、模式管理都对容器流量产生影响。用户从学习模式调整成保护模式时可以对容器流量进行阻断操作，那么agent
中是如何实现阻断的？
这里我们将核心流程抽离出来，通过demo的方式来介绍具体的实现细节&lt;/p&gt;

&lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;h3 id=&quot;默认形态&quot;&gt;默认形态&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_dp1.png&quot; alt=&quot;neuvector_pcap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;开始之前先看下默认情况下容器的网络形态（以calico为例）:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;容器网卡使用veth设备，一端在容器内部，一端在host上&lt;/li&gt;
  &lt;li&gt;在host上通过路由规则的方式将流量引到host上的veth设备，最终到达容器内部&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;引流形态&quot;&gt;引流形态&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_dp2.png&quot; alt=&quot;neuvector_pcap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;引流形态下流量会优先经过agent容器，根据策略规则过滤后转发到具体的容器内部。本次我们需要从host上直接ping container-ns中的网卡ip 10.10.10.88,&lt;/p&gt;

&lt;p&gt;首先创建两个network namespace&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ns1：引流实现位置&lt;/li&gt;
  &lt;li&gt;ns2：模拟普通容器的network namespace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;命令&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;ip netns add agent-ns
ip netns add container-ns
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在host上创建三组veth&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;vex：流量入口&lt;/li&gt;
  &lt;li&gt;vin：跨agent-ns和container-ns，在agent中将流量转发到这个设备就可以到达容器内部&lt;/li&gt;
  &lt;li&gt;vbr：监控设备，也是dp工作的设备&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;命令：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 创建veth
ip link add vex type veth peer name vex-peer
ip link add vin type veth peer name vin-peer
ip link add vbr type veth peer name vth
 
# 将vex放入agent-ns
ip link set vex netns agent-ns
ip netns exec agent-ns ip link set vex up
 
# 将vin放入agent-ns
ip link set vin netns agent-ns
ip netns exec agent-ns ip link set vin up
 
# 将vin-peer放入container-ns
ip link set vin-peer netns container-ns
 
# 在container-ns修改网卡名称为eth0
ip netns exec container-ns ip link set vin-peer name eth0
ip addr add 10.10.10.88/24 dev eth0
ip netns exec container-ns ip link set eth0 up
 
# 将vbr和vth都放入agent-ns
ip link set vbr netns agent-ns
ip link set vth netns agent-ns
ip netns exec agent-ns ip link set vbr up
ip netns exec agent-ns ip link set vth up

# 配置host上的vex-peer
ip addr add 10.10.10.66/24 dev vex-peer
ip link set vex-peer up
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;tc引流&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 创建qdisc
tc qdisc add dev vin ingress
tc qdisc add dev vex ingress
tc qdisc add dev vbr ingress

# 创建filter
tc filter add dev vex pref 10001 parent ffff: protocol ip u32 match u8 0 1 at -14 match u16 0x1a27 0xffff at -14 match u32 0xf3873a27 0xffffffff at -12 action pedit munge offset -14 u16 set 0x4e65 munge offset -12 u32 set 0x755600b4 pipe action mirred egress mirror dev vbr
 
tc filter add dev vex pref 10002 parent ffff: protocol all u32 match u8 0 0 action mirred egress mirror dev vin
 
tc filter add dev vbr pref 5 parent ffff: protocol all u32 match u16 0x4e65 0xffff at -14 match u32 0x755600b4 0xffffffff at -12 action pedit munge offset -14 u16 set 0x1a27 munge offset -12 u32 set 0xf3873a27 pipe action mirred egress mirror dev vin
 
tc filter add dev vin pref 10002 parent ffff: protocol all u32 match u8 0 0 action mirred egress mirror dev vex
 
tc filter add dev vin pref 10001 parent ffff: protocol ip u32 match u8 0 1 at -14 match u32 0x1a27f387 0xffffffff at -8 match u16 0x3a27 0xffff at -4 action pedit munge offset -8 u32 set 0x4e657556 munge offset -4 u16 set 0x00b4 pipe action mirred egress mirror dev vbr
 
tc filter add dev vbr pref 172 parent ffff: protocol all u32 match u32 0x4e657556 0xffffffff at -8 match u16 0x00b4 0xffff at -4 action pedit munge offset -8 u32 set 0x1a27f387 munge offset -4 u16 set 0x3a27 pipe action mirred egress mirror dev vex
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;此时我们从host上ping 10.10.10.88，同时在agent-ns的vex和vbr抓包会看到数据包&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这里从抓包结果会看到目标mac变了，这个是通过tc完成的&lt;/li&gt;
  &lt;li&gt;但是在vin上是抓不到包的（可以抓到广播包），因为dp没有开始工作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;抓包结果：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;sh-4.2# tcpdump -enpli vex
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on vex, link-type EN10MB (Ethernet), capture size 262144 bytes
17:31:40.761531 06:82:9c:02:54:91 &amp;gt; 1a:27:f3:87:3a:27, ethertype IPv4 (0x0800), length 98: 10.10.10.66 &amp;gt; 10.10.10.88: ICMP echo request, id 32052, seq 214, length 64
17:31:41.761533 06:82:9c:02:54:91 &amp;gt; 1a:27:f3:87:3a:27, ethertype IPv4 (0x0800), length 98: 10.10.10.66 &amp;gt; 10.10.10.88: ICMP echo request, id 32052, seq 215, length 64
17:31:42.761516 06:82:9c:02:54:91 &amp;gt; 1a:27:f3:87:3a:27, ethertype IPv4 (0x0800), length 98: 10.10.10.66 &amp;gt; 10.10.10.88: ICMP echo request, id 32052, seq 216, length 64
^C
3 packets captured
3 packets received by filter
0 packets dropped by kernel
sh-4.2# tcpdump -enpli vbr
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on vbr, link-type EN10MB (Ethernet), capture size 262144 bytes
17:31:45.761528 06:82:9c:02:54:91 &amp;gt; 4e:65:75:56:00:b4, ethertype IPv4 (0x0800), length 98: 10.10.10.66 &amp;gt; 10.10.10.88: ICMP echo request, id 32052, seq 219, length 64
17:31:46.761515 06:82:9c:02:54:91 &amp;gt; 4e:65:75:56:00:b4, ethertype IPv4 (0x0800), length 98: 10.10.10.66 &amp;gt; 10.10.10.88: ICMP echo request, id 32052, seq 220, length 64
17:31:47.761531 06:82:9c:02:54:91 &amp;gt; 4e:65:75:56:00:b4, ethertype IPv4 (0x0800), length 98: 10.10.10.66 &amp;gt; 10.10.10.88: ICMP echo request, id 32052, seq 221, length 64
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;运行dp&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;ip netns exec agent-ns dp -n 1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里需要重新编译dp，对dp做一些改造，保证可以在host运行，主要是mmap那部分逻辑&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;调用dp接口&lt;/p&gt;

&lt;p&gt;分别调用dp接口：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ctrl_add_srvc_port： 配置引流设备名称等参数&lt;/li&gt;
  &lt;li&gt;ctrl_cfg_internal_net：配置当前节点设备ip和类型&lt;/li&gt;
  &lt;li&gt;ctrl_add_mac：在引流设备中添加需要引流的mac&lt;/li&gt;
  &lt;li&gt;ctrl_cfg_mac：配置mac参数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;调用脚本&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import socket
import json
 
DP_SERVER_PATH = &apos;/tmp/dp_listen.sock&apos;
SOCK = None

def connect_dp():
    global SOCK
    if SOCK:
        return SOCK
    socket_family = socket.AF_UNIX
    socket_type = socket.SOCK_DGRAM
 
    sock = socket.socket(socket_family, socket_type)
    sock.connect(DP_SERVER_PATH)
    SOCK = sock
    print(&quot;dp connected&quot;)
    return sock
 
 
def send_msg(msg):
    sock = connect_dp()
    sock.sendall(json.dumps(msg).encode())
 
    # data = sock.recv(1024)
    # print(&quot;receive data:&quot;, data)
    print(&apos;send msg: %s success&apos; % msg)
 
 
def add_srvc_port():
    data = {
        &quot;iface&quot;: &quot;vth&quot;,
        &quot;jumboframe&quot;: False
    }
    send_msg({&apos;ctrl_add_srvc_port&apos;: data})
 
 
def add_internal_net():
    data = {
        &quot;flag&quot;: 3,
        &quot;subnet_addr&quot;: [
            {&quot;ip&quot;: &quot;172.17.0.0&quot;, &quot;mask&quot;: &quot;255.255.0.0&quot;},
            {&quot;ip&quot;: &quot;179.20.23.0&quot;, &quot;mask&quot;: &quot;255.255.255.0&quot;},
            {&quot;ip&quot;: &quot;10.10.10.0&quot;, &quot;mask&quot;: &quot;255.255.255.0&quot;},
            {&quot;ip&quot;: &quot;172.20.166.0&quot;, &quot;mask&quot;: &quot;255.255.255.0&quot;},
            {&quot;ip&quot;: &quot;10.10.10.66&quot;, &quot;mask&quot;: &quot;255.255.255.0&quot;, &quot;iptype&quot;: &quot;devip&quot;},
            {&quot;ip&quot;: &quot;10.10.10.88&quot;, &quot;mask&quot;: &quot;255.255.255.0&quot;, &quot;iptype&quot;: &quot;devip&quot;},
 
        ]
    }
    send_msg({&apos;ctrl_cfg_internal_net&apos;: data})
 
 
def add_mac():
    data = {
        &quot;iface&quot;: &quot;vth&quot;,
        &quot;mac&quot;: &quot;1a:27:f3:87:3a:27&quot;,
        &quot;ucmac&quot;: &quot;4e:65:75:56:00:b4&quot;,
        &quot;bcmac&quot;: &quot;ff:ff:ff:00:00:b4&quot;,
        &quot;oldmac&quot;: &quot;&quot;,
        &quot;pmac&quot;: &quot;&quot;,
        &quot;pips&quot;: None,
    }
    send_msg({&apos;ctrl_add_mac&apos;: data})
 
 
def cfg_mac():
    data = {
        &quot;tap&quot;: False,
        &quot;macs&quot;: [&quot;1a:27:f3:87:3a:27&quot;],
    }
    send_msg({&apos;ctrl_cfg_mac&apos;: data})
 
add_internal_net()
add_srvc_port()
add_mac()
cfg_mac()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;可以看到neuvector的引流方式和容器使用的cni无关&lt;/li&gt;
  &lt;li&gt;引流的准备操作实际是agent执行的，dp只是对经过vth的数据包根据策略规则进行过滤，并转发&lt;/li&gt;
  &lt;li&gt;后期可以探索这种使用方式是否可以应用到虚拟机状态下&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里只是调用dp接口，dp内部实现将在后面详细分析&lt;/p&gt;
</description>
        <pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2022/08/23/neuvector-dp/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2022/08/23/neuvector-dp/</guid>
        
        <category>neuvector</category>
        
        <category>deep packet inspection</category>
        
        <category>dpi</category>
        
        <category>tc</category>
        
        
      </item>
    
      <item>
        <title>Neuvector源码之 网络抓包</title>
        <description>&lt;h2 id=&quot;功能介绍&quot;&gt;功能介绍&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/neuvector_tcpdump.png&quot; alt=&quot;neuvector_pcap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;抓包功能是针对容器的功能，用户在界面选择某个容器点击抓包功能，可以控制抓包开始和结束，可以选择抓包时间；完成后可以下载对应的pcap格式的文件，
在本地的wireshark中直接打开进行分析。底层实际还是通过进入容器的网络namespace，执行tcpdump命令来实现。&lt;/p&gt;

&lt;p&gt;说明下：hostnetwork的容器暂不支持抓包功能&lt;/p&gt;

&lt;p&gt;API接口：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;neuvector\controller\rest\rest.go:1517

r.GET(&quot;/v1/sniffer&quot;, handlerSnifferList)
r.GET(&quot;/v1/sniffer/:id&quot;, handlerSnifferShow)
r.POST(&quot;/v1/sniffer&quot;, handlerSnifferStart)
r.PATCH(&quot;/v1/sniffer/stop/:id&quot;, handlerSnifferStop)
r.DELETE(&quot;/v1/sniffer/:id&quot;, handlerSnifferDelete)
r.GET(&quot;/v1/sniffer/:id/pcap&quot;, handlerSnifferGetFile)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;源码分析&quot;&gt;源码分析&lt;/h2&gt;

&lt;p&gt;这里我们重点看下创建，也就是handlerSnifferStart的代码流程：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func handlerSnifferStart(w http.ResponseWriter, r *http.Request, ps httprouter.Params) {
  ...
  # 从request中获取对应的参数，这里是workloadid，也就是对应的pause容器的container id
  query := restParseQuery(r)

  # 获取容器id参数，并根据容器id获取对应的agentid
  agentID, wlID, err := getAgentWorkloadFromFilter(query.filters, acc)
  if err != nil {
      restRespNotFoundLogAccessDenied(w, login, err)
      return
  }

  // Check if we can config workload
  wl, err := cacher.GetWorkloadBrief(wlID, &quot;&quot;, acc)
  if wl == nil {
      restRespNotFoundLogAccessDenied(w, login, err)
      return
  } else if !acc.Authorize(&amp;amp;share.CLUSSnifferDummy{WorkloadDomain: wl.Domain}, nil) {
      restRespAccessDenied(w, login)
      return
  }
  ...

  args := proc.Sniffer
  req := &amp;amp;share.CLUSSnifferRequest{WorkloadID: wlID, Cmd: share.SnifferCmd_StartSniffer}
  ...

  res, err := rpc.SnifferCmd(agentID, req)
  ...
  restRespSuccess(w, r, &amp;amp;resp, acc, login, &amp;amp;proc, &quot;Start sniffer&quot;)
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;代码会从request中获取需要抓包的容器id&lt;/li&gt;
  &lt;li&gt;getAgentWorkloadFromFilter中获取容器id并查询对应的agent id&lt;/li&gt;
  &lt;li&gt;GetWorkloadBrief 获取指定容器的详细信息，并校验容器是否允许抓包&lt;/li&gt;
  &lt;li&gt;SnifferCmd 通过grpc调用对应agent的抓包接口，这里会提前配置一些抓包的参数，包括文件名称、文件大小（默认2M）、抓包时间等等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们在agent中查看对应的调用接口SnifferCmd，文件位置：neuvector\agent\service.go:830&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func (rs *RPCService) SnifferCmd(ctx context.Context, req *share.CLUSSnifferRequest) (*share.CLUSSnifferResponse, error) {
    if req.Cmd == share.SnifferCmd_StartSniffer {
        id, err := startSniffer(req)
        return &amp;amp;share.CLUSSnifferResponse{ID: id}, err
    } else if req.Cmd == share.SnifferCmd_StopSniffer {
        return &amp;amp;share.CLUSSnifferResponse{}, stopSniffer(req.ID)
    } else if req.Cmd == share.SnifferCmd_RemoveSniffer {
        return &amp;amp;share.CLUSSnifferResponse{}, removeSniffer(req.ID)
    }
    return &amp;amp;share.CLUSSnifferResponse{}, grpc.Errorf(codes.InvalidArgument, &quot;Invalid sniffer command&quot;)
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;继续查看对应的startSniffer方法：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func startSniffer(info *share.CLUSSnifferRequest) (string, error) {
    var pid int

    gInfoRLock()
    c, ok := gInfo.activeContainers[info.WorkloadID]
    ...

    proc := &amp;amp;procInfo{
        workload:   info.WorkloadID,
        fileNumber: uint(info.FileNumber),
        duration:   uint(info.DurationInSecond),
    }

    key := generateSnifferID()

    proc.fileName, proc.args = parseArgs(info, key[:share.SnifferIdAgentField])
    _, err := startSnifferProc(key, proc, pid)
    if err != nil {
        return &quot;&quot;, grpc.Errorf(codes.Internal, err.Error())
    } else {
        return key, nil
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;这里根据容器id或者内存中容器对象，这个对象实际是通过独立线程监听节点的runtime维护的信息&lt;/li&gt;
  &lt;li&gt;generateSnifferID 这个是根据agent id生成一个id作为文件名称的一部分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;生成tcpdump命令代码&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func parseArgs(info *share.CLUSSnifferRequest, keyname string) (string, []string) {
    ...
    filename = defaultPcapDir + keyname + &quot;_&quot;
    filenumber = fmt.Sprintf(&quot;%d&quot;, info.FileNumber)
    filesize = fmt.Sprintf(&quot;%d&quot;, info.FileSizeInMB)
    ...

    tcpdumpCmd := []string{&quot;-i&quot;, &quot;any&quot;, &quot;-U&quot;, &quot;-C&quot;}
    cmdStr = append(tcpdumpCmd, filesize, &quot;-w&quot;, filename, &quot;-W&quot;, filenumber)
    ...
    return filename, cmdStr
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;parseArgs用来生成完整的文件名称，并准备具体的tcpdump命令，完整的命令类似： tcpdump -i any -U -C 2 -w /var/neuvector/pcap/0a5bdf2c_0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面就是进入容器的network namespace然后执行tcpdump命令&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;func startSnifferProc(key string, proc *procInfo, pid int) (string, error) {
    ...

    var script string
    if proc.duration &amp;gt; 0 {
        script = fmt.Sprintf(&quot;timeout %d &quot;, proc.duration)
    }
    script += &quot;tcpdump &quot; + strings.Join(proc.args, &quot; &quot;)
    log.WithFields(log.Fields{&quot;key&quot;: key, &quot;cmd&quot;: script}).Debug()

    proc.cmd = exec.Command(system.ExecNSTool, system.NSActRun, &quot;-i&quot;, &quot;-n&quot;, global.SYS.GetNetNamespacePath(pid))
    proc.cmd.SysProcAttr = &amp;amp;syscall.SysProcAttr{Setsid: true}
    proc.cmd.Stderr = &amp;amp;proc.errb
    stdin, err := proc.cmd.StdinPipe()
    if err != nil {
        e := fmt.Errorf(&quot;Open nsrun stdin error&quot;)
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(e)
        return &quot;&quot;, e
    }

    err = proc.cmd.Start()
    if err != nil {
        e := fmt.Errorf(&quot;Failed to start sniffer&quot;)
        log.WithFields(log.Fields{&quot;error&quot;: err}).Error(e)
        return &quot;&quot;, e
    }

    pgid := proc.cmd.Process.Pid
    global.SYS.AddToolProcess(pgid, pid, &quot;sniffer&quot;, script)

    io.WriteString(stdin, script)
    stdin.Close()

    ...
    return status, err
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;这里就是通过nstool工具进入容器network namespace, 将tcpdump命令作为stdin在namespace中执行&lt;/li&gt;
  &lt;li&gt;完整的命令类似：echo “tcpdump -i any -U -C 2 -w /var/neuvector/pcap/xxx  -W 5” | ./nstools run -i -n /proc/2271/ns/net&lt;/li&gt;
  &lt;li&gt;nstools这个工具类似nsenter，为了安全工具内部会校验调用方必须是neuvector agent服务，所以一般情况下执行这个命令是会失败的&lt;/li&gt;
  &lt;li&gt;监听tcpdump进程状态并返回状态信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其他方法比如stop、下载抓包文件的调用路径是类似的&lt;/p&gt;

&lt;p&gt;nstools工具使用（移除父进程校验后）：
&lt;img src=&quot;/blog/img/neuvector_nstools.png&quot; alt=&quot;neuvector_pcap&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2022/08/10/neuvector-tcpdump/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2022/08/10/neuvector-tcpdump/</guid>
        
        <category>tcpdump</category>
        
        <category>neuvector</category>
        
        <category>抓包</category>
        
        
      </item>
    
  </channel>
</rss>
