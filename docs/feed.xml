<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hujin Blog</title>
    <description>Hujin，Openstack &amp; SDN &amp; Kubernetes Lover，Software Engineer，| 与你一起发现更大的世界</description>
    <link>http://0.0.0.0:4000/blog/</link>
    <atom:link href="http://0.0.0.0:4000/blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 10 Aug 2021 05:38:18 +0000</pubDate>
    <lastBuildDate>Tue, 10 Aug 2021 05:38:18 +0000</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Docker Containerd CRI-O Runc等概念的区别[译]</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;从Docker引发这场热潮以来，越来越多的工具和标准涌现，来帮助用户使用“容器”这个技术&lt;/p&gt;

&lt;p&gt;大型的技术公司由于相互之间的竞争，引入越来越多的技术概念和实现，使得普通用户很容易困惑和不理解。&lt;/p&gt;

&lt;p&gt;本文将详细介绍所有相关的概念名称，并尝试解释具体术语，解释容器生态系统如何在2021年协同工作。&lt;/p&gt;

&lt;p&gt;你不是唯一一个不理解这些概念的人，也不是最后一个…&lt;/p&gt;

&lt;h2 id=&quot;理解docker&quot;&gt;理解Docker&lt;/h2&gt;
&lt;p&gt;区别Docker这个公司，Docker容器，Docker镜像以及我们使用的Docker开发工具。&lt;/p&gt;

&lt;p&gt;我们需要认识一点：容器并不是和Docker紧密耦合，Docker只是容器整套工具的一种&lt;/p&gt;

&lt;h2 id=&quot;架构&quot;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/container-ecosystem.png&quot; alt=&quot;container_ecosystem_arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;容器生态系统由很多专业的技术、大量技术术语还包括大公司的竞争组成&lt;/p&gt;

&lt;p&gt;不过幸运的是，这些大公司偶尔会休战并站到一起，去制定一些标准，这些标准将有助于容器生态系统的操作便捷、跨平台、减少对某个公司或者项目的依赖&lt;/p&gt;

&lt;p&gt;相关的主要标准有：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Container Runtime Interface(CRI): 定义了容器runtime和K8S的交互API&lt;/li&gt;
  &lt;li&gt;Open Container Initiative(OCI)：定义发布镜像和容器的规则&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;docker&quot;&gt;Docker&lt;/h2&gt;
&lt;p&gt;由于Docker是使用容器最流行的开发工具，必须先从Docker开始。对很多人来说，“Docker”和“Container”两个词是等价的&lt;/p&gt;

&lt;p&gt;Docker公司触发了整个容器革命，并且提供了一套非常易使用的容器开发工具，称为“Docker”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/container-ecosystem-docker.png&quot; alt=&quot;container_ecosystem_arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Docker被设计安装在工作站或者服务器上，并附带一系列方便开发者build和run容器的工具&lt;/p&gt;

&lt;p&gt;Docker命令行工具可以从registry pull镜像，可以build容器镜像，也可以create/start容器&lt;/p&gt;

&lt;p&gt;Docker项目包含：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;docker-cli： 这是和容器交互的docker命令行工具&lt;/li&gt;
  &lt;li&gt;containerd： 这是一个daemon进程，用来管理并运行容器。可以push、pull镜像，管理存储和网络，监控运行中的容器&lt;/li&gt;
  &lt;li&gt;runc: 这是底层容器runtime，主要包含使用GO语言实现的libcontainer组件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当使用docker运行一个容器时，执行流程是： docker daemon – containerd – runc&lt;/p&gt;

&lt;h3 id=&quot;dockershim-docker-in-kubernetes&quot;&gt;Dockershim: Docker in Kubernetes&lt;/h3&gt;

&lt;p&gt;Kubernetes包括一个叫dockershim的组件，用来支持docker&lt;/p&gt;

&lt;p&gt;Kubernetes希望使用任意的容器runtime来运行容器，只需要支持CRI规范即可。&lt;/p&gt;

&lt;p&gt;但是由于Docker比K8S出现还早，Docker出现时CRI规范还没有形成，所以在K8S发布后，K8S官方通过dockershim组件来支持Docker&lt;/p&gt;

&lt;p&gt;K8S将会在后面直接移除对Docker的支持，也就是移除dockershim，只会使用支持CRI的runtime，类似containerd或者CRI-O&lt;/p&gt;

&lt;p&gt;这个并不意味着K8S不能运行Docker格式的容器，Containerd和CRI-O都可以运行Docker格式的容器，因为他们都属于OCI格式的镜像&lt;/p&gt;

&lt;p&gt;所以如果你想在K8S中使用容器，并不一定要安装Docker，containerd或者CRI-O等实现了CRI接口的组件都可以成为你的选择&lt;/p&gt;

&lt;h3 id=&quot;docker-images&quot;&gt;Docker images&lt;/h3&gt;
&lt;p&gt;大家常说的Docker镜像，实际是OCI格式的镜像，属于同一个规范&lt;/p&gt;

&lt;p&gt;所以当你从DockerHub或其他registry上pull一个镜像，你可以直接通过docker工具来使用它，也可以在K8S集群使用，也可以通过podman或者任意支持OCI标准的工具来使用这个镜像&lt;/p&gt;

&lt;h2 id=&quot;cri&quot;&gt;CRI&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/container-ecosystem-cri.png&quot; alt=&quot;container_ecosystem_arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CRI是K8S用来管理不同runtime 创建和管理容器的API规范&lt;/p&gt;

&lt;p&gt;CRI使得K8S更容易使用不同的容器runtime，定义了K8S如何和每个runtime交互&lt;/p&gt;

&lt;p&gt;因此实际上只取决于runtime本身如何管理容器，只要这个runtime支持CRI API的规范，类似的你可以根据喜好任意选择使用containerd或者CRI-O作为你的runtime，因为他们都支持CRI规范&lt;/p&gt;

&lt;p&gt;CRI设计上是可插拔的，如果你是终端用户，那么CRI具体的实现细节其实你是不关心的，你只需要使用一个支持这个规范的组件，能实现某些功能即可。&lt;/p&gt;

&lt;p&gt;当前Red Hat负责维护CRI-O，Docker负责维护自家的产品Containerd&lt;/p&gt;

&lt;h3 id=&quot;containerd&quot;&gt;Containerd&lt;/h3&gt;
&lt;p&gt;Containerd是Docker公司负责开发的高级容器runtime，支持CRI规范。从registry中pull镜像、管理镜像，负责和底层具体的runtime交互&lt;/p&gt;

&lt;p&gt;Containerd已经从Docker项目分离出来，是一个独立的项目，使得Docker更加模块化&lt;/p&gt;

&lt;p&gt;也就是说Docker内部会使用containerd，当前安装Docker时，也会安装Containerd&lt;/p&gt;

&lt;p&gt;Conatinerd通过cri插件支持k8s CRI规范&lt;/p&gt;

&lt;h3 id=&quot;cri-o&quot;&gt;CRI-O&lt;/h3&gt;

&lt;p&gt;CRI-O是支持容器CRI规范的另一种高级容器runtime，它是containerd的一个替代选择，从registry获取镜像，在磁盘上管理镜像，然后调用底层runtime来运行容器进程&lt;/p&gt;

&lt;p&gt;CRI-O是由Red Hat/IBM/Intel/SUSE等厂商提出并维护&lt;/p&gt;

&lt;p&gt;在K8S中提供启动、停止、重启容器的能力，类似containerd&lt;/p&gt;

&lt;h2 id=&quot;oci&quot;&gt;OCI&lt;/h2&gt;

&lt;p&gt;OCI是一群技术公司组织定义的容器镜像格式、容器如何运行的规范&lt;/p&gt;

&lt;p&gt;OCI的设计思想是允许用户选择任意符合规范的runtime，这些不同的runtime都有各自的实现，比如可以针对linux 有一个runtime，同时针对windows还有一个&lt;/p&gt;

&lt;p&gt;OCI得益于：one standard, many implementations, 一种标准，多种实现&lt;/p&gt;

&lt;h3 id=&quot;runc&quot;&gt;runc&lt;/h3&gt;

&lt;p&gt;runc是一种支持OCI规范的容器runtime&lt;/p&gt;

&lt;p&gt;runc为容器提供所有的底层功能，和底层Linux模块交互，类似namespace、cgroup等模块，通过这些模块来创建和运行容器进程&lt;/p&gt;

&lt;p&gt;runc的可替代方案：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;crun： 使用C语言编写的runtime（runc使用Go语言编写）&lt;/li&gt;
  &lt;li&gt;kata-runtime： 由katacontainers项目提供，在支持OCI规范下，提供一个轻量级虚拟机功能&lt;/li&gt;
  &lt;li&gt;gVisor：Google提供，在支持OCI规范下，支持容器有独立的内核模块&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;从上文可以发现，Docker只是很多容器组件的一小部分&lt;/p&gt;

&lt;p&gt;K8S通过制定CRI来实现对不同runtime的交互，这里CRI包括：containerd和CRI-O，dockershim即将不支持&lt;/p&gt;

&lt;p&gt;对应的runtime标准是OCI，这里包括runc、kata-runtime以及其他runtime，用来具体和底层的Linux交互，并最终创建和运行一个真正的容器&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/08/06/cri-oci/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/08/06/cri-oci/</guid>
        
        <category>k8s</category>
        
        <category>cni</category>
        
        <category>oci</category>
        
        <category>runc</category>
        
        <category>containerd</category>
        
        
      </item>
    
      <item>
        <title>Multus CNI</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;当前社区支持多个cni插件，当前集群底层网络实现变化后存在切换CNI插件的需求。由于cni插件主要在pod网卡创建和删除流程中使用，切换CNI插件后，要求pod重建，服务和网络都会中断&lt;/p&gt;

&lt;p&gt;这里需要一种更加平滑的切换方式，在保证页面和网络不中断的情况下切换CNI&lt;/p&gt;

&lt;h2 id=&quot;架构&quot;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/multus1.png&quot; alt=&quot;multus_arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图可以看到，一个pod可以同时有多个网卡，其中eth0是默认网卡，也就是默认路由指向的网卡&lt;/p&gt;

&lt;p&gt;net0和net1是通过在pod的annotation中指定的网络创建的网卡&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/multus2.png&quot; alt=&quot;multus_arch2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里通过创建pod来看multus-cni的实现方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;multus-cni实际是一个CNI插件，实现的接口也是CmdAdd CmdCheck CmdDel，这里通过提供CRD：NetworkAttachmentDefinition来定义其他CNI插件的内容，格式见下文&lt;/li&gt;
  &lt;li&gt;在kubelet调用cni创建网卡时，multus-cni会通过查询pod annotation定义的key获取是否有指定使用的cni插件，并通过调用k8s api获取crd的具体数据&lt;/li&gt;
  &lt;li&gt;将指定的pod和crd中定义的cni数据进行处理，最终得到符合CNI规范的格式，这个配置实际是保存在内存中的&lt;/li&gt;
  &lt;li&gt;调用cni提供的AddNetwork方法，将内存中第三方的cni配置作为参数传递过去（默认网络在内存中也是一种CNI配置）&lt;/li&gt;
  &lt;li&gt;最终实现动态创建POD网卡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;annotations key：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;k8s.v1.cni.cncf.io/resourceName：创建网络CRD时指定的资源名称，一般用来指定底层物理设备的名称，比如sriov场景&lt;/li&gt;
  &lt;li&gt;v1.multus-cni.io/default-network：配置pod的默认网络，只能配置一个。用来替换k8s集群中原来默认的网络&lt;/li&gt;
  &lt;li&gt;k8s.v1.cni.cncf.io/networks：配置pod的网络，支持配置多个&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这里的网络是指通过multus crd建出来的network-attachment-definition对象&lt;/li&gt;
  &lt;li&gt;指定网络时支持指定ip、mac、ifname、qos、portmap、gateway、device id&lt;/li&gt;
  &lt;li&gt;默认路由对应的网卡名称不支持指定&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;部署&quot;&gt;部署&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;git clone https://github.com/k8snetworkplumbingwg/multus-cni.git
cd multus-cni
kubectl apply -f images/multus-daemonset.yml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;打包&quot;&gt;打包&lt;/h2&gt;
&lt;p&gt;编译可执行文件&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cd multus-cni
./build
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;打包镜像，使用分支：release-3.7&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cd multus-cni
docker build --file ./Dockerfile -t harbor.huayun.org/huayun-kubernetes/multus-cni:dev .
docker push harbor.huayun.org/huayun-kubernetes/multus-cni:dev
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;功能验证&quot;&gt;功能验证&lt;/h2&gt;
&lt;p&gt;创建macvlan NetworkAttachmentDefinition&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: macvlan-conf
spec:
  config: &apos;{
      &quot;cniVersion&quot;: &quot;0.3.0&quot;,
      &quot;type&quot;: &quot;macvlan&quot;,
      &quot;master&quot;: &quot;ens192&quot;,
      &quot;mode&quot;: &quot;bridge&quot;,
      &quot;ipam&quot;: {
        &quot;type&quot;: &quot;host-local&quot;,
        &quot;subnet&quot;: &quot;192.168.1.0/24&quot;,
        &quot;rangeStart&quot;: &quot;192.168.1.200&quot;,
        &quot;rangeEnd&quot;: &quot;192.168.1.216&quot;,
        &quot;routes&quot;: [
          { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }
        ],
        &quot;gateway&quot;: &quot;192.168.1.1&quot;
      }
    }&apos;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建pod并指定使用macvlan cni&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;apiVersion: v1
kind: Pod
metadata:
  name: samplepod
  annotations:
    k8s.v1.cni.cncf.io/networks: macvlan-conf
spec:
  containers:
  - name: samplepod
    command: [&quot;/bin/ash&quot;, &quot;-c&quot;, &quot;trap : TERM INT; sleep infinity &amp;amp; wait&quot;]
    image: alpine
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;查看pod中网卡创建情况&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 multus]# kubectl get pods  -owide
NAME        READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
busybox     1/1     Running   213        8d    10.244.1.12   node2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
samplepod   1/1     Running   0          8s    10.244.2.22   node3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@node1 multus]# kubectl exec -it samplepod -- ip a 
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: tunl0@NONE: &amp;lt;NOARP&amp;gt; mtu 1480 qdisc noop state DOWN qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if19: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu 1440 qdisc noqueue state UP 
    link/ether a2:3e:de:3e:fd:d0 brd ff:ff:ff:ff:ff:ff
    inet 10.244.2.22/32 scope global eth0
       valid_lft forever preferred_lft forever
5: net1@if14: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN 
    link/ether 32:b2:70:77:50:df brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.201/24 brd 192.168.1.255 scope global net1
       valid_lft forever preferred_lft forever
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建pod时支持指定多种cni&lt;/li&gt;
  &lt;li&gt;pod中存在多种cni的网卡，默认路由使用默认网络的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不支持更新，pod如果已经创建，不支持动态创建其他cni的网卡
    &lt;blockquote&gt;
      &lt;p&gt;查看代码发现multus实际也是走的CNI的流程，实现CmdAdd和CmdDelete接口
也就是说在pod创建完成后，CNI是不响应增加和删除网卡操作的，只响应创建和删除POD的操作&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/06/28/k8s-multus/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/06/28/k8s-multus/</guid>
        
        <category>k8s</category>
        
        <category>cni</category>
        
        <category>multus</category>
        
        
      </item>
    
      <item>
        <title>KubeOVN - VPCNatGateway</title>
        <description>&lt;h2 id=&quot;功能&quot;&gt;功能&lt;/h2&gt;
&lt;p&gt;用来给k8s中pod/service/ingress等网络资源访问外部网络&lt;/p&gt;

&lt;h3 id=&quot;支持功能&quot;&gt;支持功能&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;指定外部网络出口物理网卡&lt;/li&gt;
  &lt;li&gt;指定多子网以及子网的网关&lt;/li&gt;
  &lt;li&gt;浮动IP功能&lt;/li&gt;
  &lt;li&gt;端口转发功能&lt;/li&gt;
  &lt;li&gt;静态路由规则&lt;/li&gt;
  &lt;li&gt;SNAT功能&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/blog/img/vpc-nat-gateway.png&quot; alt=&quot;kube-ovn vpc-nat-gateway&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建一个nat gateway deployment，副本数是1&lt;/li&gt;
  &lt;li&gt;通过configmap获取pod对应的镜像文件路径，创建nat gateway pod；重建策略是recreate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pod中设置annotations：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ovn.kubernetes.io/vpc_nat_gw： nat gateway的名称&lt;/li&gt;
  &lt;li&gt;k8s.v1.cni.cncf.io/networks： 指定pod对应namespace的外部网络&lt;/li&gt;
  &lt;li&gt;ovn.kubernetes.io/logical_switch：为nat gateway添加的子网列表&lt;/li&gt;
  &lt;li&gt;ovn.kubernetes.io/ip_address：为nat gateway设置的网关IP列表&lt;/li&gt;
  &lt;li&gt;等待nat gateway pod变成Running后，触发nat gateway初始化，执行命令： 
  bash /kube-ovn/nat-gateway.sh init&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;init执行的命令：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;ip link set net1 up
ip rule add iif net1 table $ROUTE_TABLE
ip rule add iif eth0 table $ROUTE_TABLE

# add static chain
iptables -t nat -N DNAT_FILTER
iptables -t nat -N SNAT_FILTER
iptables -t nat -N EXCLUSIVE_DNAT # floatingIp DNAT
iptables -t nat -N EXCLUSIVE_SNAT # floatingIp SNAT
iptables -t nat -N SHARED_DNAT
iptables -t nat -N SHARED_SNAT

iptables -t nat -A PREROUTING -j DNAT_FILTER
iptables -t nat -A DNAT_FILTER -j EXCLUSIVE_DNAT
iptables -t nat -A DNAT_FILTER -j SHARED_DNAT

iptables -t nat -A POSTROUTING -j SNAT_FILTER
iptables -t nat -A SNAT_FILTER -j EXCLUSIVE_SNAT
iptables -t nat -A SNAT_FILTER -j SHARED_SNAT
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;vpc nat gateway相关线程说明:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;addVpcWorker:监听vpc创建事件，并在ovn中创建logical router&lt;/li&gt;
  &lt;li&gt;AddOrUpdateVpcNatGwWorker：监听vpc nat gateway创建或更新事件，创建或更新对应的nat gateway deployment&lt;/li&gt;
  &lt;li&gt;initVpcNatGwWorker：初始化nat gateway deployment中对应的pod容器，执行命令见上文&lt;/li&gt;
  &lt;li&gt;delVpcNatGwWorker：删除nat gateway deployment&lt;/li&gt;
  &lt;li&gt;updateVpcEipWorker：找到nat gateway deployment对应的pod，获取annotations中eip相关字段，判断需要删除和新增的eip，并更新到annotations中&lt;/li&gt;
  &lt;li&gt;updateVpcFloatingIpWorker：在nat gateway 中更新浮动IP的映射关系&lt;/li&gt;
  &lt;li&gt;updateVpcDnatWorker：在nat gateway 中更新端口转发的映射关系&lt;/li&gt;
  &lt;li&gt;updateVpcSnatWorker：在nat gateway 中更新子网和eip的snat的映射关系&lt;/li&gt;
  &lt;li&gt;updateVpcSubnetWorker：在nat gateway 中新增、删除子网网卡，创建或者删除对应子网的路由信息&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;使用方式&quot;&gt;使用方式&lt;/h2&gt;
&lt;p&gt;开启VPC Nat Gatway功能&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;kind: ConfigMap
apiVersion: v1
metadata:
  name: ovn-vpc-nat-gw-config
  namespace: kube-system
data:
  image: &apos;kubeovn/vpc-nat-gateway:v1.7.0&apos;  # Docker image for vpc nat gateway
  enable-vpc-nat-gw: true                  # &apos;true&apos; for enable, &apos;false&apos; for disable 
  nic: eth1                                # The nic that connect to underlay network, use as the &apos;master&apos; for macvlan
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建VPC Nat Gateway&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;kind: VpcNatGateway
apiVersion: kubeovn.io/v1
  name: ngw
spec:
  vpc: test-vpc-1                  # Specifies which VPC the gateway belongs to 
  subnet: sn                       # Subnet in VPC  
  lanIp: 10.0.1.254                # Internal IP for nat gateway pod, IP should be within the range of the subnet 
  eips:                            # Underlay IPs assigned to the gateway
    - eipCIDR: 192.168.0.111/24
       gateway: 192.168.0.254
    - eipCIDR: 192.168.0.112/24
       gateway: 192.168.0.254
  floatingIpRules: 
    - eip: 192.168.0.111
      internalIp: 10.0.1.5
  dnatRules:
    - eip: 192.168.0.112
      externalPort: 8888
      protocol: tcp
      internalIp: 10.0.1.10
      internalPort: 80
  snatRules:
    - eip: 192.168.0.112
       internalCIDR: 10.0.1.0/24
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;给VPC添加静态路由&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;kind: Vpc
apiVersion: kubeovn.io/v1
metadata:
  name: test-vpc-1
spec:
  staticRoutes:
    - cidr: 0.0.0.0/0
      nextHopIP: 10.0.1.254     # Should be the same as the &apos;lanIp&apos; for vpc gateway
      policy: policyDst
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/05/24/k8s-kubeovn-vpcnatgateway/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/05/24/k8s-kubeovn-vpcnatgateway/</guid>
        
        <category>k8s</category>
        
        <category>cni</category>
        
        <category>kube-ovn</category>
        
        <category>vpc-nat-gateway</category>
        
        
      </item>
    
      <item>
        <title>Kuryr CNI</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;当k8s部署在openstack节点或者虚拟机中时，可以使用openstack提供的网络功能实现k8s cni，这里主要对接的是neutron和lbaas模块，社区提供了kuryr的实现方案。
Kuryr 是 OpenStack Neutron 的子项目，其主要目标是透过该项目来集成 OpenStack 与 Kubernetes 的网络。该项目在 Kubernetes 中实作了原生 Neutron-based 的网络，
因此使用 Kuryr-Kubernetes 可以让 OpenStack VM 与 Kubernetes Pods 能够选择在同一个子网络上运作，并且能够使用 Neutron L3 与 Security Group 来对网络进行路由，以及阻挡特定来源 Port，并且也提供基于 Neutron LBaaS 的 Service 集成。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/kuryr_k8s_arch.png&quot; alt=&quot;kuryr_kubernetes_arch&quot; /&gt;
&lt;img src=&quot;/blog/img/kuryr_k8s_pipline.png&quot; alt=&quot;kuryr_kubernetes_arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;kuryr kubernetes支持两种部署模式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;nested：嵌套部署，通过虚机网卡子接口的方式实现，依赖社区trunk port功能&lt;/li&gt;
  &lt;li&gt;none-nested：非嵌套部署，pod网卡类似虚机网卡直接添加到OVS网桥中，通过ovn-agent管理流表&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;控制节点&lt;/p&gt;

&lt;p&gt;通过kuryr controller来watch kubernetes资源创建、更新和删除&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pod&lt;/li&gt;
  &lt;li&gt;service&lt;/li&gt;
  &lt;li&gt;endpoints&lt;/li&gt;
  &lt;li&gt;namespace&lt;/li&gt;
  &lt;li&gt;ingress(待定)&lt;/li&gt;
  &lt;li&gt;networkpolicy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在watch到资源创建后，在neutron中创建对应的网络资源
Kubernetes|Neutron |说明&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;Pod&lt;/td&gt;
          &lt;td&gt;VM&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;Service&lt;/td&gt;
          &lt;td&gt;LB&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;Ingress&lt;/td&gt;
          &lt;td&gt;L7Router/LB&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;Endpoints&lt;/td&gt;
          &lt;td&gt;LB&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;NetworkPolicy&lt;/td&gt;
          &lt;td&gt;SecurityGroup&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;Namespaces&lt;/td&gt;
          &lt;td&gt;Network&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;计算节点&lt;/p&gt;

&lt;p&gt;计算节点中有两个服务：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;kuryr cni：kuryr cni当前有python和golang两种实现，golang实现是标准的cni方式，社区慢慢往这个方向切换&lt;/li&gt;
  &lt;li&gt;kuryr daemon服务：为cni提供api服务用来配置、更新和删除容器网卡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当kubelet收到pod创建事件时，kubelet会调用kuryr cni的方法cmdAdd，这里kuryrcni会调用kuryr daemon的addNetwork接口&lt;/p&gt;

&lt;p&gt;kuryr daemon有三个线程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;watcher：watch k8s中当前节点上的pod操作&lt;/li&gt;
  &lt;li&gt;server：提供api供kuryr cni调用；对watch到的pod执行addNetwork：创建并配置pod网卡，delNetwork:删除pod网卡&lt;/li&gt;
  &lt;li&gt;health check： 健康检查&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PS: kuryr controller在watch 资源创建后，会在neutron中创建对应资源，同时更新k8s中资源，将neutron中的资源信息设置到annotations中&lt;/p&gt;

&lt;h2 id=&quot;安装三节点openstack&quot;&gt;安装三节点OpenStack&lt;/h2&gt;

&lt;p&gt;准备&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat  /etc/yum.repos.d/qemu-kvm-rhev.repo
[qemu-kvm-rhev]
name=oVirt rebuilds of qemu-kvm-rhev
baseurl=http://resources.ovirt.org/pub/ovirt-3.5/rpm/el7Server/
mirrorlist=http://resources.ovirt.org/pub/yum-repo/mirrorlist-ovirt-3.5-el7Server
enabled=1
skip_if_unavailable=1
gpgcheck=0

yum install -y centos-release-openstack-train
yum update -y
yum install -y openstack-packstack
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;生成部署配置文件并修改&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;packstack --gen-answer-file=/root/answer.txt
CONFIG_CONTROLLER_HOST=192.168.1.30
CONFIG_COMPUTE_HOSTS=192.168.1.31,192.168.1.32
CONFIG_NETWORK_HOSTS=192.168.1.30
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;开始部署&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;packstack --answer-file=./answer.conf
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ps: 部署时发现nova报qmeu版本需要2.8以上，这里参考文档https://www.huaweicloud.com/articles/023bb022255569c81600e6e372fa06c0.html安装2.8版本的qemu&lt;/p&gt;

&lt;h2 id=&quot;部署kubernetes&quot;&gt;部署Kubernetes&lt;/h2&gt;
&lt;p&gt;TODO: kubespray&lt;/p&gt;

&lt;p&gt;修改apiserver配置，开放8080端口&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim /etc/kubernetes/manifests/kube-apiserver.yaml
- --insecure-bind-address=0.0.0.0
- --insecure-port=8080
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;部署kuryr&quot;&gt;部署kuryr&lt;/h2&gt;

&lt;h3 id=&quot;控制节点&quot;&gt;控制节点&lt;/h3&gt;

&lt;p&gt;获取代码&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;git clone https://opendev.org/openstack/kuryr-kubernetes.git
cd kuryr-kubernetes
git checkout remotes/origin/stable/train
cd ..
pip install -e kuryr-kubernetes
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;生成配置文件&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cd kuryr-kubernetes
./tools/generate_config_file_samples.sh
mkdir -p /etc/kuryr/
cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建kuryr租户和用户&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;openstack project create --domain Default --description &quot;kuryr Project&quot; kuryr
openstack user create --domain Default --password kuryr kuryr --project kuryr
openstack role add --project kuryr --user kuryr admin
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建k8s pod、service网络&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;neutron net-create pod-net
neutron subnet-create pod-net 10.244.0.0/16

neutron net-create service-net
neutron subnet-create service-net 10.96.0.0/16

neutron security-group-create kuryr-sg
neutron security-group-rule-create kuryr-sg --direction ingress
neutron security-group-rule-create kuryr-sg --direction egress
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;修改kuryr controller配置&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim /etc/kuryr/kuryr.conf
[DEFAULT]use_stderr = true
bindir = /usr/local/libexec/kuryr

[kubernetes]
api_root = http://172.16.41.130:8080
enabled_handlers = vif,kuryrport  # lb, lbaasspec

[neutron]
auth_url = http://172.16.41.130:5000/v3
username = kuryr
user_domain_name = Default
password = kuryr
project_name = kuryr
project_domain_name = Default
auth_type = password

[neutron_defaults]
ovs_bridge = br-int
pod_security_groups = {id_of_secuirity_group_for_pods}
pod_subnet = {id_of_subnet_for_pods}
project = {id_of_project}
service_subnet = {id_of_subnet_for_k8s_services}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;启动kuryr controller服务（需要做成systemd服务）&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;kuryr-k8s-controller --config-file /etc/kuryr/kuryr.conf
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;计算节点&quot;&gt;计算节点&lt;/h3&gt;

&lt;p&gt;获取代码&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;git clone https://opendev.org/openstack/kuryr-kubernetes.git
cd kuryr-kubernetes
git checkout remotes/origin/stable/train
cd ..
pip install -e kuryr-kubernetes
pip install &apos;oslo.privsep&amp;gt;=1.20.0&apos; &apos;os-vif&amp;gt;=1.5.0&apos;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建配置文件&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim /etc/kuryr/kuryr.conf
[DEFAULT]
use_stderr = true
bindir = /usr/local/libexec/kuryr

[kubernetes]
api_root = http://172.16.41.130:8080
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建软链接kuryr-cni&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;mkdir -p /opt/cni/bin
ln -s $(which kuryr-cni) /opt/cni/bin/
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建cni配置文件&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;rm -rf /etc/cni/net.d/
mkdir -p /etc/cni/net.d/
vim /etc/cni/net.d/10-kuryr.conf
{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;kuryr&quot;,
    &quot;type&quot;: &quot;kuryr-cni&quot;,
    &quot;kuryr_conf&quot;: &quot;/etc/kuryr/kuryr.conf&quot;,
    &quot;debug&quot;: true
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;重启kubelet&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;systemctl daemon-reload &amp;amp;&amp;amp; systemctl restart kubelet.service
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;启动kuryr daemon服务&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;kuryr-daemon --config-file /etc/kuryr/kuryr.conf -d
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;自测&quot;&gt;自测&lt;/h2&gt;

&lt;p&gt;创建虚机&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@controller01 ~]# glance image-list
+--------------------------------------+--------+
| ID                                   | Name   |
+--------------------------------------+--------+
| 5d87d4cf-384f-4644-9e81-3aafaec567f9 | cirros |
| e9e97933-c2e7-40a7-bd39-9520be75f937 | cirros |
+--------------------------------------+--------+
[root@controller01 ~]# nova flavor-list
+----+-----------+------------+------+-----------+------+-------+-------------+-----------+-------------+
| ID | Name      | Memory_MiB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | Description |
+----+-----------+------------+------+-----------+------+-------+-------------+-----------+-------------+
| 1  | m1.tiny   | 512        | 1    | 0         | 0    | 1     | 1.0         | True      | -           |
| 2  | m1.small  | 2048       | 20   | 0         | 0    | 1     | 1.0         | True      | -           |
| 3  | m1.medium | 4096       | 40   | 0         | 0    | 2     | 1.0         | True      | -           |
| 4  | m1.large  | 8192       | 80   | 0         | 0    | 4     | 1.0         | True      | -           |
| 5  | m1.xlarge | 16384      | 160  | 0         | 0    | 8     | 1.0         | True      | -           |
+----+-----------+------------+------+-----------+------+-------+-------------+-----------+-------------+
[root@controller01 ~]# neutron net-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+--------------------------------------+-------------+----------------------------------+----------------------------------------------------+
| id                                   | name        | tenant_id                        | subnets                                            |
+--------------------------------------+-------------+----------------------------------+----------------------------------------------------+
| 032a6394-4efe-4ef5-949e-e9717bcaeed6 | public      | 298316f4f2574e898ba50b89cdae58c3 | 8255a4e7-4902-4bdf-9feb-5b552371c924 172.24.4.0/24 |
| acd5d8f3-8b1b-495a-98f2-78a8bb23290b | service-net | 497fd0c4ce624c18a8007c4f72f021f6 | a9f5361a-9e71-457c-bd3d-6e52b638f52c 10.96.0.0/16  |
| b3102426-e8d5-4a9e-b18b-6cdd28fd5af4 | pod-net     | 497fd0c4ce624c18a8007c4f72f021f6 | eddcc6a4-0c65-413b-9aaf-31548b31e10e 10.244.0.0/16 |
| b5962808-53ee-4ab4-a19b-f0396535302f | private     | 1fbe0c13d018486d8c53a112621b9fc1 | 39e76bea-fc39-432c-bb28-3b1e40a350f9 10.0.0.0/24   |
+--------------------------------------+-------------+----------------------------------+----------------------------------------------------+
[root@controller01 ~]# nova boot  --image e9e97933-c2e7-40a7-bd39-9520be75f937 --nic net-id=b3102426-e8d5-4a9e-b18b-6cdd28fd5af4 hujin1  --flavor 1
[root@controller01 ~]# nova list
+--------------------------------------+--------+--------+------------+-------------+---------------------+
| ID                                   | Name   | Status | Task State | Power State | Networks            |
+--------------------------------------+--------+--------+------------+-------------+---------------------+
| 88e871ff-05c8-45c4-9c09-3cbc6e61eb61 | hujin1 | ACTIVE | -          | Running     | pod-net=10.244.2.62 |
+--------------------------------------+--------+--------+------------+-------------+---------------------+
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建pod&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@controller01 ~]# cat busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:latest
    command:
      - sleep
      - &quot;3600&quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
[root@controller01 ~]# kubectl get pods -owide
NAME      READY   STATUS    RESTARTS   AGE   IP             NODE        NOMINATED NODE   READINESS GATES
busybox   1/1     Running   19         19h   10.244.2.195   compute02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;pod和虚机通信&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@controller01 ~]# nova list
+--------------------------------------+--------+--------+------------+-------------+---------------------+
| ID                                   | Name   | Status | Task State | Power State | Networks            |
+--------------------------------------+--------+--------+------------+-------------+---------------------+
| 88e871ff-05c8-45c4-9c09-3cbc6e61eb61 | hujin1 | ACTIVE | -          | Running     | pod-net=10.244.2.62 |
+--------------------------------------+--------+--------+------------+-------------+---------------------+
[root@controller01 ~]# kubectl get pods -owide
NAME      READY   STATUS    RESTARTS   AGE   IP             NODE        NOMINATED NODE   READINESS GATES
busybox   1/1     Running   19         19h   10.244.2.195   compute02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
[root@controller01 ~]# kubectl exec -it busybox -- sh
/ # ping 10.244.2.62
PING 10.244.2.62 (10.244.2.62): 56 data bytes
64 bytes from 10.244.2.62: seq=0 ttl=64 time=1.193 ms
64 bytes from 10.244.2.62: seq=1 ttl=64 time=0.961 ms
64 bytes from 10.244.2.62: seq=2 ttl=64 time=0.594 ms
64 bytes from 10.244.2.62: seq=3 ttl=64 time=0.685 ms
^C
--- 10.244.2.62 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 0.594/0.858/1.193 ms   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;pod和虚机互通实现&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@compute02 qemu]# ovs-vsctl show
0e110214-9c8c-438d-9bee-d1f4ad0f89d7
    Manager &quot;ptcp:6640:127.0.0.1&quot;
        is_connected: true
    Bridge br-int
        fail_mode: secure
        datapath_type: system
        Port &quot;tap59fdfbe4-10&quot;
            Interface &quot;tap59fdfbe4-10&quot;
        Port &quot;tap6e153f27-29&quot;
            Interface &quot;tap6e153f27-29&quot;
        Port &quot;ovn-b6c0da-0&quot;
            Interface &quot;ovn-b6c0da-0&quot;
                type: geneve
                options: {csum=&quot;true&quot;, key=flow, remote_ip=&quot;172.16.41.131&quot;}
        Port br-int
            Interface br-int
                type: internal
        Port &quot;ovn-11a523-0&quot;
            Interface &quot;ovn-11a523-0&quot;
                type: geneve
                options: {csum=&quot;true&quot;, key=flow, remote_ip=&quot;172.16.41.130&quot;}
        Port &quot;tap2d79fedf-f5&quot;
            Interface &quot;tap2d79fedf-f5&quot;
    ovs_version: &quot;2.12.0&quot;
[root@compute02 qemu]# ovs-ofctl show br-int
OFPT_FEATURES_REPLY (xid=0x2): dpid:0000ee3ac6759a4e
n_tables:254, n_buffers:0
capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP
actions: output enqueue set_vlan_vid set_vlan_pcp strip_vlan mod_dl_src mod_dl_dst mod_nw_src mod_nw_dst mod_nw_tos mod_tp_src mod_tp_dst
 7(tap2d79fedf-f5): addr:2a:c3:86:4f:bc:f2
     config:     0
     state:      0
     current:    10GB-FD COPPER
     speed: 10000 Mbps now, 0 Mbps max
 9(tap6e153f27-29): addr:fe:16:3e:bc:48:d9
     config:     0
     state:      0
     current:    10MB-FD COPPER
     speed: 10 Mbps now, 0 Mbps max
 12(tap59fdfbe4-10): addr:8e:f5:d0:5f:9d:4d
     config:     0
     state:      0
     current:    10GB-FD COPPER
     speed: 10000 Mbps now, 0 Mbps max
 13(ovn-b6c0da-0): addr:ea:25:f6:13:cc:a7
     config:     0
     state:      0
     speed: 0 Mbps now, 0 Mbps max
 14(ovn-11a523-0): addr:da:7e:b4:cd:6c:c2
     config:     0
     state:      0
     speed: 0 Mbps now, 0 Mbps max
 LOCAL(br-int): addr:ee:3a:c6:75:9a:4e
     config:     PORT_DOWN
     state:      LINK_DOWN
     speed: 0 Mbps now, 0 Mbps max
OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0
[root@compute02 qemu]# cat flows |grep reg15=0x5,metadata=0x4
 cookie=0x0, duration=23100.073s, table=33, n_packets=1094, n_bytes=107044, idle_age=158, priority=100,reg15=0x5,metadata=0x4 actions=load:0x1-&amp;gt;NXM_NX_REG13[],load:0x3-&amp;gt;NXM_NX_REG11[],load:0x2-&amp;gt;NXM_NX_REG12[],resubmit(,34)
 cookie=0x0, duration=23100.073s, table=34, n_packets=0, n_bytes=0, idle_age=23100, priority=100,reg10=0/0x1,reg14=0x5,reg15=0x5,metadata=0x4 actions=drop
 cookie=0xb623d20f, duration=23100.061s, table=44, n_packets=0, n_bytes=0, idle_age=23100, priority=34000,udp,reg15=0x5,metadata=0x4,dl_src=fa:16:3e:18:1f:10,nw_src=10.244.0.1,tp_src=67,tp_dst=68 actions=ct(commit,zone=NXM_NX_REG13[0..15]),resubmit(,45)
 cookie=0xc28406ec, duration=22063.406s, table=44, n_packets=0, n_bytes=0, idle_age=22063, priority=2002,ct_state=-new+est-rpl+trk,ct_label=0x1/0x1,ip,reg15=0x5,metadata=0x4 actions=load:0x1-&amp;gt;NXM_NX_XXREG0[97],resubmit(,45)
 cookie=0x22d5455a, duration=22063.406s, table=44, n_packets=47, n_bytes=4606, idle_age=22012, priority=2002,ct_state=-new+est-rpl+trk,ct_label=0/0x1,ip,reg15=0x5,metadata=0x4 actions=resubmit(,45)
 cookie=0xc28406ec, duration=22063.406s, table=44, n_packets=2, n_bytes=196, idle_age=22054, priority=2002,ct_state=+new-est+trk,ip,reg15=0x5,metadata=0x4 actions=load:0x1-&amp;gt;NXM_NX_XXREG0[97],resubmit(,45)
 cookie=0x43b1967d, duration=23100.063s, table=44, n_packets=0, n_bytes=0, idle_age=23100, priority=2001,ct_state=+est+trk,ct_label=0x1/0x1,ipv6,reg15=0x5,metadata=0x4 actions=drop
 cookie=0x4a7b7997, duration=23100.062s, table=44, n_packets=0, n_bytes=0, idle_age=23100, priority=2001,ct_state=+est+trk,ct_label=0/0x1,ip,reg15=0x5,metadata=0x4 actions=ct(commit,zone=NXM_NX_REG13[0..15],exec(load:0x1-&amp;gt;NXM_NX_CT_LABEL[0]))
 cookie=0x43b1967d, duration=23100.062s, table=44, n_packets=0, n_bytes=0, idle_age=23100, priority=2001,ct_state=+est+trk,ct_label=0x1/0x1,ip,reg15=0x5,metadata=0x4 actions=drop
 cookie=0x4a7b7997, duration=23100.061s, table=44, n_packets=0, n_bytes=0, idle_age=23100, priority=2001,ct_state=+est+trk,ct_label=0/0x1,ipv6,reg15=0x5,metadata=0x4 actions=ct(commit,zone=NXM_NX_REG13[0..15],exec(load:0x1-&amp;gt;NXM_NX_CT_LABEL[0]))
 cookie=0x43b1967d, duration=23100.063s, table=44, n_packets=0, n_bytes=0, idle_age=23100, priority=2001,ct_state=-est+trk,ipv6,reg15=0x5,metadata=0x4 actions=drop
 cookie=0x43b1967d, duration=23100.062s, table=44, n_packets=1036, n_bytes=101528, idle_age=22063, priority=2001,ct_state=-est+trk,ip,reg15=0x5,metadata=0x4 actions=drop
 cookie=0xa1320a97, duration=23100.062s, table=48, n_packets=0, n_bytes=0, idle_age=23100, priority=90,ip,reg15=0x5,metadata=0x4,dl_dst=fa:16:3e:57:d9:64,nw_dst=255.255.255.255 actions=resubmit(,49)
 cookie=0xa1320a97, duration=23100.062s, table=48, n_packets=55, n_bytes=5390, idle_age=158, priority=90,ip,reg15=0x5,metadata=0x4,dl_dst=fa:16:3e:57:d9:64,nw_dst=10.244.2.195 actions=resubmit(,49)
 cookie=0xa1320a97, duration=23100.061s, table=48, n_packets=0, n_bytes=0, idle_age=23100, priority=90,ip,reg15=0x5,metadata=0x4,dl_dst=fa:16:3e:57:d9:64,nw_dst=224.0.0.0/4 actions=resubmit(,49)
 cookie=0xe6b695a, duration=23100.062s, table=48, n_packets=0, n_bytes=0, idle_age=23100, priority=80,ip,reg15=0x5,metadata=0x4,dl_dst=fa:16:3e:57:d9:64 actions=drop
 cookie=0xe6b695a, duration=23100.062s, table=48, n_packets=0, n_bytes=0, idle_age=23100, priority=80,ipv6,reg15=0x5,metadata=0x4,dl_dst=fa:16:3e:57:d9:64 actions=drop
 cookie=0xf4e4ab82, duration=23100.063s, table=49, n_packets=58, n_bytes=5516, idle_age=158, priority=50,reg15=0x5,metadata=0x4,dl_dst=fa:16:3e:57:d9:64 actions=resubmit(,64)
 cookie=0x0, duration=23100.073s, table=64, n_packets=3, n_bytes=126, idle_age=176, priority=100,reg10=0x1/0x1,reg15=0x5,metadata=0x4 actions=push:NXM_OF_IN_PORT[],load:0-&amp;gt;NXM_OF_IN_PORT[],resubmit(,65),pop:NXM_OF_IN_PORT[]
 cookie=0x0, duration=23100.073s, table=65, n_packets=61, n_bytes=5642, idle_age=158, priority=100,reg15=0x5,metadata=0x4 actions=output:7
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;packstack部署： https://www.rdoproject.org/install/packstack/&lt;/li&gt;
  &lt;li&gt;kuryr部署： https://docs.openstack.org/kuryr-kubernetes/latest/installation/manual.html&lt;/li&gt;
  &lt;li&gt;kuryr设计文档： https://docs.openstack.org/kuryr-kubernetes/latest/devref/kuryr_kubernetes_design.html&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/05/14/k8s-kuryr-neutron/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/05/14/k8s-kuryr-neutron/</guid>
        
        <category>k8s</category>
        
        <category>cni</category>
        
        <category>neutron</category>
        
        <category>kuryr</category>
        
        
      </item>
    
      <item>
        <title>GO MOD使用私有仓库</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;在常见的项目中使用go mod做包管理，在引用第三方包时这个包可能维护在内部私有仓库中，需要解决几个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;引用第三方库&lt;/li&gt;
  &lt;li&gt;引用私有仓库时不使用代理&lt;/li&gt;
  &lt;li&gt;使用git方式获取代码&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;替换引用的仓库地址&quot;&gt;替换引用的仓库地址&lt;/h2&gt;
&lt;p&gt;这里直接修改代码中的引用工作量太大了，可以通过在go.mod中添加replace的方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;github.com/gophercloud/gophercloud =&amp;gt; gerrit-infrastructure.xxx.org/gerrit/a/container/gophercloud.git xxx-v0.7&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意这里末尾要加.git，否则默认使用https获取代码&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;私有仓库&quot;&gt;私有仓库&lt;/h2&gt;

&lt;p&gt;设置GOPRIVATE&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;export GOPRIVATE=gerrit-infrastructure.xxx.org&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;使用git方式获取代码&quot;&gt;使用git方式获取代码&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@localhost cloud-provider-openstack]# cat /root/.gitconfig
[user]
    name = xxx
    email = xxx@xxxx.com
[url &quot;ssh://xxx@gerrit-infrastructure.xxx.org:29418/container/gophercloud&quot;]
    insteadOf = https://gerrit-infrastructure.xxx.org/gerrit/a/container/gophercloud
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://goproxy.io/zh/&lt;/li&gt;
  &lt;li&gt;https://segmentfault.com/a/1190000021127791&lt;/li&gt;
  &lt;li&gt;https://juejin.cn/post/6844903975859257352&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/04/06/k8s-go-mod/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/04/06/k8s-go-mod/</guid>
        
        <category>go mod</category>
        
        <category>gerrit</category>
        
        
      </item>
    
      <item>
        <title>Cinder CSI</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;由于openstack cinder提供丰富的后端存储服务，当k8s部署在openstack的虚拟机中时，使用cinder 的csi提供存储服务可以非常方便对接不同的后端存储&lt;/p&gt;

&lt;h2 id=&quot;要求&quot;&gt;要求&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;分别部署openstack和k8s集群&lt;/li&gt;
  &lt;li&gt;openstack cinder使用v3 api&lt;/li&gt;
  &lt;li&gt;openstack nova和cinder在keystone中的endpoint url对应的ip地址，k8s的虚机可以访问到&lt;/li&gt;
  &lt;li&gt;openstack开启metadata服务&lt;/li&gt;
  &lt;li&gt;openstack虚机中需要安装cloud-init
    &lt;blockquote&gt;
      &lt;p&gt;/var/lib/cloud/data/instance-id虚机中这个目录必须是虚机的uuid，这个是通过cloud-init注入的&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;虚机挂盘后要求在/dev/disk/by-id目录下有对应volume的设备存在&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;部署&quot;&gt;部署&lt;/h2&gt;

&lt;p&gt;获取k8s-openstack-provider源码（非必须）&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;git clone https://github.com/kubernetes/cloud-provider-openstack
cd cloud-provider-openstack
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;编译（需要docker-ce 17.05以上的版本， docker需要翻墙）&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;export ARCH=amd64 # Defaults to amd64
编译： make build-cmd-cinder-csi-plugin
生成镜像：make image-cinder-csi-plugin
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;修改kubelet配置&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim /etc/kubernetes/kubelet.env
KUBELET_CLOUDPROVIDER=&quot;--cloud-provider=external&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;配置openstack信息并使用base64加密&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim cloud.conf
[Global]
username = ArcherAdmin
password = ArcherAdmin@123
domain-name = Default
auth-url = http://172.118.23.20:45357/v3
tenant-id = ad88dd5d24ce4e2189a6ae7491c33e9d
region = RegionOne

[Metadata]
search-order = configDrive,metadataService
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;使用base64对openstack配置加密&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat cloud.conf | base64 -w 0
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;获取社区yaml文件&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;https://github.com/kubernetes/cloud-provider-openstack/tree/master/manifests/cinder-csi-plugin
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;将之前openstack的配置对应的base64结果更新到csi-secret-cinderplugin.yaml文件中&lt;/p&gt;

&lt;p&gt;创建cinder csi资源&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;kubectl apply -f cinder-csi-plugin
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;查看cinder csi pod状态&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 csi-cinder]# kubectl get pods -n kube-system
NAME                                       READY   STATUS        RESTARTS   AGE
csi-cinder-controllerplugin-0              5/5     Running       25         4h12m
csi-cinder-nodeplugin-cwzpr                2/2     Running       0          7m28s
csi-cinder-nodeplugin-wxl6f                2/2     Running       0          7m29s
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建storeageclass和pvc&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: csi-sc-cinderplugin
provisioner: cinder.csi.openstack.org


cat pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: csi-pvc-cinderplugin
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-sc-cinderplugin
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;确认sc和pvc状态&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 resource-yamls]# kubectl get sc
NAME                  PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
csi-sc-cinderplugin   cinder.csi.openstack.org   Delete          Immediate           false                  26h
csi-sc-hujin          arstor.csi.huayun.io       Delete          Immediate           false                  4d9h
[root@node1 resource-yamls]# kubectl get pvc
NAME                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
csi-pvc-cinderplugin   Bound    pvc-e5aec543-ff37-40ad-85d5-ce038975e14c   1Gi        RWO            csi-sc-cinderplugin   12m
[root@node1 resource-yamls]#
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建pod&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    ports:
    - containerPort: 80
      protocol: TCP
    volumeMounts:
      - mountPath: /var/lib/www/html
        name: csi-data-cinderplugin
  volumes:
  - name: csi-data-cinderplugin
    persistentVolumeClaim:
      claimName: csi-pvc-cinderplugin
      readOnly: false
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;从pv中获取volume id&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 resource-yamls]# kubectl describe pv pvc-e5aec543-ff37-40ad-85d5-ce038975e14c
Name:            pvc-e5aec543-ff37-40ad-85d5-ce038975e14c
Labels:          &amp;lt;none&amp;gt;
Annotations:     pv.kubernetes.io/provisioned-by: cinder.csi.openstack.org
Finalizers:      [kubernetes.io/pv-protection external-attacher/cinder-csi-openstack-org]
StorageClass:    csi-sc-cinderplugin
Status:          Bound
Claim:           default/csi-pvc-cinderplugin
Reclaim Policy:  Delete
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   &amp;lt;none&amp;gt;
Message:
Source:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            cinder.csi.openstack.org
    FSType:            ext4
    VolumeHandle:      ef9037a7-9a67-408a-9f92-adbd2badc5db
    ReadOnly:          false
    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity=1616250534343-8081-cinder.csi.openstack.org
Events:                &amp;lt;none&amp;gt;
[root@node1 resource-yamls]#
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在openstack中查看volume状态&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@controller01 ~]# cinder list |grep ef9037a7
| ef9037a7-9a67-408a-9f92-adbd2badc5db |   in-use  | pvc-e5aec543-ff37-40ad-85d5-ce038975e14c |  1   | basic-replica2 |  false   | 94a932d0-79da-4ed2-a228-a4e96264d1c0 |
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;确认pod状态&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@node1 csi-cinder]# kubectl get pods     -owide
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          35s   10.244.28.16   node3   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

[root@node1 resource-yamls]# kubectl exec -it nginx -- sh
/ # ls /var/lib/www/html/
lost+found
/ #
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;测试场景&quot;&gt;测试场景&lt;/h2&gt;

&lt;h3 id=&quot;命令行删除deployment-pod&quot;&gt;命令行删除deployment pod&lt;/h3&gt;
&lt;p&gt;删除后正常新建pod，且使用原来的volume&lt;/p&gt;

&lt;h3 id=&quot;k8s虚机网络故障&quot;&gt;k8s虚机网络故障&lt;/h3&gt;
&lt;p&gt;deployment中其中一个pod所在节点down机后，新建pod失败，报volume in-use
statefulset中其中一个pod所在节点down机后，不新建pod&lt;/p&gt;

&lt;h3 id=&quot;k8s虚机重启&quot;&gt;k8s虚机重启&lt;/h3&gt;
&lt;p&gt;同节点down机&lt;/p&gt;

&lt;h3 id=&quot;虚机迁移&quot;&gt;虚机迁移&lt;/h3&gt;
&lt;p&gt;热迁移不影响pod使用
冷迁移：同节点down机&lt;/p&gt;

&lt;h3 id=&quot;k8s虚机对应计算节点故障&quot;&gt;k8s虚机对应计算节点故障&lt;/h3&gt;
&lt;p&gt;此时原来的pod一直是删除状态，新建的pod也无法正常创建，必须等待down机的节点恢复
这是一种安全的做法&lt;/p&gt;

&lt;h3 id=&quot;虚机挂在最大磁盘个数&quot;&gt;虚机挂在最大磁盘个数&lt;/h3&gt;
&lt;p&gt;这是一个bug，升级qemu后可以解决&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md&lt;/li&gt;
  &lt;li&gt;https://www.jianshu.com/p/87b02040991c&lt;/li&gt;
  &lt;li&gt;https://silenceper.com/kubernetes-book/csi/how-to-write-csi-driver.html&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/03/24/k8s-csi-cinder/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/03/24/k8s-csi-cinder/</guid>
        
        <category>k8s</category>
        
        <category>csi</category>
        
        <category>cinder</category>
        
        
      </item>
    
      <item>
        <title>Golang Debug -- gdb</title>
        <description>&lt;h2 id=&quot;获取源码并编译&quot;&gt;获取源码并编译&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;git clone https://github.com/coredns/coredns.git
cd coredns
go build -gcflags &quot;-N -l&quot; coredns.go

yum install -y gdb
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;debug&quot;&gt;debug&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;gdb coredns
b coredns.go:11
run
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://github.com/kubevirt/user-guide/pull/196/files/700980d5099b969c4fe2defb3773ec84e628a207&lt;/li&gt;
  &lt;li&gt;https://www.juniper.net/documentation/en_US/day-one-books/topics/concept/contrail-as-a-cni.html&lt;/li&gt;
  &lt;li&gt;https://www.juniper.net/documentation/en_US/contrail19/topics/concept/kubernetes-cni-contrail.html&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/03/04/k8s-gdb/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/03/04/k8s-gdb/</guid>
        
        <category>golang</category>
        
        <category>gdb</category>
        
        
      </item>
    
      <item>
        <title>TungstenFabric Kubernetes Annotation</title>
        <description>&lt;h2 id=&quot;annotation整理&quot;&gt;Annotation整理&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;名称&lt;/th&gt;
      &lt;th&gt;值&lt;/th&gt;
      &lt;th&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;opencontrail.org/fip-pool&lt;/td&gt;
      &lt;td&gt;”{‘domain’: ‘default-domain’, ‘project’: ‘ArcherAdmin’, ‘network’: ‘public1420’, ‘name’: ‘floating-ip-pool’}”&lt;/td&gt;
      &lt;td&gt;指定fip-pool&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;opencontrail.org/network&lt;/td&gt;
      &lt;td&gt;‘opencontrail.org/network’: ‘{“domain”:”default-domain”, “project”: “ArcherAdmin”, “name”:”test-net”}’&lt;/td&gt;
      &lt;td&gt;指定network,如果设置在pod中，pod使用指定网络建网卡，如果指定在namespace中，则namespace中所有pod使用指定网络建网卡&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;opencontrail.org/isolation&lt;/td&gt;
      &lt;td&gt;‘opencontrail.org/isolation’: ‘true’&lt;/td&gt;
      &lt;td&gt;设置namespace是否隔离&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;opencontrail.org/ip_fabric_forwarding&lt;/td&gt;
      &lt;td&gt;“opencontrail.org/ip_fabric_forwarding” : “true”&lt;/td&gt;
      &lt;td&gt;使用fabric网络创建pod网卡&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;opencontrail.org/ip_fabric_snat&lt;/td&gt;
      &lt;td&gt;“opencontrail.org/ip_fabric_snat” : “true”&lt;/td&gt;
      &lt;td&gt;启用pod网络的fabric_snat&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;opencontrail.org/cidr&lt;/td&gt;
      &lt;td&gt;“opencontrail.org/cidr” : “1.0.0.0/24”&lt;/td&gt;
      &lt;td&gt;使用自定义网络（非contrail网络）时指定&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://github.com/kubevirt/user-guide/pull/196/files/700980d5099b969c4fe2defb3773ec84e628a207&lt;/li&gt;
  &lt;li&gt;https://www.juniper.net/documentation/en_US/day-one-books/topics/concept/contrail-as-a-cni.html&lt;/li&gt;
  &lt;li&gt;https://www.juniper.net/documentation/en_US/contrail19/topics/concept/kubernetes-cni-contrail.html&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/01/27/tf-k8s-annotation/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/01/27/tf-k8s-annotation/</guid>
        
        <category>kubernetes</category>
        
        <category>tungstenfabric</category>
        
        <category>annotation</category>
        
        
      </item>
    
      <item>
        <title>CoreDNS</title>
        <description>&lt;h2 id=&quot;架构&quot;&gt;架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/2018-11-07-coredns-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;搭建&quot;&gt;搭建&lt;/h2&gt;

&lt;p&gt;正常使用kubeadm等工具部署的集群中默认都已经部署有coredns，以下为手动部署的流程：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;创建一个serviceaccount用来做鉴权用&lt;/li&gt;
  &lt;li&gt;创建clusterrole对象system:coredns&lt;/li&gt;
  &lt;li&gt;绑定serviceaccount和clusterrole&lt;/li&gt;
  &lt;li&gt;创建coredns的配置configmap，在后面创建deployment时引用。这里需要注意根据需要修改forward和pods属性&lt;/li&gt;
  &lt;li&gt;创建coredns的deployment，容器引用对应的configmap，限制资源、暴露端口、配置调度策略等到&lt;/li&gt;
  &lt;li&gt;创建service，设置podselector和端口&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;coredns.yaml&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat coredns.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: &quot;CoreDNS&quot;
spec:
  # replicas: not specified here:
  # 1. Default is 1.
  # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      tolerations:
        - key: &quot;CriticalAddonsOnly&quot;
          operator: &quot;Exists&quot;
      nodeSelector:
        kubernetes.io/os: linux
      affinity:
         podAntiAffinity:
           preferredDuringSchedulingIgnoredDuringExecution:
           - weight: 100
             podAffinityTerm:
               labelSelector:
                 matchExpressions:
                   - key: k8s-app
                     operator: In
                     values: [&quot;kube-dns&quot;]
               topologyKey: kubernetes.io/hostname
      containers:
      - name: coredns
        image: coredns/coredns:1.8.0
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ &quot;-conf&quot;, &quot;/etc/coredns/Corefile&quot; ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: &quot;9153&quot;
    prometheus.io/scrape: &quot;true&quot;
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: &quot;true&quot;
    kubernetes.io/name: &quot;CoreDNS&quot;
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: CLUSTER_DNS_IP
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建后：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[root@none-nested-master hujin]# kubectl get service -n kube-system
NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
k8s-keystone-auth-service   ClusterIP   10.233.40.69   &amp;lt;none&amp;gt;        8443/TCP        4d20h
kube-dns                    ClusterIP   10.233.0.110   &amp;lt;none&amp;gt;        53/UDP,53/TCP   7d2h
[root@none-nested-master hujin]# kubectl get deploy -n kube-system
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
coredns             1/1     1            1           7d2h
k8s-keystone-auth   1/1     1            1           4d18h
[root@none-nested-master hujin]# kubectl get deploy -n kube-system
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
coredns             1/1     1            1           7d2h
k8s-keystone-auth   1/1     1            1           4d18h
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;修改集群cluster dns配置，这里如果集群默认部署coredns，会设置成pod的ip，修改后，新建的pod中/etc/resolv.conf文件会获取配置后的ip&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim /etc/kubernetes/kubelet-config.yaml
clusterDNS:
- 10.233.0.110

service kubelet restart
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;使用&quot;&gt;使用&lt;/h2&gt;

&lt;h3 id=&quot;通过使用busybox中的nslookup命令访问域名&quot;&gt;通过使用busybox中的nslookup命令访问域名&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
  namespace: default
spec:
  containers:
  - image: aarch64/busybox
    command:
      - sleep
      - &quot;3600&quot;
    imagePullPolicy: IfNotPresent
    name: busybox2
  restartPolicy: Always
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;访问service&quot;&gt;访问service&lt;/h3&gt;

&lt;p&gt;格式：[svc-name].[namespace].svc.[self-defined cluster domain]&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 查询域名：
/ # nslookup my-apache.default.svc.cluster.local
Server:    10.233.0.110
Address 1: 10.233.0.110 kube-dns.kube-system.svc.cluster.local

Name:      my-apache.default.svc.cluster.local
Address 1: 10.233.4.36 my-apache.default.svc.cluster.local

# 通过域名访问服务
sh-4.2# curl my-apache.default.svc.cluster.local
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;访问pod&quot;&gt;访问pod&lt;/h3&gt;

&lt;p&gt;格式：[pod-ip].[namespace].pod.[self-defined cluster domain]&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 查询域名
/ # nslookup 10-233-127-250.default.pod.cluster.local
Server:    10.233.0.110
Address 1: 10.233.0.110 kube-dns.kube-system.svc.cluster.local

Name:      10-233-127-250.default.pod.cluster.local
Address 1: 10.233.127.250 10-233-127-250.my-apache.default.svc.cluster.local

# 通过域名访问服务
sh-4.2# curl 10-233-127-250.default.pod.cluster.local
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;自定义域名&quot;&gt;自定义域名&lt;/h2&gt;

&lt;p&gt;一般情况下我们使用coredns是在k8s中的，但是实际coredns提供了很多插件，这里我们可以通过hosts插件实现自定义域名的功能,&lt;/p&gt;

&lt;p&gt;hosts插件的配置参数参考：
    hosts [FILE [ZONES…]] {
        [INLINE]
        ttl SECONDS
        no_reverse
        reload DURATION
        fallthrough [ZONES…]
    }&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;FILE：需要读取与解析的hosts文件；如果省略，默认取值”/etc/hosts”；每5s扫描一次hosts文件的变更。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;ZONES：如果为空，取配置块中的zone。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;INLINE：宿主机hosts文件在corefile中的内联；在”fallthrough”之前的所有”INLINE”都可视为hosts文件的附加内容，hosts文件中相同条目将被覆盖，以”INLINE”为准。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;fallthrough：如果zone匹配且无法生成记录，将请求传递给下一个插件；如果省略，对所有zones有效，如果列出特定zone，则只有列出的zone受到影响。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;修改coredns的configmap，这里修改完成后需要等待短暂时间重新加载配置&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;kubectl edit configmaps -n kube-system coredns
apiVersion: v1
data:
  Corefile: |
    .:53 {
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        hosts /etc/hosts {
           10.244.0.4 nginx.me.test
           fallthrough
        }
    }

[root@hujin-test ~]# kubectl exec -it my-busybox-6dffd8765-pkcl7 -- sh
/ # ping nginx.me.test
PING nginx.me.test (10.244.0.4): 56 data bytes
64 bytes from 10.244.0.4: seq=0 ttl=63 time=0.281 ms
64 bytes from 10.244.0.4: seq=1 ttl=63 time=0.254 ms
64 bytes from 10.244.0.4: seq=2 ttl=63 time=0.195 ms
64 bytes from 10.244.0.4: seq=3 ttl=63 time=0.166 ms
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/coredns/deployment/tree/master/kubernetes&lt;/li&gt;
  &lt;li&gt;https://coredns.io/plugins/hosts/&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/01/18/k8s-coredns/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/01/18/k8s-coredns/</guid>
        
        <category>kubernetes</category>
        
        <category>coredns</category>
        
        
      </item>
    
      <item>
        <title>Kuberntes Webhook -- Keystone</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;k8s本身不提供用户和项目管理功能，需要依赖第三方组件来实现。
由于k8s和openstack的融合需求，需要提供一种统一的用户鉴权机制，本次重点介绍k8s如何使用keystone进行鉴权。通过调用keystone获取token，并使用这个token调用k8s的接口获取资源信息。&lt;/p&gt;

&lt;h2 id=&quot;架构图&quot;&gt;架构图&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://superuser.openstack.org/wp-content/uploads/2019/03/fig2.png&quot; alt=&quot;k8s-keystone-auth&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要流程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;客户端通过rest api调用openstack keystone获取token&lt;/li&gt;
  &lt;li&gt;通过获取到的token调用k8s api&lt;/li&gt;
  &lt;li&gt;kube-apiserver 根据配置调用webhook程序，校验token并获取token对应用户的权限&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;部署&quot;&gt;部署&lt;/h2&gt;
&lt;p&gt;webhook和odic一样也是集成外部认证系统的一种方式，当client发起api-server请求时会触发webhook服务TokenReview调用，webhook会检查用户的凭证信息，如果是合法则返回authenticated”: true等信息。api-server会等待webhook服务返回，如果返回的authenticated结果为true，则表明认证成功，否则拒绝访问。&lt;/p&gt;

&lt;h3 id=&quot;配置openstack&quot;&gt;配置openstack&lt;/h3&gt;

&lt;p&gt;在openstack中创建资源&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;openstack role create k8s-admin
openstack user create demo_admin --project demo --password secret
openstack role add --user demo_admin --project demo k8s-admin
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建demo-rc,用来后面获取token&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;export OS_AUTH_URL=&quot;http://172.16.41.80:35357/v3&quot;
export OS_USERNAME=&quot;demo_admin&quot;
export OS_PASSWORD=&quot;secret&quot;
export OS_PROJECT_NAME=&quot;demo&quot;
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_REGION_NAME=RegionTwo
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
export PYTHONIOENCODING=&apos;utf-8&apos;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;kubernetes中创建k8s-keystone-auth-webhook&quot;&gt;kubernetes中创建k8s-keystone-auth webhook&lt;/h3&gt;

&lt;p&gt;创建keystone权限configmap&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat keystone_configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-auth-policy
  namespace: kube-system
data:
  policies: |
    [
      {
        &quot;users&quot;: {
          &quot;projects&quot;: [&quot;demo&quot;],
          &quot;roles&quot;: [&quot;member&quot;]
        },
        &quot;resource_permissions&quot;: {
          &quot;*/pods&quot;: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
        }
      }
    ]

kubectl apply -f keystone_configmap.yaml
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建证书，给keystone-webhook容器使用&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes -subj /CN=k8s-keystone-auth.kube-system/
kubectl --namespace kube-system create secret tls keystone-auth-certs --cert=cert.pem --key=key.pem
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建keystone rbac&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat keystone-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: k8s-keystone-auth
  name: k8s-keystone-auth
rules:
  # Allow k8s-keystone-auth to get k8s-auth-policy configmap
- apiGroups: [&quot;&quot;]
  resources: [&quot;configmaps&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8s-keystone-auth
  labels:
    k8s-app: k8s-keystone-auth
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8s-keystone-auth
subjects:
- kind: ServiceAccount
  name: k8s-keystone
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8s-keystone
  namespace: kube-system
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;部署 k8s-keystone-auth，注意替换&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keystone-url&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat keystone-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-keystone-auth
  namespace: kube-system
  labels:
    app: k8s-keystone-auth
spec:
  replicas: 2
  selector:
    matchLabels:
      app: k8s-keystone-auth
  template:
    metadata:
      labels:
        app: k8s-keystone-auth
    spec:
      serviceAccountName: k8s-keystone
      containers:
        - name: k8s-keystone-auth
          image: k8scloudprovider/k8s-keystone-auth:latest
          args:
            - ./bin/k8s-keystone-auth
            - --tls-cert-file
            - /etc/pki/tls.crt
            - --tls-private-key-file
            - /etc/pki/tls.key
            - --policy-configmap-name
            - k8s-auth-policy
            - --keystone-url
            - http://172.16.41.80:35357/v3
          volumeMounts:
            - mountPath: /etc/pki
              name: certs
              readOnly: true
          ports:
            - containerPort: 8443
      volumes:
      - name: certs
        secret:
          secretName: keystone-auth-certs
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建keystone_service，这里建议创建后给service绑定浮动IP，方便用来给外部调用用&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cat keystone-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: k8s-keystone-auth-service
  namespace: kube-system
spec:
  selector:
    app: k8s-keystone-auth
  ports:
    - protocol: TCP
      port: 8443
      targetPort: 8443
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;验证k8s-keystone-auth服务&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# input:
token = `source demo-rc;openstack token issue -f yaml -c id |awk &apos;{print $2}&apos;`
kubectl run curl --rm -it --restart=Never --image curlimages/curl -- \
  -k -XPOST https://k8s-keystone-auth-service.kube-system:8443/webhook -d &apos;
{
  &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;TokenReview&quot;,
  &quot;metadata&quot;: {
    &quot;creationTimestamp&quot;: null
  },
  &quot;spec&quot;: {
    &quot;token&quot;: &quot;&apos;$token&apos;&quot;
  }
}&apos;

# output:
{
  &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;TokenReview&quot;,
  &quot;metadata&quot;: {
    &quot;creationTimestamp&quot;: null
  },
  &quot;spec&quot;: {
    &quot;token&quot;: &quot;gAAAAABf_7dmngBBu9cThzYmRs8Hkqv9Gm1FBRL0duTFAv7xl5dwwHA3yhKnCo_cqvsnKt90ukdmV5crq1s6EgBjh_e5cvhGBejFdRADViH9Vmmr6KI2L9I8gG4Dkj52whKNVqxZ-2R81rOj_Amqj83Iwa5TEWURXVfKNaL9ktLPR3-qY4TkjWU&quot;
  },
  &quot;status&quot;: {
    &quot;authenticated&quot;: true,
    &quot;user&quot;: {
      &quot;username&quot;: &quot;demo_admin&quot;,
      &quot;uid&quot;: &quot;b9b167f5e86b48839a879e111e20a0b1&quot;,
      &quot;groups&quot;: [
        &quot;bf37908a629b4d2ca02dfc840da027cd&quot;
      ],
      &quot;extra&quot;: {
        &quot;alpha.kubernetes.io/identity/project/id&quot;: [
          &quot;bf37908a629b4d2ca02dfc840da027cd&quot;
        ],
        &quot;alpha.kubernetes.io/identity/project/name&quot;: [
          &quot;demo&quot;
        ],
        &quot;alpha.kubernetes.io/identity/roles&quot;: [
          &quot;k8s-admin&quot;
        ],
        &quot;alpha.kubernetes.io/identity/user/domain/id&quot;: [
          &quot;default&quot;
        ],
        &quot;alpha.kubernetes.io/identity/user/domain/name&quot;: [
          &quot;Default&quot;
        ]
      }
    }
  }
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;配置kube-apisever&quot;&gt;配置kube-apisever&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;mkdir /etc/kubernetes/webhooks
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/kubernetes/webhooks/webhookconfig.yaml
---
apiVersion: v1
kind: Config
preferences: {}
clusters:
  - cluster:
      insecure-skip-tls-verify: true
      server: https://178.119.220.88:8443/webhook
    name: webhook
users:
  - name: webhook
contexts:
  - context:
      cluster: webhook
      user: webhook
    name: webhook
current-context: webhook
EOF
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;178.119.220.88是k8s-keystone-auth的浮动IP&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;vim /etc/kubernetes/manifests/kube-apiserver.yaml
spec:
  containers:
  - command:
  ...
  - --authentication-token-webhook-config-file=/etc/kubernetes/webhooks/webhookconfig.yaml
  - --authorization-webhook-config-file=/etc/kubernetes/webhooks/webhookconfig.yaml
  - --authorization-mode=Node,RBAC,Webhook

  volumeMounts:
  ...
  - mountPath: /etc/kubernetes/webhooks
    name: webhooks
    readOnly: true
volumes:
...
- hostPath:
    path: /etc/kubernetes/webhooks
    type: DirectoryOrCreate
  name: webhooks
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;修改完成后kube-apiserver会自动重启，注意查看日志是否有报错&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;测试&quot;&gt;测试&lt;/h2&gt;

&lt;p&gt;脚本：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import eventlet
eventlet.monkey_patch()

from keystoneclient.v3 import client as keystone_client
import requests
from pprint import pprint
import json


def get_openstack_token():
    keystone = keystone_client.Client(username=&apos;demo_admin&apos;, password=&apos;secret&apos;,
                                      auth_url=&apos;http://172.16.41.80:35357/v3&apos;,
                                      tenant_name=&apos;demo&apos;,
                                      project_domain_name=&apos;Default&apos;,
                                      user_domain_name=&apos;Default&apos;,
                                      project_name=&apos;demo&apos;)
    print(keystone.auth_token)
    return keystone.auth_token


def check_token(token):
    data = {&quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,
            &quot;kind&quot;: &quot;TokenReview&quot;,
            &quot;metadata&quot;: {
                &quot;creationTimestamp&quot;: None
            },
            &quot;spec&quot;: {
                &quot;token&quot;:  token
            }}
    headers = {&apos;Content-Type&apos;: &apos;application/json&apos;, &apos;Connection&apos;: &apos;Keep-Alive&apos;}
    req = requests.post(&apos;https://178.119.220.88:8443/webhook&apos;,
                        data=json.dumps(data), headers=headers, verify=False, timeout=5)
    print(req.content)


def list_ingress(tk):
    auth_url = &apos;https://179.18.3.180:6443&apos;
    headers = {&apos;Connection&apos;: &apos;Keep-Alive&apos;,
               &apos;Authorization&apos;: &apos;Bearer %s&apos; % tk}
    req_url = &apos;%s/api/v1/namespaces&apos; % auth_url
    print(&apos;=&apos;*20, req_url, headers)
    req = requests.get(req_url, headers=headers, verify=False)
    pprint(json.loads(req.content))

token = get_openstack_token()
list_ingress(token)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;参考文档&quot;&gt;参考文档：&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/keystone-auth/using-keystone-webhook-authenticator-and-authorizer.md&lt;/li&gt;
  &lt;li&gt;https://k2r2bai.com/2018/05/30/kubernetes/keystone-auth/&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/97797321&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/blog/2021/01/15/k8s-webhook-keystone/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/blog/2021/01/15/k8s-webhook-keystone/</guid>
        
        <category>kubernetes</category>
        
        <category>webhook</category>
        
        <category>keystone</category>
        
        
      </item>
    
  </channel>
</rss>
